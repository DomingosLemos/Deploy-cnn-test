{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLNN_v01.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "i5PjZetq5X08",
        "QvIpODRc5YUe",
        "y9Yza1ir5YuX"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomingosLemos/Deploy-cnn-test/blob/main/DLNN_v01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvL02fSdjVfM",
        "outputId": "6e1b7aaf-2057-4e44-991b-fc8e34d8d408"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "if device_name != '/device:GPU:0':\r\n",
        "  raise SystemError('GPU device not found')\r\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXavUfmhoK5U",
        "outputId": "e5b2fa9d-d968-47e9-871b-cd8d0af3cf67"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfETxsFEjZyg",
        "outputId": "9103e504-22ce-444f-90bf-6f8efd2b7a0d"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "import tensorflow as tf\r\n",
        "import timeit\r\n",
        "\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "if device_name != '/device:GPU:0':\r\n",
        "  print(\r\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\r\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\r\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\r\n",
        "  raise SystemError('GPU device not found')\r\n",
        "\r\n",
        "def cpu():\r\n",
        "  with tf.device('/cpu:0'):\r\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\r\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\r\n",
        "    return tf.math.reduce_sum(net_cpu)\r\n",
        "\r\n",
        "def gpu():\r\n",
        "  with tf.device('/device:GPU:0'):\r\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\r\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\r\n",
        "    return tf.math.reduce_sum(net_gpu)\r\n",
        "  \r\n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\r\n",
        "cpu()\r\n",
        "gpu()\r\n",
        "\r\n",
        "# Run the op several times.\r\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\r\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\r\n",
        "print('CPU (s):')\r\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\r\n",
        "print(cpu_time)\r\n",
        "print('GPU (s):')\r\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\r\n",
        "print(gpu_time)\r\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.7184771029997137\n",
            "GPU (s):\n",
            "0.03866130999995221\n",
            "GPU speedup over CPU: 70x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLvSpNQqkT08"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import h5py\r\n",
        "import gc\r\n",
        "import tensorflow as tf\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras_preprocessing.image import ImageDataGenerator\r\n",
        "from keras import models\r\n",
        "from keras import layers\r\n",
        "from keras import optimizers\r\n",
        "from keras import regularizers\r\n",
        "from keras.callbacks import EarlyStopping\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras import Model\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping\r\n",
        "from keras.constraints import maxnorm\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.optimizers import RMSprop\r\n",
        "\r\n",
        "#models\r\n",
        "from keras.applications.resnet50 import ResNet50\r\n",
        "from keras.applications.inception_v3 import InceptionV3"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk1fVSGHp7WU",
        "outputId": "42a47f20-7c3b-453e-8121-5a55d4ed5325"
      },
      "source": [
        "!7z x /content/drive/MyDrive/EDSA/images.zip -o./images/"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/drive/MyDrive/EDSA/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 5132666035 bytes (4895 MiB)\n",
            "\n",
            "Extracting archive: /content/drive/MyDrive/EDSA/images.zip\n",
            "  4% 4096 Open\b\b\b\b\b\b\b\b\b\b\b\b\b\b              \b\b\b\b\b\b\b\b\b\b\b\b\b\b--\n",
            "Path = /content/drive/MyDrive/EDSA/images.zip\n",
            "Type = zip\n",
            "Physical Size = 5132666035\n",
            "64-bit = +\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  0% 676 - apple_pie/3327221.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  1% 1227 - baby_back_ribs/1729685.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  1% 1804\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b  2% 2107 - baklava/1366248.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  2% 2638 - baklava/322414.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  3% 3178 - beef_carpaccio/1558727.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  3% 3758 - beef_carpaccio/3567014.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% 4182 - beef_tartare/1720794.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% 4727 - beef_tartare/3562074.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% 5323 - beet_salad/2046163.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% 5894 - beet_salad/561790.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% 6384 - beignets/2278359.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% 6874 - beignets/599749.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% 7502\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b  8% 7888 - bibimbap/553263.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% 8512 - bread_pudding/2787343.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  9% 8979 - bread_pudding/881148.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  9% 9673 - breakfast_burrito/3486949.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% 10252 - bruschetta/1973102.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% 10684 - bruschetta/3579764.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% 11169 - caesar_salad/1635167.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% 11726 - caesar_salad/3439225.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% 12295 - cannoli/190939.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% 12780 - cannoli/3742967.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% 13349 - caprese_salad/2260759.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% 13898 - caprese_salad/680059.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% 14535 - carrot_cake/2939724.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 15% 14909 - carrot_cake/653778.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 15% 15537 - ceviche/2953128.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% 15992 - ceviche/906523.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% 16722 - cheesecake/3519289.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% 17203 - cheese_plate/1621222.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% 17756 - cheese_plate/3667453.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% 18337 - chicken_curry/210587.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% 18874 - chicken_curry/523331.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% 19382 - chicken_quesadilla/2324741.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% 19729 - chicken_quesadilla/349749.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% 19866 - chicken_quesadilla/479440.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% 19997 - chicken_quesadilla/917946.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 20148 - chicken_wings/1432342.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 20291 - chicken_wings/191800.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 20423 - chicken_wings/2425119.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 20562 - chicken_wings/2878316.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 20698 - chicken_wings/3385347.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 20836 - chicken_wings/44776.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 20966 - chicken_wings/836213.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 21108 - chocolate_cake/1306649.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 21254 - chocolate_cake/1847512.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 21395 - chocolate_cake/2408310.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 21541 - chocolate_cake/2933701.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 21686 - chocolate_cake/3348435.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 21835 - chocolate_cake/3855842.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 21991 - chocolate_cake/873888.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 22142 - chocolate_mousse/1452244.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 22294 - chocolate_mousse/2015024.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 22455 - chocolate_mousse/2483058.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 22620 - chocolate_mousse/3102761.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 22785 - chocolate_mousse/3760569.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 22946 - chocolate_mousse/726911.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 23098 - churros/123162.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% 23247 - churros/1797535.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% 23396 - churros/227002.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% 23544 - churros/2776578.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% 23693 - churros/3323514.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% 23839 - churros/3885949.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% 23973 - churros/833252.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% 24132 - clam_chowder/1373718.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 24306 - clam_chowder/2058701.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 24474 - clam_chowder/2640463.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 24654 - clam_chowder/3203674.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 24831 - clam_chowder/38898.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 24995\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 24% 25137 - club_sandwich/1329330.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 25263 - club_sandwich/1727403.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 25403 - club_sandwich/2277118.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 25540 - club_sandwich/2852142.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 25676 - club_sandwich/3329011.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 25820 - club_sandwich/3783437.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 25951 - club_sandwich/726363.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 26096 - crab_cakes/123352.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 26241 - crab_cakes/1722425.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 26408 - crab_cakes/2269732.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 26555 - crab_cakes/2778048.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 26704 - crab_cakes/3392380.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 26857 - crab_cakes/519336.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 26994 - crab_cakes/907229.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 27146 - creme_brulee/1297063.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 27295 - creme_brulee/1849031.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 27447 - creme_brulee/2352946.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 27606 - creme_brulee/287343.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 27746 - creme_brulee/3533595.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 27912 - creme_brulee/671647.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 28087 - croque_madame/1197634.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 28219 - croque_madame/160044.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 28365\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 28% 28505 - croque_madame/269992.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% 28642 - croque_madame/3190488.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% 28841 - croque_madame/3917620.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% 28983 - croque_madame/839178.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% 29162 - cup_cakes/1437964.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% 29308 - cup_cakes/1976170.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% 29438 - cup_cakes/2433557.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 29590 - cup_cakes/3025419.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 29738 - cup_cakes/3624968.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 29880 - cup_cakes/588751.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 30032 - deviled_eggs/1006887.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 30200 - deviled_eggs/1561986.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 30361 - deviled_eggs/2082375.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 30505 - deviled_eggs/2697260.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 30649 - deviled_eggs/3228645.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 30798 - deviled_eggs/3806278.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 30948 - deviled_eggs/732499.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 31128 - donuts/1360217.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 31265 - donuts/1840238.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 31403 - donuts/2249805.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 31550 - donuts/2740296.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% 31681 - donuts/325885.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% 31819 - donuts/42050.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% 31953 - donuts/778567.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% 32124 - dumplings/1254489.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% 32264 - dumplings/1727493.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% 32487 - dumplings/2544171.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% 32643 - dumplings/3132824.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 32800 - dumplings/3868944.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 32948 - dumplings/776475.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 33099 - edamame/1264036.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 33246 - edamame/1812993.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 33390 - edamame/2304068.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 33533 - edamame/2866674.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 33682 - edamame/3325153.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 33839 - edamame/3908633.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 33977 - edamame/784656.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 34143 - eggs_benedict/1388232.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 34288 - eggs_benedict/1880800.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 34448 - eggs_benedict/2458593.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 34607 - eggs_benedict/3080487.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 34762 - eggs_benedict/3603645.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% 34918 - eggs_benedict/64083.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% 35072 - escargots/1107023.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% 35198 - escargots/1548113.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% 35336 - escargots/2022296.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% 35471 - escargots/2492055.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% 35605 - escargots/3032821.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% 35739 - escargots/3459901.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 35877 - escargots/464232.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 36016 - escargots/942183.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 36142 - falafel/1412347.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 36279 - falafel/1863600.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 36422 - falafel/2375267.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 36560 - falafel/2930114.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 36701 - falafel/3394578.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 36851 - falafel/4143.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 36985 - falafel/843221.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 37125 - filet_mignon/1283636.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 37281 - filet_mignon/1865455.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 37424 - filet_mignon/2398171.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 37580 - filet_mignon/287177.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 37730 - filet_mignon/3506641.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% 37884 - filet_mignon/550190.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% 38025 - filet_mignon/969953.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% 38173 - fish_and_chips/1515359.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% 38314 - fish_and_chips/2011865.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% 38450 - fish_and_chips/2496986.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% 38601 - fish_and_chips/3087259.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% 38742 - fish_and_chips/3647155.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 38880 - fish_and_chips/535559.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 39013 - fish_and_chips/946237.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 39162 - foie_gras/1421924.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 39302 - foie_gras/1834984.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 39457\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 38% 39569 - foie_gras/2787447.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 39727 - foie_gras/3334523.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 39885 - foie_gras/467426.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 40031 - foie_gras/952007.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 40179 - french_fries/1606755.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 40329 - french_fries/2195192.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 40483 - french_fries/2661646.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 40641 - french_fries/3152740.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 40785 - french_fries/3626833.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 40928 - french_fries/578368.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 41072\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 40% 41213 - french_onion_soup/1505015.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 41359 - french_onion_soup/2074767.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 41500 - french_onion_soup/2597616.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 41647 - french_onion_soup/3037781.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 41800 - french_onion_soup/3637619.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 41952 - french_onion_soup/677583.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 42100 - french_toast/1214016.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 42224 - french_toast/1593854.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 42355 - french_toast/2069063.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 42485 - french_toast/2533516.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 42618 - french_toast/3020879.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 42749 - french_toast/352239.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 42867 - french_toast/505316.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 43005 - french_toast/864354.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 43144\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 42% 43288 - fried_calamari/1772646.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 43423 - fried_calamari/2176324.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 43556 - fried_calamari/2584656.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 43695 - fried_calamari/317180.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 43832 - fried_calamari/440673.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 43969 - fried_calamari/781403.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 44112 - fried_rice/1219625.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 44248 - fried_rice/1668806.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 44376 - fried_rice/2150469.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 44511 - fried_rice/2598868.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 44640 - fried_rice/3026143.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 44770 - fried_rice/3571082.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 44906 - fried_rice/490569.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 45039 - fried_rice/983544.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 45214 - frozen_yogurt/1596916.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 45385 - frozen_yogurt/2089799.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 45550 - frozen_yogurt/2539995.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 45726 - frozen_yogurt/35457.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 45897 - frozen_yogurt/566342.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 46085 - garlic_bread/1133840.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 46211 - garlic_bread/1745995.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 46350 - garlic_bread/218445.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 46492 - garlic_bread/2609711.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 46638 - garlic_bread/3087427.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 46771 - garlic_bread/3525232.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 46910 - garlic_bread/471159.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 47049 - gnocchi/1005096.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 47207 - gnocchi/157925.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 47347 - gnocchi/2136654.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 47490 - gnocchi/2717303.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 47634 - gnocchi/3269265.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 47781 - gnocchi/3703320.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 47935\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 47% 48085 - greek_salad/1155906.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 48202 - greek_salad/1646423.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 48324 - greek_salad/2020687.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 48456 - greek_salad/2437071.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 48587 - greek_salad/3029549.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 48717 - greek_salad/3495612.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 48852 - greek_salad/3882470.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 48984 - greek_salad/775939.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 49124 - grilled_cheese_sandwich/1195476.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 49257 - grilled_cheese_sandwich/1702468.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 49391 - grilled_cheese_sandwich/2082630.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 49530 - grilled_cheese_sandwich/2658290.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 49672 - grilled_cheese_sandwich/3222407.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 49814 - grilled_cheese_sandwich/499035.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 49949 - grilled_cheese_sandwich/751419.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 50083 - grilled_salmon/1156486.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 50222 - grilled_salmon/1713655.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 50370 - grilled_salmon/2155390.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 50500 - grilled_salmon/2621480.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 50644 - grilled_salmon/3165921.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 50785\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 50% 50921 - grilled_salmon/593514.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 51079 - guacamole/1130569.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 51211 - guacamole/1619276.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 51342 - guacamole/2074886.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 51472 - guacamole/2570824.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 51610 - guacamole/3118608.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 51737 - guacamole/3556914.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 51873 - guacamole/469602.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 52006 - guacamole/847841.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 52160\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 51% 52303 - gyoza/2015509.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 52444 - gyoza/255283.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 52592 - gyoza/3035452.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 52738 - gyoza/34871.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 52887 - gyoza/488002.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 53024 - gyoza/893221.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 53174\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 52% 53310 - hamburger/1950289.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 53449 - hamburger/2437718.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 53551 - hamburger/2882999.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 53684 - hamburger/3402120.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 53829 - hamburger/402200.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 53963 - hamburger/708954.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 54124 - hot_and_sour_soup/1202090.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 54270 - hot_and_sour_soup/1624423.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 54422 - hot_and_sour_soup/227324.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 54572 - hot_and_sour_soup/2783901.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 54722 - hot_and_sour_soup/3319686.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 54883 - hot_and_sour_soup/449812.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% 55033 - hot_and_sour_soup/942430.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% 55185\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 54% 55342 - hot_dog/1944978.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% 55486 - hot_dog/2488519.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% 55647 - hot_dog/3264272.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% 55786 - hot_dog/3844876.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% 55924 - hot_dog/642331.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 56095\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 55% 56212 - huevos_rancheros/1501442.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 56353 - huevos_rancheros/2022323.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 56488 - huevos_rancheros/254613.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 56622 - huevos_rancheros/3000534.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 56764 - huevos_rancheros/3574628.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 56896 - huevos_rancheros/531423.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 57032 - huevos_rancheros/935838.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 57190\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 56% 57333 - hummus/2045125.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 57473 - hummus/2603010.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 57617 - hummus/3086599.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 57769 - hummus/3590863.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 57921 - hummus/596821.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% 58084 - ice_cream/1161150.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% 58225\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 57% 58374 - ice_cream/2146803.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% 58520 - ice_cream/2569140.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% 58674 - ice_cream/3305807.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% 58830 - ice_cream/390886.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% 58974 - ice_cream/725512.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 59133 - lasagna/1309938.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 59270 - lasagna/1814146.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 59404 - lasagna/2316480.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 59548 - lasagna/2857779.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 59682 - lasagna/3262130.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 59817 - lasagna/3636608.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 59958 - lasagna/554376.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 60125 - lobster_bisque/1224865.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 60287 - lobster_bisque/1873122.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 60456 - lobster_bisque/2468039.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 60632 - lobster_bisque/2995983.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 60806 - lobster_bisque/3527965.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 60987 - lobster_bisque/684948.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 61154 - lobster_roll_sandwich/1361258.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 61288 - lobster_roll_sandwich/1821009.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 61424 - lobster_roll_sandwich/219166.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 61549\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 60% 61681 - lobster_roll_sandwich/3586162.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 61820 - lobster_roll_sandwich/635673.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 61954 - lobster_roll_sandwich/785005.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 62094 - macaroni_and_cheese/111620.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% 62239 - macaroni_and_cheese/1614052.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% 62386 - macaroni_and_cheese/2132996.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% 62535 - macaroni_and_cheese/2663143.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% 62679\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 61% 62836 - macaroni_and_cheese/3812599.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% 62990 - macaroni_and_cheese/767127.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% 63138 - macarons/1301530.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 63285 - macarons/1770415.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 63428 - macarons/2272089.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 63577\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 62% 63732 - macarons/3384641.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 63885 - macarons/461265.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 64025 - macarons/893596.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 64193 - miso_soup/1426270.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% 64367\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 63% 64531\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 63% 64708 - miso_soup/3195966.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% 64882 - miso_soup/3894245.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% 65056 - miso_soup/980263.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% 65200 - mussels/1503438.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% 65339 - mussels/2089032.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 65465 - mussels/2528407.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 65592 - mussels/301463.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 65733 - mussels/3418122.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 65865 - mussels/383903.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 65994 - mussels/719903.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 66135\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 64% 66274 - nachos/1754283.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 66402 - nachos/2242753.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 66529 - nachos/2717275.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 66659 - nachos/319520.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 66794 - nachos/3570393.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 66924 - nachos/527996.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 67058 - nachos/99050.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 67201 - omelette/1542679.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 67327 - omelette/1979759.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 67465 - omelette/2417622.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 67605 - omelette/2938281.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 67745 - omelette/3473916.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 67882 - omelette/444822.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 68015 - omelette/766716.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 68180 - onion_rings/1382661.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 68321 - onion_rings/1886568.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% 68462 - onion_rings/2387754.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% 68610 - onion_rings/2887719.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% 68758 - onion_rings/3390568.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% 68900 - onion_rings/450303.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% 69039\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 67% 69189 - oysters/1522355.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% 69309 - oysters/1957306.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 69436 - oysters/2494314.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 69564 - oysters/2936119.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 69695 - oysters/3333406.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 69827 - oysters/3734101.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 69956 - oysters/620700.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 70109 - pad_thai/1149119.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 70224 - pad_thai/1473039.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 70348 - pad_thai/1888788.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 70477 - pad_thai/2308686.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 70601 - pad_thai/2737484.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 70727 - pad_thai/3265128.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 70862 - pad_thai/3825803.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 70991 - pad_thai/699940.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 71114 - paella/1177036.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 71244 - paella/1575993.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 71366 - paella/1960401.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 71378 - paella/2026438.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 71501 - paella/2456775.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 71642 - paella/2969655.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 71763 - paella/3432423.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 71872 - paella/3774465.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 72011 - paella/763373.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 72160 - pancakes/1349450.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 72265 - pancakes/1757383.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 72401 - pancakes/2265781.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 72565 - pancakes/2933412.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 72697 - pancakes/3350595.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 72815 - pancakes/3761339.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 72973 - pancakes/708334.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% 73114\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 72% 73277 - panna_cotta/16976.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% 73447 - panna_cotta/231860.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% 73627 - panna_cotta/2896272.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% 73792 - panna_cotta/3517381.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% 73960 - panna_cotta/531678.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% 74135 - peking_duck/1204839.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 74277 - peking_duck/1704386.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 74383 - peking_duck/211584.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 74552 - peking_duck/273947.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 74687 - peking_duck/3140919.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 74817 - peking_duck/3676141.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 74937 - peking_duck/535775.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 75129 - pho/1151446.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 75257 - pho/159768.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 75357 - pho/2055313.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 75519 - pho/2742214.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 75661 - pho/317654.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 75794 - pho/3741977.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 75927 - pho/570845.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 76059\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 75% 76179 - pizza/1351631.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% 76304 - pizza/1881674.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% 76443 - pizza/2331467.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% 76559 - pizza/2723529.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% 76675\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 75% 76818 - pizza/3784357.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% 76934 - pizza/63480.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 77043 - pizza/918506.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 77179 - pork_chop/1330802.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 77344 - pork_chop/2019026.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 77462 - pork_chop/2431253.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 77598 - pork_chop/2899150.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 77754 - pork_chop/346349.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 77887\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 77% 78013 - pork_chop/760637.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 78159\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 77% 78329 - poutine/1895233.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 78430 - poutine/230239.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 78567 - poutine/2769954.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 78725 - poutine/3448160.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 78857 - poutine/3857482.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 78988 - poutine/699064.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 79147 - prime_rib/1168261.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 79273 - prime_rib/1564514.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 79374 - prime_rib/1933441.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 79536 - prime_rib/2673030.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 79674 - prime_rib/3095715.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 79788 - prime_rib/3526979.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% 79921 - prime_rib/53393.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% 80069\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 79% 80214 - pulled_pork_sandwich/1449171.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% 80323 - pulled_pork_sandwich/1793584.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% 80479\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 79% 80614 - pulled_pork_sandwich/2785941.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% 80738 - pulled_pork_sandwich/3318245.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% 80871 - pulled_pork_sandwich/455908.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% 81015 - pulled_pork_sandwich/820154.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% 81149\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 80% 81289 - ramen/1743752.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% 81455 - ramen/2416462.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% 81588\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 80% 81698 - ramen/3270629.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 81852 - ramen/3898155.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 81983 - ramen/63127.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 82110 - ravioli/1088753.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 82251 - ravioli/1553608.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 82430 - ravioli/2242446.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 82575 - ravioli/2725271.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 82711 - ravioli/3193252.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 82879 - ravioli/3835711.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 83024 - ravioli/793377.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 83160 - red_velvet_cake/1271257.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 83334 - red_velvet_cake/1867849.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 83463 - red_velvet_cake/2229825.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 83595 - red_velvet_cake/2681237.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 83736 - red_velvet_cake/3093897.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 83894 - red_velvet_cake/372523.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 84038 - red_velvet_cake/798716.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 84186 - risotto/1398886.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 84365 - risotto/2036021.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 84492 - risotto/2441986.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 84645 - risotto/2980898.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 84780 - risotto/3425852.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 84947 - risotto/507345.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 85104 - samosa/1053873.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 85232 - samosa/1598130.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 85372 - samosa/2157234.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 85522 - samosa/2634281.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 85656 - samosa/303715.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 85830 - samosa/3686755.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 85983\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 85% 86119 - sashimi/1104720.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 86262 - sashimi/1607096.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 86433 - sashimi/2160425.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 86554\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 85% 86695 - sashimi/3057527.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 86862 - sashimi/3694198.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 86993 - sashimi/662582.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 87164\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 86% 87313 - scallops/1819119.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 87457 - scallops/2377722.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 87591 - scallops/2916280.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 87760 - scallops/3479526.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 87908 - scallops/425100.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 88058\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 87% 88184 - seaweed_salad/1326835.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 88342 - seaweed_salad/1859812.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 88460 - seaweed_salad/2237921.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 88571 - seaweed_salad/2684780.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 88722 - seaweed_salad/3278389.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 88849 - seaweed_salad/3768649.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 88964 - seaweed_salad/640622.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 89128 - shrimp_and_grits/1115568.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 89276 - shrimp_and_grits/1658840.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 89413 - shrimp_and_grits/2166512.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 89529 - shrimp_and_grits/2571099.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 89694 - shrimp_and_grits/3129300.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 89837 - shrimp_and_grits/3676580.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 89963 - shrimp_and_grits/618011.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 90116 - spaghetti_bolognese/1104453.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 90277 - spaghetti_bolognese/1701293.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 90379 - spaghetti_bolognese/204901.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 90512 - spaghetti_bolognese/2495059.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 90669 - spaghetti_bolognese/3011412.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 90791 - spaghetti_bolognese/3425652.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 90905 - spaghetti_bolognese/3815350.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 91045 - spaghetti_bolognese/805225.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 91204 - spaghetti_carbonara/144135.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 91314 - spaghetti_carbonara/1766862.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 91454 - spaghetti_carbonara/2226033.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 91623 - spaghetti_carbonara/2776641.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 91757 - spaghetti_carbonara/3210760.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 91886 - spaghetti_carbonara/3781925.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 92042 - spaghetti_carbonara/778614.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 92223 - spring_rolls/1442066.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 92340 - spring_rolls/1829187.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 92544 - spring_rolls/2471532.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 92697 - spring_rolls/3056329.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 92839 - spring_rolls/3664748.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 92996\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 92% 93196\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 92% 93321 - steak/1869467.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 93436 - steak/228683.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 93587\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 92% 93725 - steak/3297938.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 93831 - steak/3671021.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 93971 - steak/606820.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 94161 - strawberry_shortcake/131497.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 94313 - strawberry_shortcake/1847556.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 94439 - strawberry_shortcake/2234323.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 94618 - strawberry_shortcake/2993989.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 94759 - strawberry_shortcake/3599122.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 94902 - strawberry_shortcake/538795.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 95063 - strawberry_shortcake/903589.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 95247 - sushi/1456211.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 95360 - sushi/1864809.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 95497 - sushi/2414653.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 95656 - sushi/3045664.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 95781 - sushi/3570543.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 95934 - sushi/584916.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 96079 - sushi/945662.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 96080\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 95% 96218 - tacos/1474360.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 96350 - tacos/1907855.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 96481 - tacos/2361789.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 96622 - tacos/2971659.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 96753 - tacos/346150.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 96883 - tacos/3905539.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 97013 - tacos/718434.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 97145 - takoyaki/114959.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 97270\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b 96% 97398 - takoyaki/2053633.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 97536 - takoyaki/2541299.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 97662 - takoyaki/2966363.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 97789 - takoyaki/3466307.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 97920 - takoyaki/454980.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 98038 - takoyaki/818954.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 98165 - tiramisu/1236136.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 98308 - tiramisu/1780792.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 98448 - tiramisu/2298090.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 98597 - tiramisu/2807556.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 98741 - tiramisu/3304053.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 98889 - tiramisu/3742123.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 99037 - tiramisu/801669.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 99180 - tuna_tartare/1392968.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 99339 - tuna_tartare/1937188.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 99489 - tuna_tartare/2531864.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 99644 - tuna_tartare/3119513.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 99801 - tuna_tartare/36290.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 99961 - tuna_tartare/549841.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 100128 - waffles/1102436.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 100246 - waffles/1520986.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 100380 - waffles/1927240.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 100521 - waffles/245977.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 100659 - waffles/2999331.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 100796 - waffles/3446096.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 100942 - waffles/520878.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 101075 - waffles/923472.jpg\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Folders: 101\n",
            "Files: 101000\n",
            "Size:       5116621947\n",
            "Compressed: 5132666035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RSCOdfZKNbl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c1a06d-d4df-41c1-81aa-16aa796230cd"
      },
      "source": [
        "!pip install split-folders\r\n",
        "import splitfolders\r\n",
        "splitfolders.ratio('/content/images', output=\"output\", seed=1337, ratio=(.7, .2, .1)) "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.6/dist-packages (0.4.3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Copying files: 0 files [00:00, ? files/s]\u001b[A\n",
            "Copying files: 30 files [00:00, 297.13 files/s]\u001b[A\n",
            "Copying files: 69 files [00:00, 319.02 files/s]\u001b[A\n",
            "Copying files: 115 files [00:00, 351.17 files/s]\u001b[A\n",
            "Copying files: 156 files [00:00, 366.65 files/s]\u001b[A\n",
            "Copying files: 211 files [00:00, 407.25 files/s]\u001b[A\n",
            "Copying files: 271 files [00:00, 450.52 files/s]\u001b[A\n",
            "Copying files: 353 files [00:00, 520.43 files/s]\u001b[A\n",
            "Copying files: 424 files [00:00, 562.79 files/s]\u001b[A\n",
            "Copying files: 511 files [00:00, 628.68 files/s]\u001b[A\n",
            "Copying files: 579 files [00:01, 619.67 files/s]\u001b[A\n",
            "Copying files: 675 files [00:01, 626.95 files/s]\u001b[A\n",
            "Copying files: 745 files [00:01, 647.16 files/s]\u001b[A\n",
            "Copying files: 819 files [00:01, 666.34 files/s]\u001b[A\n",
            "Copying files: 899 files [00:01, 698.83 files/s]\u001b[A\n",
            "Copying files: 971 files [00:01, 704.89 files/s]\u001b[A\n",
            "Copying files: 1102 files [00:01, 818.21 files/s]\u001b[A\n",
            "Copying files: 1224 files [00:01, 907.65 files/s]\u001b[A\n",
            "Copying files: 1376 files [00:01, 1032.16 files/s]\u001b[A\n",
            "Copying files: 1497 files [00:01, 1079.53 files/s]\u001b[A\n",
            "Copying files: 1614 files [00:02, 1074.78 files/s]\u001b[A\n",
            "Copying files: 1768 files [00:02, 1181.47 files/s]\u001b[A\n",
            "Copying files: 1894 files [00:02, 1118.50 files/s]\u001b[A\n",
            "Copying files: 2012 files [00:02, 843.86 files/s] \u001b[A\n",
            "Copying files: 2111 files [00:02, 636.96 files/s]\u001b[A\n",
            "Copying files: 2193 files [00:02, 590.27 files/s]\u001b[A\n",
            "Copying files: 2265 files [00:03, 572.75 files/s]\u001b[A\n",
            "Copying files: 2332 files [00:03, 583.92 files/s]\u001b[A\n",
            "Copying files: 2398 files [00:03, 603.95 files/s]\u001b[A\n",
            "Copying files: 2464 files [00:03, 618.48 files/s]\u001b[A\n",
            "Copying files: 2530 files [00:03, 621.71 files/s]\u001b[A\n",
            "Copying files: 2595 files [00:03, 624.07 files/s]\u001b[A\n",
            "Copying files: 2677 files [00:03, 671.87 files/s]\u001b[A\n",
            "Copying files: 2747 files [00:03, 663.59 files/s]\u001b[A\n",
            "Copying files: 2825 files [00:03, 584.03 files/s]\u001b[A\n",
            "Copying files: 2891 files [00:04, 604.87 files/s]\u001b[A\n",
            "Copying files: 2954 files [00:04, 608.96 files/s]\u001b[A\n",
            "Copying files: 3375 files [00:04, 819.15 files/s]\u001b[A\n",
            "Copying files: 3768 files [00:04, 1074.22 files/s]\u001b[A\n",
            "Copying files: 4005 files [00:04, 1159.55 files/s]\u001b[A\n",
            "Copying files: 4214 files [00:05, 776.47 files/s] \u001b[A\n",
            "Copying files: 4374 files [00:05, 735.48 files/s]\u001b[A\n",
            "Copying files: 4506 files [00:05, 725.23 files/s]\u001b[A\n",
            "Copying files: 4619 files [00:05, 758.95 files/s]\u001b[A\n",
            "Copying files: 4724 files [00:05, 721.26 files/s]\u001b[A\n",
            "Copying files: 4817 files [00:05, 769.13 files/s]\u001b[A\n",
            "Copying files: 4910 files [00:05, 781.15 files/s]\u001b[A\n",
            "Copying files: 4999 files [00:06, 669.89 files/s]\u001b[A\n",
            "Copying files: 5077 files [00:06, 528.88 files/s]\u001b[A\n",
            "Copying files: 5142 files [00:06, 501.91 files/s]\u001b[A\n",
            "Copying files: 5201 files [00:06, 501.03 files/s]\u001b[A\n",
            "Copying files: 5262 files [00:06, 528.96 files/s]\u001b[A\n",
            "Copying files: 5328 files [00:06, 561.14 files/s]\u001b[A\n",
            "Copying files: 5390 files [00:06, 574.30 files/s]\u001b[A\n",
            "Copying files: 5459 files [00:07, 595.16 files/s]\u001b[A\n",
            "Copying files: 5529 files [00:07, 619.21 files/s]\u001b[A\n",
            "Copying files: 5604 files [00:07, 650.07 files/s]\u001b[A\n",
            "Copying files: 5674 files [00:07, 663.80 files/s]\u001b[A\n",
            "Copying files: 5742 files [00:07, 665.92 files/s]\u001b[A\n",
            "Copying files: 5814 files [00:07, 680.90 files/s]\u001b[A\n",
            "Copying files: 5883 files [00:07, 674.35 files/s]\u001b[A\n",
            "Copying files: 5964 files [00:07, 709.40 files/s]\u001b[A\n",
            "Copying files: 6037 files [00:07, 715.38 files/s]\u001b[A\n",
            "Copying files: 6159 files [00:07, 816.68 files/s]\u001b[A\n",
            "Copying files: 6247 files [00:08, 566.06 files/s]\u001b[A\n",
            "Copying files: 6318 files [00:08, 590.43 files/s]\u001b[A\n",
            "Copying files: 6433 files [00:08, 691.14 files/s]\u001b[A\n",
            "Copying files: 6516 files [00:08, 712.16 files/s]\u001b[A\n",
            "Copying files: 6618 files [00:08, 658.07 files/s]\u001b[A\n",
            "Copying files: 6708 files [00:08, 715.64 files/s]\u001b[A\n",
            "Copying files: 6787 files [00:08, 730.10 files/s]\u001b[A\n",
            "Copying files: 6879 files [00:09, 778.16 files/s]\u001b[A\n",
            "Copying files: 6962 files [00:09, 785.30 files/s]\u001b[A\n",
            "Copying files: 7044 files [00:09, 670.78 files/s]\u001b[A\n",
            "Copying files: 7117 files [00:09, 589.00 files/s]\u001b[A\n",
            "Copying files: 7182 files [00:09, 590.80 files/s]\u001b[A\n",
            "Copying files: 7245 files [00:09, 564.11 files/s]\u001b[A\n",
            "Copying files: 7306 files [00:09, 574.84 files/s]\u001b[A\n",
            "Copying files: 7386 files [00:09, 626.69 files/s]\u001b[A\n",
            "Copying files: 7470 files [00:10, 634.62 files/s]\u001b[A\n",
            "Copying files: 7543 files [00:10, 655.71 files/s]\u001b[A\n",
            "Copying files: 7622 files [00:10, 690.92 files/s]\u001b[A\n",
            "Copying files: 7693 files [00:10, 684.79 files/s]\u001b[A\n",
            "Copying files: 7784 files [00:10, 739.63 files/s]\u001b[A\n",
            "Copying files: 7861 files [00:10, 732.38 files/s]\u001b[A\n",
            "Copying files: 7936 files [00:10, 721.87 files/s]\u001b[A\n",
            "Copying files: 8010 files [00:10, 668.70 files/s]\u001b[A\n",
            "Copying files: 8079 files [00:10, 488.31 files/s]\u001b[A\n",
            "Copying files: 8136 files [00:11, 438.05 files/s]\u001b[A\n",
            "Copying files: 8187 files [00:11, 442.86 files/s]\u001b[A\n",
            "Copying files: 8246 files [00:11, 478.59 files/s]\u001b[A\n",
            "Copying files: 8308 files [00:11, 512.22 files/s]\u001b[A\n",
            "Copying files: 8381 files [00:11, 560.63 files/s]\u001b[A\n",
            "Copying files: 8466 files [00:11, 624.23 files/s]\u001b[A\n",
            "Copying files: 8547 files [00:11, 607.43 files/s]\u001b[A\n",
            "Copying files: 8626 files [00:11, 651.77 files/s]\u001b[A\n",
            "Copying files: 8695 files [00:12, 659.35 files/s]\u001b[A\n",
            "Copying files: 8771 files [00:12, 686.16 files/s]\u001b[A\n",
            "Copying files: 8845 files [00:12, 701.28 files/s]\u001b[A\n",
            "Copying files: 8918 files [00:12, 709.00 files/s]\u001b[A\n",
            "Copying files: 8994 files [00:12, 723.08 files/s]\u001b[A\n",
            "Copying files: 9616 files [00:12, 983.92 files/s]\u001b[A\n",
            "Copying files: 10010 files [00:12, 1265.94 files/s]\u001b[A\n",
            "Copying files: 10292 files [00:13, 845.17 files/s] \u001b[A\n",
            "Copying files: 10504 files [00:13, 798.00 files/s]\u001b[A\n",
            "Copying files: 10673 files [00:13, 752.81 files/s]\u001b[A\n",
            "Copying files: 10812 files [00:13, 783.27 files/s]\u001b[A\n",
            "Copying files: 10935 files [00:14, 730.49 files/s]\u001b[A\n",
            "Copying files: 11040 files [00:14, 631.73 files/s]\u001b[A\n",
            "Copying files: 11128 files [00:14, 545.93 files/s]\u001b[A\n",
            "Copying files: 11202 files [00:14, 558.93 files/s]\u001b[A\n",
            "Copying files: 11272 files [00:14, 562.88 files/s]\u001b[A\n",
            "Copying files: 11357 files [00:14, 626.00 files/s]\u001b[A\n",
            "Copying files: 11436 files [00:14, 666.68 files/s]\u001b[A\n",
            "Copying files: 11521 files [00:15, 710.13 files/s]\u001b[A\n",
            "Copying files: 11610 files [00:15, 754.48 files/s]\u001b[A\n",
            "Copying files: 11698 files [00:15, 787.86 files/s]\u001b[A\n",
            "Copying files: 11790 files [00:15, 822.83 files/s]\u001b[A\n",
            "Copying files: 11876 files [00:15, 813.79 files/s]\u001b[A\n",
            "Copying files: 11979 files [00:15, 867.85 files/s]\u001b[A\n",
            "Copying files: 12069 files [00:15, 642.35 files/s]\u001b[A\n",
            "Copying files: 12144 files [00:15, 581.48 files/s]\u001b[A\n",
            "Copying files: 12211 files [00:16, 577.51 files/s]\u001b[A\n",
            "Copying files: 12275 files [00:16, 583.42 files/s]\u001b[A\n",
            "Copying files: 12349 files [00:16, 622.65 files/s]\u001b[A\n",
            "Copying files: 12424 files [00:16, 655.10 files/s]\u001b[A\n",
            "Copying files: 12515 files [00:16, 712.79 files/s]\u001b[A\n",
            "Copying files: 12590 files [00:16, 710.73 files/s]\u001b[A\n",
            "Copying files: 12679 files [00:16, 756.00 files/s]\u001b[A\n",
            "Copying files: 12758 files [00:16, 754.31 files/s]\u001b[A\n",
            "Copying files: 12840 files [00:16, 653.45 files/s]\u001b[A\n",
            "Copying files: 12913 files [00:17, 674.31 files/s]\u001b[A\n",
            "Copying files: 12989 files [00:17, 697.91 files/s]\u001b[A\n",
            "Copying files: 13061 files [00:17, 505.95 files/s]\u001b[A\n",
            "Copying files: 13121 files [00:17, 414.46 files/s]\u001b[A\n",
            "Copying files: 13173 files [00:17, 440.17 files/s]\u001b[A\n",
            "Copying files: 13227 files [00:17, 464.10 files/s]\u001b[A\n",
            "Copying files: 13288 files [00:17, 497.65 files/s]\u001b[A\n",
            "Copying files: 13360 files [00:18, 545.31 files/s]\u001b[A\n",
            "Copying files: 13427 files [00:18, 574.15 files/s]\u001b[A\n",
            "Copying files: 13488 files [00:18, 550.09 files/s]\u001b[A\n",
            "Copying files: 13579 files [00:18, 587.57 files/s]\u001b[A\n",
            "Copying files: 13648 files [00:18, 612.60 files/s]\u001b[A\n",
            "Copying files: 13715 files [00:18, 628.38 files/s]\u001b[A\n",
            "Copying files: 13788 files [00:18, 655.47 files/s]\u001b[A\n",
            "Copying files: 13855 files [00:18, 659.43 files/s]\u001b[A\n",
            "Copying files: 13922 files [00:18, 650.08 files/s]\u001b[A\n",
            "Copying files: 14001 files [00:19, 665.17 files/s]\u001b[A\n",
            "Copying files: 14069 files [00:19, 516.38 files/s]\u001b[A\n",
            "Copying files: 14127 files [00:19, 494.07 files/s]\u001b[A\n",
            "Copying files: 14182 files [00:19, 505.41 files/s]\u001b[A\n",
            "Copying files: 14236 files [00:19, 502.14 files/s]\u001b[A\n",
            "Copying files: 14293 files [00:19, 519.72 files/s]\u001b[A\n",
            "Copying files: 14365 files [00:19, 566.89 files/s]\u001b[A\n",
            "Copying files: 14432 files [00:19, 593.63 files/s]\u001b[A\n",
            "Copying files: 14511 files [00:19, 587.86 files/s]\u001b[A\n",
            "Copying files: 14583 files [00:20, 621.90 files/s]\u001b[A\n",
            "Copying files: 14647 files [00:20, 625.13 files/s]\u001b[A\n",
            "Copying files: 14715 files [00:20, 640.41 files/s]\u001b[A\n",
            "Copying files: 14783 files [00:20, 651.32 files/s]\u001b[A\n",
            "Copying files: 14849 files [00:20, 643.10 files/s]\u001b[A\n",
            "Copying files: 14918 files [00:20, 655.81 files/s]\u001b[A\n",
            "Copying files: 14984 files [00:20, 652.35 files/s]\u001b[A\n",
            "Copying files: 15050 files [00:20, 523.52 files/s]\u001b[A\n",
            "Copying files: 15107 files [00:21, 477.62 files/s]\u001b[A\n",
            "Copying files: 15159 files [00:21, 469.18 files/s]\u001b[A\n",
            "Copying files: 15217 files [00:21, 496.07 files/s]\u001b[A\n",
            "Copying files: 15273 files [00:21, 512.87 files/s]\u001b[A\n",
            "Copying files: 15344 files [00:21, 558.00 files/s]\u001b[A\n",
            "Copying files: 15415 files [00:21, 594.30 files/s]\u001b[A\n",
            "Copying files: 15480 files [00:21, 609.82 files/s]\u001b[A\n",
            "Copying files: 15558 files [00:21, 593.32 files/s]\u001b[A\n",
            "Copying files: 15624 files [00:21, 611.55 files/s]\u001b[A\n",
            "Copying files: 15689 files [00:21, 622.06 files/s]\u001b[A\n",
            "Copying files: 15756 files [00:22, 632.56 files/s]\u001b[A\n",
            "Copying files: 15820 files [00:22, 632.06 files/s]\u001b[A\n",
            "Copying files: 15895 files [00:22, 660.48 files/s]\u001b[A\n",
            "Copying files: 15962 files [00:22, 661.02 files/s]\u001b[A\n",
            "Copying files: 16029 files [00:22, 635.39 files/s]\u001b[A\n",
            "Copying files: 16094 files [00:22, 556.51 files/s]\u001b[A\n",
            "Copying files: 16152 files [00:22, 528.38 files/s]\u001b[A\n",
            "Copying files: 16212 files [00:22, 545.42 files/s]\u001b[A\n",
            "Copying files: 16278 files [00:22, 573.76 files/s]\u001b[A\n",
            "Copying files: 16358 files [00:23, 624.88 files/s]\u001b[A\n",
            "Copying files: 16441 files [00:23, 670.06 files/s]\u001b[A\n",
            "Copying files: 16520 files [00:23, 701.37 files/s]\u001b[A\n",
            "Copying files: 16608 files [00:23, 746.68 files/s]\u001b[A\n",
            "Copying files: 16686 files [00:23, 755.71 files/s]\u001b[A\n",
            "Copying files: 16780 files [00:23, 802.61 files/s]\u001b[A\n",
            "Copying files: 16863 files [00:23, 790.38 files/s]\u001b[A\n",
            "Copying files: 16951 files [00:23, 813.87 files/s]\u001b[A\n",
            "Copying files: 17034 files [00:23, 645.80 files/s]\u001b[A\n",
            "Copying files: 17105 files [00:24, 544.94 files/s]\u001b[A\n",
            "Copying files: 17167 files [00:24, 516.67 files/s]\u001b[A\n",
            "Copying files: 17226 files [00:24, 534.34 files/s]\u001b[A\n",
            "Copying files: 17284 files [00:24, 541.81 files/s]\u001b[A\n",
            "Copying files: 17359 files [00:24, 590.85 files/s]\u001b[A\n",
            "Copying files: 17427 files [00:24, 613.28 files/s]\u001b[A\n",
            "Copying files: 17519 files [00:24, 681.43 files/s]\u001b[A\n",
            "Copying files: 17592 files [00:24, 684.25 files/s]\u001b[A\n",
            "Copying files: 17680 files [00:25, 732.81 files/s]\u001b[A\n",
            "Copying files: 17771 files [00:25, 778.03 files/s]\u001b[A\n",
            "Copying files: 17858 files [00:25, 803.13 files/s]\u001b[A\n",
            "Copying files: 17941 files [00:25, 664.18 files/s]\u001b[A\n",
            "Copying files: 18013 files [00:25, 660.92 files/s]\u001b[A\n",
            "Copying files: 18083 files [00:25, 551.48 files/s]\u001b[A\n",
            "Copying files: 18144 files [00:25, 523.38 files/s]\u001b[A\n",
            "Copying files: 18201 files [00:25, 533.57 files/s]\u001b[A\n",
            "Copying files: 18265 files [00:26, 558.60 files/s]\u001b[A\n",
            "Copying files: 18340 files [00:26, 602.76 files/s]\u001b[A\n",
            "Copying files: 18414 files [00:26, 636.20 files/s]\u001b[A\n",
            "Copying files: 18498 files [00:26, 685.10 files/s]\u001b[A\n",
            "Copying files: 18570 files [00:26, 670.45 files/s]\u001b[A\n",
            "Copying files: 18669 files [00:26, 741.98 files/s]\u001b[A\n",
            "Copying files: 18755 files [00:26, 665.62 files/s]\u001b[A\n",
            "Copying files: 18829 files [00:26, 686.25 files/s]\u001b[A\n",
            "Copying files: 18901 files [00:26, 685.47 files/s]\u001b[A\n",
            "Copying files: 18984 files [00:26, 722.56 files/s]\u001b[A\n",
            "Copying files: 19059 files [00:27, 533.96 files/s]\u001b[A\n",
            "Copying files: 19121 files [00:27, 495.07 files/s]\u001b[A\n",
            "Copying files: 19178 files [00:27, 489.47 files/s]\u001b[A\n",
            "Copying files: 19235 files [00:27, 505.38 files/s]\u001b[A\n",
            "Copying files: 19296 files [00:27, 532.65 files/s]\u001b[A\n",
            "Copying files: 19353 files [00:27, 540.04 files/s]\u001b[A\n",
            "Copying files: 19410 files [00:27, 545.40 files/s]\u001b[A\n",
            "Copying files: 19476 files [00:27, 572.86 files/s]\u001b[A\n",
            "Copying files: 19535 files [00:28, 290.44 files/s]\u001b[A\n",
            "Copying files: 19594 files [00:28, 342.18 files/s]\u001b[A\n",
            "Copying files: 19660 files [00:28, 399.20 files/s]\u001b[A\n",
            "Copying files: 19724 files [00:28, 449.90 files/s]\u001b[A\n",
            "Copying files: 19796 files [00:28, 506.77 files/s]\u001b[A\n",
            "Copying files: 19858 files [00:28, 526.03 files/s]\u001b[A\n",
            "Copying files: 19923 files [00:29, 557.77 files/s]\u001b[A\n",
            "Copying files: 19995 files [00:29, 598.04 files/s]\u001b[A\n",
            "Copying files: 20060 files [00:29, 456.62 files/s]\u001b[A\n",
            "Copying files: 20115 files [00:29, 413.38 files/s]\u001b[A\n",
            "Copying files: 20164 files [00:29, 433.38 files/s]\u001b[A\n",
            "Copying files: 20220 files [00:29, 463.54 files/s]\u001b[A\n",
            "Copying files: 20273 files [00:29, 478.52 files/s]\u001b[A\n",
            "Copying files: 20345 files [00:29, 529.65 files/s]\u001b[A\n",
            "Copying files: 20418 files [00:30, 576.34 files/s]\u001b[A\n",
            "Copying files: 20490 files [00:30, 612.47 files/s]\u001b[A\n",
            "Copying files: 20555 files [00:30, 612.49 files/s]\u001b[A\n",
            "Copying files: 20629 files [00:30, 643.23 files/s]\u001b[A\n",
            "Copying files: 20705 files [00:30, 674.13 files/s]\u001b[A\n",
            "Copying files: 20778 files [00:30, 689.49 files/s]\u001b[A\n",
            "Copying files: 20860 files [00:30, 723.85 files/s]\u001b[A\n",
            "Copying files: 20934 files [00:30, 700.87 files/s]\u001b[A\n",
            "Copying files: 21006 files [00:30, 661.75 files/s]\u001b[A\n",
            "Copying files: 21074 files [00:31, 505.16 files/s]\u001b[A\n",
            "Copying files: 21131 files [00:31, 481.45 files/s]\u001b[A\n",
            "Copying files: 21184 files [00:31, 477.98 files/s]\u001b[A\n",
            "Copying files: 21240 files [00:31, 499.88 files/s]\u001b[A\n",
            "Copying files: 21300 files [00:31, 524.50 files/s]\u001b[A\n",
            "Copying files: 21355 files [00:31, 484.38 files/s]\u001b[A\n",
            "Copying files: 21421 files [00:31, 524.44 files/s]\u001b[A\n",
            "Copying files: 21516 files [00:31, 605.65 files/s]\u001b[A\n",
            "Copying files: 21583 files [00:31, 611.20 files/s]\u001b[A\n",
            "Copying files: 21680 files [00:32, 622.17 files/s]\u001b[A\n",
            "Copying files: 21754 files [00:32, 653.36 files/s]\u001b[A\n",
            "Copying files: 21829 files [00:32, 679.51 files/s]\u001b[A\n",
            "Copying files: 21904 files [00:32, 696.15 files/s]\u001b[A\n",
            "Copying files: 21985 files [00:32, 726.26 files/s]\u001b[A\n",
            "Copying files: 22381 files [00:32, 961.85 files/s]\u001b[A\n",
            "Copying files: 23001 files [00:32, 1280.39 files/s]\u001b[A\n",
            "Copying files: 23290 files [00:33, 636.71 files/s] \u001b[A\n",
            "Copying files: 23501 files [00:34, 654.68 files/s]\u001b[A\n",
            "Copying files: 23670 files [00:34, 638.03 files/s]\u001b[A\n",
            "Copying files: 23807 files [00:34, 649.30 files/s]\u001b[A\n",
            "Copying files: 23923 files [00:34, 632.32 files/s]\u001b[A\n",
            "Copying files: 24022 files [00:34, 608.65 files/s]\u001b[A\n",
            "Copying files: 24108 files [00:35, 534.04 files/s]\u001b[A\n",
            "Copying files: 24181 files [00:35, 523.32 files/s]\u001b[A\n",
            "Copying files: 24247 files [00:35, 536.64 files/s]\u001b[A\n",
            "Copying files: 24311 files [00:35, 553.03 files/s]\u001b[A\n",
            "Copying files: 24384 files [00:35, 596.01 files/s]\u001b[A\n",
            "Copying files: 24455 files [00:35, 623.68 files/s]\u001b[A\n",
            "Copying files: 24530 files [00:35, 654.75 files/s]\u001b[A\n",
            "Copying files: 24599 files [00:35, 640.67 files/s]\u001b[A\n",
            "Copying files: 24666 files [00:35, 599.19 files/s]\u001b[A\n",
            "Copying files: 24734 files [00:36, 621.10 files/s]\u001b[A\n",
            "Copying files: 24802 files [00:36, 628.99 files/s]\u001b[A\n",
            "Copying files: 24872 files [00:36, 648.12 files/s]\u001b[A\n",
            "Copying files: 24938 files [00:36, 649.12 files/s]\u001b[A\n",
            "Copying files: 25004 files [00:36, 607.74 files/s]\u001b[A\n",
            "Copying files: 25066 files [00:36, 522.55 files/s]\u001b[A\n",
            "Copying files: 25122 files [00:36, 485.56 files/s]\u001b[A\n",
            "Copying files: 25174 files [00:36, 487.00 files/s]\u001b[A\n",
            "Copying files: 25234 files [00:37, 513.15 files/s]\u001b[A\n",
            "Copying files: 25290 files [00:37, 523.97 files/s]\u001b[A\n",
            "Copying files: 25363 files [00:37, 571.18 files/s]\u001b[A\n",
            "Copying files: 25437 files [00:37, 613.10 files/s]\u001b[A\n",
            "Copying files: 25522 files [00:37, 668.18 files/s]\u001b[A\n",
            "Copying files: 25592 files [00:37, 624.62 files/s]\u001b[A\n",
            "Copying files: 25689 files [00:37, 648.42 files/s]\u001b[A\n",
            "Copying files: 25761 files [00:37, 668.01 files/s]\u001b[A\n",
            "Copying files: 25830 files [00:37, 670.87 files/s]\u001b[A\n",
            "Copying files: 25899 files [00:38, 664.48 files/s]\u001b[A\n",
            "Copying files: 25993 files [00:38, 728.38 files/s]\u001b[A\n",
            "Copying files: 26571 files [00:38, 987.14 files/s]\u001b[A\n",
            "Copying files: 27003 files [00:38, 1281.54 files/s]\u001b[A\n",
            "Copying files: 27288 files [00:38, 819.22 files/s] \u001b[A\n",
            "Copying files: 27501 files [00:39, 806.09 files/s]\u001b[A\n",
            "Copying files: 27674 files [00:39, 819.65 files/s]\u001b[A\n",
            "Copying files: 27821 files [00:39, 795.26 files/s]\u001b[A\n",
            "Copying files: 27946 files [00:39, 747.29 files/s]\u001b[A\n",
            "Copying files: 28053 files [00:40, 671.65 files/s]\u001b[A\n",
            "Copying files: 28145 files [00:40, 593.29 files/s]\u001b[A\n",
            "Copying files: 28223 files [00:40, 580.48 files/s]\u001b[A\n",
            "Copying files: 28294 files [00:40, 582.31 files/s]\u001b[A\n",
            "Copying files: 28367 files [00:40, 617.49 files/s]\u001b[A\n",
            "Copying files: 28450 files [00:40, 668.68 files/s]\u001b[A\n",
            "Copying files: 28528 files [00:40, 643.47 files/s]\u001b[A\n",
            "Copying files: 28599 files [00:40, 658.54 files/s]\u001b[A\n",
            "Copying files: 28673 files [00:41, 676.83 files/s]\u001b[A\n",
            "Copying files: 28753 files [00:41, 707.82 files/s]\u001b[A\n",
            "Copying files: 28826 files [00:41, 699.44 files/s]\u001b[A\n",
            "Copying files: 28906 files [00:41, 725.76 files/s]\u001b[A\n",
            "Copying files: 28994 files [00:41, 763.31 files/s]\u001b[A\n",
            "Copying files: 29072 files [00:41, 564.07 files/s]\u001b[A\n",
            "Copying files: 29137 files [00:41, 506.44 files/s]\u001b[A\n",
            "Copying files: 29195 files [00:41, 505.64 files/s]\u001b[A\n",
            "Copying files: 29257 files [00:42, 533.72 files/s]\u001b[A\n",
            "Copying files: 29315 files [00:42, 513.01 files/s]\u001b[A\n",
            "Copying files: 29370 files [00:42, 518.34 files/s]\u001b[A\n",
            "Copying files: 29427 files [00:42, 532.20 files/s]\u001b[A\n",
            "Copying files: 29502 files [00:42, 579.86 files/s]\u001b[A\n",
            "Copying files: 29563 files [00:42, 588.53 files/s]\u001b[A\n",
            "Copying files: 29647 files [00:42, 569.70 files/s]\u001b[A\n",
            "Copying files: 29712 files [00:42, 587.87 files/s]\u001b[A\n",
            "Copying files: 29775 files [00:42, 599.61 files/s]\u001b[A\n",
            "Copying files: 29843 files [00:43, 621.24 files/s]\u001b[A\n",
            "Copying files: 29906 files [00:43, 595.54 files/s]\u001b[A\n",
            "Copying files: 30001 files [00:43, 658.38 files/s]\u001b[A\n",
            "Copying files: 30565 files [00:43, 895.71 files/s]\u001b[A\n",
            "Copying files: 31001 files [00:43, 1162.72 files/s]\u001b[A\n",
            "Copying files: 31458 files [00:43, 1497.69 files/s]\u001b[A\n",
            "Copying files: 31855 files [00:43, 1841.37 files/s]\u001b[A\n",
            "Copying files: 32201 files [00:44, 1239.37 files/s]\u001b[A\n",
            "Copying files: 32464 files [00:44, 975.47 files/s] \u001b[A\n",
            "Copying files: 32668 files [00:44, 880.24 files/s]\u001b[A\n",
            "Copying files: 32833 files [00:45, 861.40 files/s]\u001b[A\n",
            "Copying files: 32973 files [00:45, 803.35 files/s]\u001b[A\n",
            "Copying files: 33092 files [00:45, 589.89 files/s]\u001b[A\n",
            "Copying files: 33186 files [00:45, 568.81 files/s]\u001b[A\n",
            "Copying files: 33268 files [00:45, 580.42 files/s]\u001b[A\n",
            "Copying files: 33344 files [00:45, 616.35 files/s]\u001b[A\n",
            "Copying files: 33419 files [00:46, 649.07 files/s]\u001b[A\n",
            "Copying files: 33494 files [00:46, 659.37 files/s]\u001b[A\n",
            "Copying files: 33567 files [00:46, 667.69 files/s]\u001b[A\n",
            "Copying files: 33659 files [00:46, 700.80 files/s]\u001b[A\n",
            "Copying files: 33740 files [00:46, 725.80 files/s]\u001b[A\n",
            "Copying files: 33828 files [00:46, 764.39 files/s]\u001b[A\n",
            "Copying files: 33907 files [00:46, 771.63 files/s]\u001b[A\n",
            "Copying files: 33990 files [00:46, 787.95 files/s]\u001b[A\n",
            "Copying files: 34071 files [00:47, 605.49 files/s]\u001b[A\n",
            "Copying files: 34139 files [00:47, 545.63 files/s]\u001b[A\n",
            "Copying files: 34200 files [00:47, 550.63 files/s]\u001b[A\n",
            "Copying files: 34260 files [00:47, 543.65 files/s]\u001b[A\n",
            "Copying files: 34321 files [00:47, 547.97 files/s]\u001b[A\n",
            "Copying files: 34386 files [00:47, 574.81 files/s]\u001b[A\n",
            "Copying files: 34457 files [00:47, 600.67 files/s]\u001b[A\n",
            "Copying files: 34533 files [00:47, 639.28 files/s]\u001b[A\n",
            "Copying files: 34607 files [00:47, 663.27 files/s]\u001b[A\n",
            "Copying files: 34682 files [00:48, 685.23 files/s]\u001b[A\n",
            "Copying files: 34752 files [00:48, 683.95 files/s]\u001b[A\n",
            "Copying files: 34831 files [00:48, 712.47 files/s]\u001b[A\n",
            "Copying files: 34904 files [00:48, 715.29 files/s]\u001b[A\n",
            "Copying files: 34984 files [00:48, 738.69 files/s]\u001b[A\n",
            "Copying files: 35059 files [00:48, 615.85 files/s]\u001b[A\n",
            "Copying files: 35125 files [00:48, 543.77 files/s]\u001b[A\n",
            "Copying files: 35184 files [00:48, 534.17 files/s]\u001b[A\n",
            "Copying files: 35241 files [00:48, 522.77 files/s]\u001b[A\n",
            "Copying files: 35308 files [00:49, 558.91 files/s]\u001b[A\n",
            "Copying files: 35390 files [00:49, 616.87 files/s]\u001b[A\n",
            "Copying files: 35472 files [00:49, 665.44 files/s]\u001b[A\n",
            "Copying files: 35547 files [00:49, 687.40 files/s]\u001b[A\n",
            "Copying files: 35620 files [00:49, 633.33 files/s]\u001b[A\n",
            "Copying files: 35695 files [00:49, 660.97 files/s]\u001b[A\n",
            "Copying files: 35774 files [00:49, 694.87 files/s]\u001b[A\n",
            "Copying files: 35846 files [00:49, 696.47 files/s]\u001b[A\n",
            "Copying files: 35924 files [00:49, 715.61 files/s]\u001b[A\n",
            "Copying files: 35997 files [00:50, 716.44 files/s]\u001b[A\n",
            "Copying files: 36430 files [00:50, 955.67 files/s]\u001b[A\n",
            "Copying files: 36694 files [00:50, 1181.84 files/s]\u001b[A\n",
            "Copying files: 37001 files [00:50, 1430.51 files/s]\u001b[A\n",
            "Copying files: 37230 files [00:50, 874.68 files/s] \u001b[A\n",
            "Copying files: 37404 files [00:51, 789.98 files/s]\u001b[A\n",
            "Copying files: 37545 files [00:51, 774.44 files/s]\u001b[A\n",
            "Copying files: 37666 files [00:51, 752.85 files/s]\u001b[A\n",
            "Copying files: 37772 files [00:51, 687.10 files/s]\u001b[A\n",
            "Copying files: 37864 files [00:51, 732.81 files/s]\u001b[A\n",
            "Copying files: 37954 files [00:51, 758.98 files/s]\u001b[A\n",
            "Copying files: 38042 files [00:51, 780.54 files/s]\u001b[A\n",
            "Copying files: 38566 files [00:52, 1047.98 files/s]\u001b[A\n",
            "Copying files: 38791 files [00:52, 1240.97 files/s]\u001b[A\n",
            "Copying files: 39009 files [00:52, 1382.06 files/s]\u001b[A\n",
            "Copying files: 39218 files [00:52, 857.43 files/s] \u001b[A\n",
            "Copying files: 39378 files [00:53, 763.02 files/s]\u001b[A\n",
            "Copying files: 39508 files [00:53, 762.36 files/s]\u001b[A\n",
            "Copying files: 39622 files [00:53, 799.78 files/s]\u001b[A\n",
            "Copying files: 39729 files [00:53, 738.96 files/s]\u001b[A\n",
            "Copying files: 39823 files [00:53, 748.68 files/s]\u001b[A\n",
            "Copying files: 39912 files [00:53, 782.68 files/s]\u001b[A\n",
            "Copying files: 40001 files [00:53, 642.93 files/s]\u001b[A\n",
            "Copying files: 40077 files [00:54, 536.83 files/s]\u001b[A\n",
            "Copying files: 40142 files [00:54, 488.46 files/s]\u001b[A\n",
            "Copying files: 40204 files [00:54, 520.49 files/s]\u001b[A\n",
            "Copying files: 40263 files [00:54, 539.22 files/s]\u001b[A\n",
            "Copying files: 40325 files [00:54, 557.74 files/s]\u001b[A\n",
            "Copying files: 40393 files [00:54, 572.42 files/s]\u001b[A\n",
            "Copying files: 40455 files [00:54, 581.23 files/s]\u001b[A\n",
            "Copying files: 40520 files [00:54, 598.04 files/s]\u001b[A\n",
            "Copying files: 40583 files [00:54, 604.45 files/s]\u001b[A\n",
            "Copying files: 40651 files [00:55, 624.75 files/s]\u001b[A\n",
            "Copying files: 40715 files [00:55, 628.89 files/s]\u001b[A\n",
            "Copying files: 40781 files [00:55, 637.82 files/s]\u001b[A\n",
            "Copying files: 40846 files [00:55, 632.40 files/s]\u001b[A\n",
            "Copying files: 40910 files [00:55, 631.85 files/s]\u001b[A\n",
            "Copying files: 40994 files [00:55, 681.91 files/s]\u001b[A\n",
            "Copying files: 41064 files [00:55, 528.97 files/s]\u001b[A\n",
            "Copying files: 41124 files [00:55, 498.79 files/s]\u001b[A\n",
            "Copying files: 41179 files [00:56, 501.22 files/s]\u001b[A\n",
            "Copying files: 41235 files [00:56, 514.86 files/s]\u001b[A\n",
            "Copying files: 41296 files [00:56, 539.79 files/s]\u001b[A\n",
            "Copying files: 41375 files [00:56, 595.34 files/s]\u001b[A\n",
            "Copying files: 41441 files [00:56, 611.64 files/s]\u001b[A\n",
            "Copying files: 41514 files [00:56, 586.58 files/s]\u001b[A\n",
            "Copying files: 41579 files [00:56, 601.20 files/s]\u001b[A\n",
            "Copying files: 41645 files [00:56, 616.92 files/s]\u001b[A\n",
            "Copying files: 41709 files [00:56, 622.56 files/s]\u001b[A\n",
            "Copying files: 41773 files [00:56, 625.29 files/s]\u001b[A\n",
            "Copying files: 41842 files [00:57, 643.10 files/s]\u001b[A\n",
            "Copying files: 41907 files [00:57, 632.71 files/s]\u001b[A\n",
            "Copying files: 41981 files [00:57, 661.16 files/s]\u001b[A\n",
            "Copying files: 42048 files [00:57, 569.82 files/s]\u001b[A\n",
            "Copying files: 42108 files [00:57, 524.23 files/s]\u001b[A\n",
            "Copying files: 42163 files [00:57, 500.53 files/s]\u001b[A\n",
            "Copying files: 42222 files [00:57, 522.72 files/s]\u001b[A\n",
            "Copying files: 42277 files [00:57, 528.94 files/s]\u001b[A\n",
            "Copying files: 42357 files [00:57, 586.83 files/s]\u001b[A\n",
            "Copying files: 42447 files [00:58, 653.54 files/s]\u001b[A\n",
            "Copying files: 42521 files [00:58, 675.89 files/s]\u001b[A\n",
            "Copying files: 42592 files [00:58, 678.28 files/s]\u001b[A\n",
            "Copying files: 42684 files [00:58, 733.74 files/s]\u001b[A\n",
            "Copying files: 42768 files [00:58, 761.72 files/s]\u001b[A\n",
            "Copying files: 42853 files [00:58, 663.62 files/s]\u001b[A\n",
            "Copying files: 42926 files [00:58, 681.77 files/s]\u001b[A\n",
            "Copying files: 42998 files [00:58, 691.00 files/s]\u001b[A\n",
            "Copying files: 43070 files [00:59, 528.15 files/s]\u001b[A\n",
            "Copying files: 43131 files [00:59, 503.74 files/s]\u001b[A\n",
            "Copying files: 43187 files [00:59, 516.43 files/s]\u001b[A\n",
            "Copying files: 43243 files [00:59, 523.00 files/s]\u001b[A\n",
            "Copying files: 43308 files [00:59, 554.95 files/s]\u001b[A\n",
            "Copying files: 43379 files [00:59, 589.85 files/s]\u001b[A\n",
            "Copying files: 43441 files [00:59, 581.55 files/s]\u001b[A\n",
            "Copying files: 43501 files [00:59, 576.87 files/s]\u001b[A\n",
            "Copying files: 43574 files [00:59, 577.17 files/s]\u001b[A\n",
            "Copying files: 43640 files [01:00, 598.01 files/s]\u001b[A\n",
            "Copying files: 43705 files [01:00, 611.57 files/s]\u001b[A\n",
            "Copying files: 43770 files [01:00, 622.13 files/s]\u001b[A\n",
            "Copying files: 43833 files [01:00, 615.16 files/s]\u001b[A\n",
            "Copying files: 43906 files [01:00, 642.54 files/s]\u001b[A\n",
            "Copying files: 43982 files [01:00, 673.36 files/s]\u001b[A\n",
            "Copying files: 44051 files [01:00, 568.65 files/s]\u001b[A\n",
            "Copying files: 44112 files [01:00, 516.57 files/s]\u001b[A\n",
            "Copying files: 44167 files [01:00, 510.11 files/s]\u001b[A\n",
            "Copying files: 44222 files [01:01, 519.93 files/s]\u001b[A\n",
            "Copying files: 44284 files [01:01, 543.12 files/s]\u001b[A\n",
            "Copying files: 44363 files [01:01, 597.71 files/s]\u001b[A\n",
            "Copying files: 44426 files [01:01, 602.56 files/s]\u001b[A\n",
            "Copying files: 44518 files [01:01, 672.10 files/s]\u001b[A\n",
            "Copying files: 44590 files [01:01, 634.45 files/s]\u001b[A\n",
            "Copying files: 44666 files [01:01, 610.03 files/s]\u001b[A\n",
            "Copying files: 44735 files [01:01, 631.51 files/s]\u001b[A\n",
            "Copying files: 44801 files [01:01, 639.73 files/s]\u001b[A\n",
            "Copying files: 44867 files [01:02, 645.65 files/s]\u001b[A\n",
            "Copying files: 44937 files [01:02, 650.71 files/s]\u001b[A\n",
            "Copying files: 45008 files [01:02, 661.74 files/s]\u001b[A\n",
            "Copying files: 45075 files [01:02, 491.96 files/s]\u001b[A\n",
            "Copying files: 45131 files [01:02, 459.74 files/s]\u001b[A\n",
            "Copying files: 45183 files [01:02, 472.16 files/s]\u001b[A\n",
            "Copying files: 45234 files [01:02, 482.78 files/s]\u001b[A\n",
            "Copying files: 45300 files [01:02, 524.24 files/s]\u001b[A\n",
            "Copying files: 45375 files [01:03, 573.97 files/s]\u001b[A\n",
            "Copying files: 45436 files [01:03, 579.49 files/s]\u001b[A\n",
            "Copying files: 45507 files [01:03, 577.00 files/s]\u001b[A\n",
            "Copying files: 45571 files [01:03, 594.52 files/s]\u001b[A\n",
            "Copying files: 45637 files [01:03, 612.46 files/s]\u001b[A\n",
            "Copying files: 45703 files [01:03, 625.67 files/s]\u001b[A\n",
            "Copying files: 45767 files [01:03, 625.72 files/s]\u001b[A\n",
            "Copying files: 45832 files [01:03, 630.67 files/s]\u001b[A\n",
            "Copying files: 45896 files [01:03, 632.77 files/s]\u001b[A\n",
            "Copying files: 45963 files [01:03, 643.43 files/s]\u001b[A\n",
            "Copying files: 46028 files [01:04, 598.74 files/s]\u001b[A\n",
            "Copying files: 46089 files [01:04, 525.62 files/s]\u001b[A\n",
            "Copying files: 46144 files [01:04, 498.98 files/s]\u001b[A\n",
            "Copying files: 46198 files [01:04, 507.77 files/s]\u001b[A\n",
            "Copying files: 46253 files [01:04, 518.43 files/s]\u001b[A\n",
            "Copying files: 46322 files [01:04, 559.69 files/s]\u001b[A\n",
            "Copying files: 46395 files [01:04, 601.10 files/s]\u001b[A\n",
            "Copying files: 46469 files [01:04, 634.61 files/s]\u001b[A\n",
            "Copying files: 46537 files [01:04, 645.04 files/s]\u001b[A\n",
            "Copying files: 46620 files [01:05, 688.02 files/s]\u001b[A\n",
            "Copying files: 46691 files [01:05, 678.75 files/s]\u001b[A\n",
            "Copying files: 46786 files [01:05, 742.28 files/s]\u001b[A\n",
            "Copying files: 46863 files [01:05, 703.70 files/s]\u001b[A\n",
            "Copying files: 46956 files [01:05, 656.00 files/s]\u001b[A\n",
            "Copying files: 47025 files [01:05, 664.57 files/s]\u001b[A\n",
            "Copying files: 47094 files [01:05, 540.53 files/s]\u001b[A\n",
            "Copying files: 47153 files [01:05, 509.63 files/s]\u001b[A\n",
            "Copying files: 47210 files [01:06, 526.00 files/s]\u001b[A\n",
            "Copying files: 47268 files [01:06, 541.05 files/s]\u001b[A\n",
            "Copying files: 47343 files [01:06, 587.47 files/s]\u001b[A\n",
            "Copying files: 47415 files [01:06, 618.44 files/s]\u001b[A\n",
            "Copying files: 47483 files [01:06, 635.19 files/s]\u001b[A\n",
            "Copying files: 47549 files [01:06, 632.15 files/s]\u001b[A\n",
            "Copying files: 47634 files [01:06, 682.66 files/s]\u001b[A\n",
            "Copying files: 47715 files [01:06, 716.32 files/s]\u001b[A\n",
            "Copying files: 47793 files [01:06, 608.50 files/s]\u001b[A\n",
            "Copying files: 47881 files [01:07, 666.85 files/s]\u001b[A\n",
            "Copying files: 47959 files [01:07, 697.15 files/s]\u001b[A\n",
            "Copying files: 48033 files [01:07, 541.89 files/s]\u001b[A\n",
            "Copying files: 48095 files [01:07, 485.11 files/s]\u001b[A\n",
            "Copying files: 48151 files [01:07, 477.37 files/s]\u001b[A\n",
            "Copying files: 48209 files [01:07, 503.41 files/s]\u001b[A\n",
            "Copying files: 48263 files [01:07, 505.71 files/s]\u001b[A\n",
            "Copying files: 48330 files [01:07, 545.60 files/s]\u001b[A\n",
            "Copying files: 48391 files [01:08, 561.05 files/s]\u001b[A\n",
            "Copying files: 48450 files [01:08, 412.04 files/s]\u001b[A\n",
            "Copying files: 48542 files [01:08, 493.68 files/s]\u001b[A\n",
            "Copying files: 48613 files [01:08, 541.43 files/s]\u001b[A\n",
            "Copying files: 48691 files [01:08, 596.05 files/s]\u001b[A\n",
            "Copying files: 48761 files [01:08, 528.33 files/s]\u001b[A\n",
            "Copying files: 48838 files [01:08, 581.95 files/s]\u001b[A\n",
            "Copying files: 48912 files [01:08, 621.70 files/s]\u001b[A\n",
            "Copying files: 48980 files [01:09, 634.46 files/s]\u001b[A\n",
            "Copying files: 49048 files [01:09, 557.23 files/s]\u001b[A\n",
            "Copying files: 49109 files [01:09, 504.78 files/s]\u001b[A\n",
            "Copying files: 49164 files [01:09, 507.52 files/s]\u001b[A\n",
            "Copying files: 49220 files [01:09, 520.86 files/s]\u001b[A\n",
            "Copying files: 49275 files [01:09, 465.48 files/s]\u001b[A\n",
            "Copying files: 49348 files [01:09, 521.70 files/s]\u001b[A\n",
            "Copying files: 49412 files [01:09, 551.94 files/s]\u001b[A\n",
            "Copying files: 49500 files [01:10, 583.62 files/s]\u001b[A\n",
            "Copying files: 49572 files [01:10, 618.42 files/s]\u001b[A\n",
            "Copying files: 49644 files [01:10, 645.69 files/s]\u001b[A\n",
            "Copying files: 49711 files [01:10, 651.93 files/s]\u001b[A\n",
            "Copying files: 49785 files [01:10, 675.64 files/s]\u001b[A\n",
            "Copying files: 49854 files [01:10, 672.60 files/s]\u001b[A\n",
            "Copying files: 49928 files [01:10, 691.05 files/s]\u001b[A\n",
            "Copying files: 49998 files [01:10, 686.29 files/s]\u001b[A\n",
            "Copying files: 50068 files [01:10, 530.60 files/s]\u001b[A\n",
            "Copying files: 50127 files [01:11, 504.27 files/s]\u001b[A\n",
            "Copying files: 50182 files [01:11, 512.46 files/s]\u001b[A\n",
            "Copying files: 50237 files [01:11, 510.14 files/s]\u001b[A\n",
            "Copying files: 50291 files [01:11, 515.78 files/s]\u001b[A\n",
            "Copying files: 50360 files [01:11, 555.75 files/s]\u001b[A\n",
            "Copying files: 50420 files [01:11, 566.84 files/s]\u001b[A\n",
            "Copying files: 50490 files [01:11, 600.63 files/s]\u001b[A\n",
            "Copying files: 50557 files [01:11, 619.64 files/s]\u001b[A\n",
            "Copying files: 50631 files [01:11, 586.41 files/s]\u001b[A\n",
            "Copying files: 50700 files [01:12, 613.38 files/s]\u001b[A\n",
            "Copying files: 50765 files [01:12, 621.68 files/s]\u001b[A\n",
            "Copying files: 50832 files [01:12, 634.97 files/s]\u001b[A\n",
            "Copying files: 50899 files [01:12, 641.65 files/s]\u001b[A\n",
            "Copying files: 50972 files [01:12, 665.79 files/s]\u001b[A\n",
            "Copying files: 51040 files [01:12, 584.20 files/s]\u001b[A\n",
            "Copying files: 51101 files [01:12, 506.41 files/s]\u001b[A\n",
            "Copying files: 51156 files [01:12, 465.59 files/s]\u001b[A\n",
            "Copying files: 51206 files [01:13, 468.05 files/s]\u001b[A\n",
            "Copying files: 51262 files [01:13, 490.58 files/s]\u001b[A\n",
            "Copying files: 51331 files [01:13, 536.38 files/s]\u001b[A\n",
            "Copying files: 51404 files [01:13, 581.97 files/s]\u001b[A\n",
            "Copying files: 51495 files [01:13, 600.54 files/s]\u001b[A\n",
            "Copying files: 51571 files [01:13, 640.47 files/s]\u001b[A\n",
            "Copying files: 51646 files [01:13, 669.57 files/s]\u001b[A\n",
            "Copying files: 51718 files [01:13, 680.24 files/s]\u001b[A\n",
            "Copying files: 51790 files [01:13, 691.57 files/s]\u001b[A\n",
            "Copying files: 51861 files [01:13, 696.85 files/s]\u001b[A\n",
            "Copying files: 51933 files [01:14, 700.36 files/s]\u001b[A\n",
            "Copying files: 52004 files [01:14, 660.60 files/s]\u001b[A\n",
            "Copying files: 52071 files [01:14, 545.88 files/s]\u001b[A\n",
            "Copying files: 52130 files [01:14, 509.59 files/s]\u001b[A\n",
            "Copying files: 52185 files [01:14, 516.18 files/s]\u001b[A\n",
            "Copying files: 52249 files [01:14, 545.36 files/s]\u001b[A\n",
            "Copying files: 52316 files [01:14, 574.69 files/s]\u001b[A\n",
            "Copying files: 52381 files [01:14, 594.75 files/s]\u001b[A\n",
            "Copying files: 52476 files [01:15, 669.51 files/s]\u001b[A\n",
            "Copying files: 52548 files [01:15, 646.95 files/s]\u001b[A\n",
            "Copying files: 52627 files [01:15, 619.36 files/s]\u001b[A\n",
            "Copying files: 52698 files [01:15, 643.78 files/s]\u001b[A\n",
            "Copying files: 52770 files [01:15, 664.72 files/s]\u001b[A\n",
            "Copying files: 52844 files [01:15, 674.90 files/s]\u001b[A\n",
            "Copying files: 52921 files [01:15, 699.19 files/s]\u001b[A\n",
            "Copying files: 53001 files [01:15, 687.95 files/s]\u001b[A\n",
            "Copying files: 53071 files [01:15, 542.72 files/s]\u001b[A\n",
            "Copying files: 53131 files [01:16, 488.92 files/s]\u001b[A\n",
            "Copying files: 53187 files [01:16, 506.39 files/s]\u001b[A\n",
            "Copying files: 53242 files [01:16, 517.58 files/s]\u001b[A\n",
            "Copying files: 53310 files [01:16, 557.45 files/s]\u001b[A\n",
            "Copying files: 53375 files [01:16, 579.88 files/s]\u001b[A\n",
            "Copying files: 53446 files [01:16, 613.25 files/s]\u001b[A\n",
            "Copying files: 53515 files [01:16, 593.98 files/s]\u001b[A\n",
            "Copying files: 53588 files [01:16, 628.61 files/s]\u001b[A\n",
            "Copying files: 53658 files [01:16, 647.90 files/s]\u001b[A\n",
            "Copying files: 53728 files [01:17, 659.98 files/s]\u001b[A\n",
            "Copying files: 53795 files [01:17, 651.84 files/s]\u001b[A\n",
            "Copying files: 53884 files [01:17, 708.53 files/s]\u001b[A\n",
            "Copying files: 53957 files [01:17, 714.75 files/s]\u001b[A\n",
            "Copying files: 54030 files [01:17, 609.72 files/s]\u001b[A\n",
            "Copying files: 54095 files [01:17, 516.61 files/s]\u001b[A\n",
            "Copying files: 54152 files [01:17, 495.15 files/s]\u001b[A\n",
            "Copying files: 54212 files [01:17, 522.42 files/s]\u001b[A\n",
            "Copying files: 54272 files [01:18, 541.37 files/s]\u001b[A\n",
            "Copying files: 54339 files [01:18, 573.01 files/s]\u001b[A\n",
            "Copying files: 54417 files [01:18, 619.61 files/s]\u001b[A\n",
            "Copying files: 54487 files [01:18, 592.85 files/s]\u001b[A\n",
            "Copying files: 54552 files [01:18, 608.86 files/s]\u001b[A\n",
            "Copying files: 54617 files [01:18, 620.31 files/s]\u001b[A\n",
            "Copying files: 54681 files [01:18, 623.84 files/s]\u001b[A\n",
            "Copying files: 54745 files [01:18, 627.09 files/s]\u001b[A\n",
            "Copying files: 54810 files [01:18, 626.70 files/s]\u001b[A\n",
            "Copying files: 54885 files [01:18, 655.03 files/s]\u001b[A\n",
            "Copying files: 54957 files [01:19, 671.76 files/s]\u001b[A\n",
            "Copying files: 55025 files [01:19, 540.37 files/s]\u001b[A\n",
            "Copying files: 55084 files [01:19, 499.07 files/s]\u001b[A\n",
            "Copying files: 55138 files [01:19, 476.70 files/s]\u001b[A\n",
            "Copying files: 55192 files [01:19, 491.01 files/s]\u001b[A\n",
            "Copying files: 55249 files [01:19, 509.43 files/s]\u001b[A\n",
            "Copying files: 55313 files [01:19, 542.64 files/s]\u001b[A\n",
            "Copying files: 55387 files [01:19, 589.56 files/s]\u001b[A\n",
            "Copying files: 55473 files [01:20, 650.53 files/s]\u001b[A\n",
            "Copying files: 55545 files [01:20, 666.27 files/s]\u001b[A\n",
            "Copying files: 55615 files [01:20, 672.42 files/s]\u001b[A\n",
            "Copying files: 55695 files [01:20, 645.48 files/s]\u001b[A\n",
            "Copying files: 55771 files [01:20, 675.82 files/s]\u001b[A\n",
            "Copying files: 55841 files [01:20, 680.92 files/s]\u001b[A\n",
            "Copying files: 55921 files [01:20, 712.59 files/s]\u001b[A\n",
            "Copying files: 55994 files [01:20, 701.93 files/s]\u001b[A\n",
            "Copying files: 56065 files [01:20, 552.05 files/s]\u001b[A\n",
            "Copying files: 56126 files [01:21, 496.95 files/s]\u001b[A\n",
            "Copying files: 56181 files [01:21, 506.67 files/s]\u001b[A\n",
            "Copying files: 56242 files [01:21, 532.84 files/s]\u001b[A\n",
            "Copying files: 56307 files [01:21, 562.37 files/s]\u001b[A\n",
            "Copying files: 56382 files [01:21, 606.92 files/s]\u001b[A\n",
            "Copying files: 56466 files [01:21, 661.66 files/s]\u001b[A\n",
            "Copying files: 56540 files [01:21, 682.04 files/s]\u001b[A\n",
            "Copying files: 56611 files [01:21, 659.16 files/s]\u001b[A\n",
            "Copying files: 56679 files [01:21, 650.72 files/s]\u001b[A\n",
            "Copying files: 56753 files [01:22, 647.91 files/s]\u001b[A\n",
            "Copying files: 56826 files [01:22, 670.29 files/s]\u001b[A\n",
            "Copying files: 56895 files [01:22, 675.91 files/s]\u001b[A\n",
            "Copying files: 56966 files [01:22, 685.55 files/s]\u001b[A\n",
            "Copying files: 57036 files [01:22, 630.24 files/s]\u001b[A\n",
            "Copying files: 57101 files [01:22, 543.20 files/s]\u001b[A\n",
            "Copying files: 57159 files [01:22, 496.80 files/s]\u001b[A\n",
            "Copying files: 57212 files [01:22, 490.45 files/s]\u001b[A\n",
            "Copying files: 57271 files [01:23, 516.05 files/s]\u001b[A\n",
            "Copying files: 57341 files [01:23, 559.40 files/s]\u001b[A\n",
            "Copying files: 57400 files [01:23, 471.49 files/s]\u001b[A\n",
            "Copying files: 57460 files [01:23, 503.84 files/s]\u001b[A\n",
            "Copying files: 57525 files [01:23, 536.37 files/s]\u001b[A\n",
            "Copying files: 57603 files [01:23, 591.60 files/s]\u001b[A\n",
            "Copying files: 57666 files [01:23, 601.19 files/s]\u001b[A\n",
            "Copying files: 57729 files [01:23, 601.65 files/s]\u001b[A\n",
            "Copying files: 57799 files [01:23, 627.83 files/s]\u001b[A\n",
            "Copying files: 57875 files [01:23, 662.23 files/s]\u001b[A\n",
            "Copying files: 57943 files [01:24, 632.14 files/s]\u001b[A\n",
            "Copying files: 58008 files [01:24, 618.80 files/s]\u001b[A\n",
            "Copying files: 58071 files [01:24, 501.17 files/s]\u001b[A\n",
            "Copying files: 58126 files [01:24, 470.89 files/s]\u001b[A\n",
            "Copying files: 58186 files [01:24, 502.30 files/s]\u001b[A\n",
            "Copying files: 58243 files [01:24, 519.43 files/s]\u001b[A\n",
            "Copying files: 58320 files [01:24, 573.13 files/s]\u001b[A\n",
            "Copying files: 58391 files [01:24, 605.62 files/s]\u001b[A\n",
            "Copying files: 58492 files [01:25, 649.31 files/s]\u001b[A\n",
            "Copying files: 58579 files [01:25, 702.48 files/s]\u001b[A\n",
            "Copying files: 58661 files [01:25, 733.93 files/s]\u001b[A\n",
            "Copying files: 58750 files [01:25, 774.59 files/s]\u001b[A\n",
            "Copying files: 58838 files [01:25, 803.27 files/s]\u001b[A\n",
            "Copying files: 58924 files [01:25, 818.98 files/s]\u001b[A\n",
            "Copying files: 59008 files [01:25, 798.12 files/s]\u001b[A\n",
            "Copying files: 59089 files [01:25, 627.42 files/s]\u001b[A\n",
            "Copying files: 59159 files [01:26, 579.27 files/s]\u001b[A\n",
            "Copying files: 59223 files [01:26, 559.34 files/s]\u001b[A\n",
            "Copying files: 59283 files [01:26, 513.85 files/s]\u001b[A\n",
            "Copying files: 59359 files [01:26, 569.01 files/s]\u001b[A\n",
            "Copying files: 59450 files [01:26, 640.90 files/s]\u001b[A\n",
            "Copying files: 59527 files [01:26, 674.81 files/s]\u001b[A\n",
            "Copying files: 59620 files [01:26, 732.34 files/s]\u001b[A\n",
            "Copying files: 59698 files [01:26, 739.53 files/s]\u001b[A\n",
            "Copying files: 59797 files [01:26, 799.68 files/s]\u001b[A\n",
            "Copying files: 59881 files [01:26, 804.86 files/s]\u001b[A\n",
            "Copying files: 59971 files [01:27, 830.92 files/s]\u001b[A\n",
            "Copying files: 60057 files [01:27, 571.94 files/s]\u001b[A\n",
            "Copying files: 60127 files [01:27, 505.73 files/s]\u001b[A\n",
            "Copying files: 60188 files [01:27, 494.25 files/s]\u001b[A\n",
            "Copying files: 60245 files [01:27, 489.48 files/s]\u001b[A\n",
            "Copying files: 60303 files [01:27, 510.34 files/s]\u001b[A\n",
            "Copying files: 60364 files [01:27, 535.25 files/s]\u001b[A\n",
            "Copying files: 60432 files [01:28, 571.50 files/s]\u001b[A\n",
            "Copying files: 60492 files [01:28, 488.61 files/s]\u001b[A\n",
            "Copying files: 60577 files [01:28, 531.33 files/s]\u001b[A\n",
            "Copying files: 60647 files [01:28, 572.43 files/s]\u001b[A\n",
            "Copying files: 60717 files [01:28, 605.39 files/s]\u001b[A\n",
            "Copying files: 60781 files [01:28, 611.45 files/s]\u001b[A\n",
            "Copying files: 60847 files [01:28, 625.20 files/s]\u001b[A\n",
            "Copying files: 60918 files [01:28, 647.31 files/s]\u001b[A\n",
            "Copying files: 60985 files [01:28, 646.48 files/s]\u001b[A\n",
            "Copying files: 61051 files [01:29, 565.25 files/s]\u001b[A\n",
            "Copying files: 61111 files [01:29, 521.00 files/s]\u001b[A\n",
            "Copying files: 61166 files [01:29, 510.99 files/s]\u001b[A\n",
            "Copying files: 61219 files [01:29, 516.50 files/s]\u001b[A\n",
            "Copying files: 61273 files [01:29, 522.47 files/s]\u001b[A\n",
            "Copying files: 61342 files [01:29, 563.20 files/s]\u001b[A\n",
            "Copying files: 61408 files [01:29, 586.62 files/s]\u001b[A\n",
            "Copying files: 61479 files [01:29, 618.07 files/s]\u001b[A\n",
            "Copying files: 61544 files [01:29, 624.60 files/s]\u001b[A\n",
            "Copying files: 61622 files [01:30, 663.20 files/s]\u001b[A\n",
            "Copying files: 61690 files [01:30, 665.65 files/s]\u001b[A\n",
            "Copying files: 61758 files [01:30, 631.85 files/s]\u001b[A\n",
            "Copying files: 61866 files [01:30, 634.71 files/s]\u001b[A\n",
            "Copying files: 61939 files [01:30, 660.26 files/s]\u001b[A\n",
            "Copying files: 62006 files [01:30, 605.18 files/s]\u001b[A\n",
            "Copying files: 62068 files [01:30, 511.37 files/s]\u001b[A\n",
            "Copying files: 62123 files [01:30, 486.22 files/s]\u001b[A\n",
            "Copying files: 62175 files [01:31, 480.92 files/s]\u001b[A\n",
            "Copying files: 62225 files [01:31, 483.09 files/s]\u001b[A\n",
            "Copying files: 62281 files [01:31, 500.12 files/s]\u001b[A\n",
            "Copying files: 62351 files [01:31, 546.76 files/s]\u001b[A\n",
            "Copying files: 62414 files [01:31, 569.32 files/s]\u001b[A\n",
            "Copying files: 62492 files [01:31, 616.43 files/s]\u001b[A\n",
            "Copying files: 62556 files [01:31, 616.78 files/s]\u001b[A\n",
            "Copying files: 62626 files [01:31, 639.58 files/s]\u001b[A\n",
            "Copying files: 62702 files [01:31, 581.39 files/s]\u001b[A\n",
            "Copying files: 62781 files [01:32, 631.07 files/s]\u001b[A\n",
            "Copying files: 62847 files [01:32, 637.88 files/s]\u001b[A\n",
            "Copying files: 62913 files [01:32, 642.59 files/s]\u001b[A\n",
            "Copying files: 62979 files [01:32, 638.08 files/s]\u001b[A\n",
            "Copying files: 63044 files [01:32, 562.59 files/s]\u001b[A\n",
            "Copying files: 63103 files [01:32, 484.94 files/s]\u001b[A\n",
            "Copying files: 63155 files [01:32, 459.09 files/s]\u001b[A\n",
            "Copying files: 63207 files [01:32, 472.21 files/s]\u001b[A\n",
            "Copying files: 63257 files [01:33, 479.85 files/s]\u001b[A\n",
            "Copying files: 63309 files [01:33, 487.37 files/s]\u001b[A\n",
            "Copying files: 63381 files [01:33, 539.25 files/s]\u001b[A\n",
            "Copying files: 63448 files [01:33, 572.74 files/s]\u001b[A\n",
            "Copying files: 63539 files [01:33, 587.95 files/s]\u001b[A\n",
            "Copying files: 63610 files [01:33, 619.67 files/s]\u001b[A\n",
            "Copying files: 63677 files [01:33, 633.29 files/s]\u001b[A\n",
            "Copying files: 63745 files [01:33, 642.50 files/s]\u001b[A\n",
            "Copying files: 63811 files [01:33, 640.73 files/s]\u001b[A\n",
            "Copying files: 63883 files [01:33, 662.35 files/s]\u001b[A\n",
            "Copying files: 63955 files [01:34, 678.22 files/s]\u001b[A\n",
            "Copying files: 64024 files [01:34, 584.93 files/s]\u001b[A\n",
            "Copying files: 64086 files [01:34, 486.40 files/s]\u001b[A\n",
            "Copying files: 64140 files [01:34, 460.35 files/s]\u001b[A\n",
            "Copying files: 64195 files [01:34, 481.19 files/s]\u001b[A\n",
            "Copying files: 64249 files [01:34, 494.32 files/s]\u001b[A\n",
            "Copying files: 64315 files [01:34, 533.21 files/s]\u001b[A\n",
            "Copying files: 64386 files [01:34, 575.01 files/s]\u001b[A\n",
            "Copying files: 64470 files [01:35, 606.06 files/s]\u001b[A\n",
            "Copying files: 64537 files [01:35, 623.91 files/s]\u001b[A\n",
            "Copying files: 64609 files [01:35, 649.89 files/s]\u001b[A\n",
            "Copying files: 64679 files [01:35, 664.03 files/s]\u001b[A\n",
            "Copying files: 64751 files [01:35, 679.52 files/s]\u001b[A\n",
            "Copying files: 64822 files [01:35, 688.38 files/s]\u001b[A\n",
            "Copying files: 64892 files [01:35, 685.36 files/s]\u001b[A\n",
            "Copying files: 64964 files [01:35, 695.20 files/s]\u001b[A\n",
            "Copying files: 65034 files [01:35, 634.67 files/s]\u001b[A\n",
            "Copying files: 65099 files [01:36, 521.27 files/s]\u001b[A\n",
            "Copying files: 65156 files [01:36, 483.69 files/s]\u001b[A\n",
            "Copying files: 65211 files [01:36, 499.37 files/s]\u001b[A\n",
            "Copying files: 65264 files [01:36, 504.98 files/s]\u001b[A\n",
            "Copying files: 65329 files [01:36, 539.81 files/s]\u001b[A\n",
            "Copying files: 65401 files [01:36, 583.17 files/s]\u001b[A\n",
            "Copying files: 65492 files [01:36, 652.06 files/s]\u001b[A\n",
            "Copying files: 65562 files [01:36, 654.75 files/s]\u001b[A\n",
            "Copying files: 65648 files [01:36, 645.46 files/s]\u001b[A\n",
            "Copying files: 65723 files [01:37, 673.47 files/s]\u001b[A\n",
            "Copying files: 65793 files [01:37, 679.61 files/s]\u001b[A\n",
            "Copying files: 65866 files [01:37, 693.96 files/s]\u001b[A\n",
            "Copying files: 65942 files [01:37, 711.35 files/s]\u001b[A\n",
            "Copying files: 66014 files [01:37, 649.08 files/s]\u001b[A\n",
            "Copying files: 66081 files [01:37, 505.75 files/s]\u001b[A\n",
            "Copying files: 66138 files [01:37, 464.52 files/s]\u001b[A\n",
            "Copying files: 66192 files [01:37, 482.06 files/s]\u001b[A\n",
            "Copying files: 66244 files [01:38, 492.20 files/s]\u001b[A\n",
            "Copying files: 66296 files [01:38, 227.05 files/s]\u001b[A\n",
            "Copying files: 66357 files [01:38, 279.23 files/s]\u001b[A\n",
            "Copying files: 66422 files [01:38, 336.83 files/s]\u001b[A\n",
            "Copying files: 66483 files [01:38, 388.52 files/s]\u001b[A\n",
            "Copying files: 66546 files [01:38, 437.16 files/s]\u001b[A\n",
            "Copying files: 66626 files [01:39, 505.92 files/s]\u001b[A\n",
            "Copying files: 66703 files [01:39, 500.48 files/s]\u001b[A\n",
            "Copying files: 66769 files [01:39, 539.43 files/s]\u001b[A\n",
            "Copying files: 66831 files [01:39, 561.07 files/s]\u001b[A\n",
            "Copying files: 66895 files [01:39, 582.61 files/s]\u001b[A\n",
            "Copying files: 66959 files [01:39, 598.71 files/s]\u001b[A\n",
            "Copying files: 67022 files [01:39, 601.39 files/s]\u001b[A\n",
            "Copying files: 67085 files [01:39, 513.38 files/s]\u001b[A\n",
            "Copying files: 67140 files [01:40, 468.85 files/s]\u001b[A\n",
            "Copying files: 67191 files [01:40, 469.11 files/s]\u001b[A\n",
            "Copying files: 67241 files [01:40, 476.74 files/s]\u001b[A\n",
            "Copying files: 67291 files [01:40, 477.31 files/s]\u001b[A\n",
            "Copying files: 67361 files [01:40, 525.83 files/s]\u001b[A\n",
            "Copying files: 67424 files [01:40, 552.65 files/s]\u001b[A\n",
            "Copying files: 67499 files [01:40, 599.87 files/s]\u001b[A\n",
            "Copying files: 67569 files [01:40, 626.52 files/s]\u001b[A\n",
            "Copying files: 67660 files [01:40, 691.08 files/s]\u001b[A\n",
            "Copying files: 67733 files [01:40, 681.91 files/s]\u001b[A\n",
            "Copying files: 67812 files [01:41, 603.72 files/s]\u001b[A\n",
            "Copying files: 67885 files [01:41, 636.70 files/s]\u001b[A\n",
            "Copying files: 67952 files [01:41, 646.18 files/s]\u001b[A\n",
            "Copying files: 68019 files [01:41, 623.78 files/s]\u001b[A\n",
            "Copying files: 68083 files [01:41, 460.94 files/s]\u001b[A\n",
            "Copying files: 68137 files [01:41, 436.62 files/s]\u001b[A\n",
            "Copying files: 68187 files [01:41, 440.58 files/s]\u001b[A\n",
            "Copying files: 68235 files [01:42, 437.96 files/s]\u001b[A\n",
            "Copying files: 68286 files [01:42, 454.08 files/s]\u001b[A\n",
            "Copying files: 68353 files [01:42, 501.53 files/s]\u001b[A\n",
            "Copying files: 68419 files [01:42, 538.16 files/s]\u001b[A\n",
            "Copying files: 68496 files [01:42, 591.36 files/s]\u001b[A\n",
            "Copying files: 68559 files [01:42, 599.10 files/s]\u001b[A\n",
            "Copying files: 68624 files [01:42, 613.05 files/s]\u001b[A\n",
            "Copying files: 68695 files [01:42, 636.93 files/s]\u001b[A\n",
            "Copying files: 68771 files [01:42, 669.31 files/s]\u001b[A\n",
            "Copying files: 68844 files [01:42, 684.74 files/s]\u001b[A\n",
            "Copying files: 68914 files [01:43, 688.16 files/s]\u001b[A\n",
            "Copying files: 68984 files [01:43, 517.47 files/s]\u001b[A\n",
            "Copying files: 69043 files [01:43, 480.19 files/s]\u001b[A\n",
            "Copying files: 69097 files [01:43, 450.43 files/s]\u001b[A\n",
            "Copying files: 69147 files [01:43, 448.59 files/s]\u001b[A\n",
            "Copying files: 69202 files [01:43, 474.46 files/s]\u001b[A\n",
            "Copying files: 69264 files [01:43, 507.60 files/s]\u001b[A\n",
            "Copying files: 69334 files [01:43, 552.55 files/s]\u001b[A\n",
            "Copying files: 69393 files [01:44, 558.33 files/s]\u001b[A\n",
            "Copying files: 69476 files [01:44, 617.10 files/s]\u001b[A\n",
            "Copying files: 69545 files [01:44, 633.33 files/s]\u001b[A\n",
            "Copying files: 69620 files [01:44, 664.18 files/s]\u001b[A\n",
            "Copying files: 69690 files [01:44, 673.71 files/s]\u001b[A\n",
            "Copying files: 69781 files [01:44, 618.40 files/s]\u001b[A\n",
            "Copying files: 69861 files [01:44, 663.18 files/s]\u001b[A\n",
            "Copying files: 69937 files [01:44, 689.49 files/s]\u001b[A\n",
            "Copying files: 70008 files [01:44, 629.69 files/s]\u001b[A\n",
            "Copying files: 70074 files [01:45, 515.17 files/s]\u001b[A\n",
            "Copying files: 70131 files [01:45, 476.10 files/s]\u001b[A\n",
            "Copying files: 70186 files [01:45, 494.09 files/s]\u001b[A\n",
            "Copying files: 70239 files [01:45, 503.88 files/s]\u001b[A\n",
            "Copying files: 70303 files [01:45, 536.48 files/s]\u001b[A\n",
            "Copying files: 70368 files [01:45, 561.03 files/s]\u001b[A\n",
            "Copying files: 70426 files [01:45, 526.57 files/s]\u001b[A\n",
            "Copying files: 70481 files [01:45, 525.55 files/s]\u001b[A\n",
            "Copying files: 70544 files [01:46, 548.17 files/s]\u001b[A\n",
            "Copying files: 70614 files [01:46, 583.50 files/s]\u001b[A\n",
            "Copying files: 70678 files [01:46, 598.94 files/s]\u001b[A\n",
            "Copying files: 70745 files [01:46, 618.29 files/s]\u001b[A\n",
            "Copying files: 70816 files [01:46, 642.83 files/s]\u001b[A\n",
            "Copying files: 70882 files [01:46, 637.82 files/s]\u001b[A\n",
            "Copying files: 70950 files [01:46, 649.91 files/s]\u001b[A\n",
            "Copying files: 71016 files [01:46, 561.04 files/s]\u001b[A\n",
            "Copying files: 71075 files [01:46, 489.79 files/s]\u001b[A\n",
            "Copying files: 71128 files [01:47, 464.59 files/s]\u001b[A\n",
            "Copying files: 71183 files [01:47, 486.81 files/s]\u001b[A\n",
            "Copying files: 71241 files [01:47, 507.38 files/s]\u001b[A\n",
            "Copying files: 71306 files [01:47, 543.09 files/s]\u001b[A\n",
            "Copying files: 71374 files [01:47, 576.40 files/s]\u001b[A\n",
            "Copying files: 71453 files [01:47, 627.07 files/s]\u001b[A\n",
            "Copying files: 71523 files [01:47, 579.65 files/s]\u001b[A\n",
            "Copying files: 71593 files [01:47, 610.72 files/s]\u001b[A\n",
            "Copying files: 71660 files [01:47, 623.75 files/s]\u001b[A\n",
            "Copying files: 71733 files [01:48, 652.13 files/s]\u001b[A\n",
            "Copying files: 71800 files [01:48, 632.80 files/s]\u001b[A\n",
            "Copying files: 71891 files [01:48, 696.32 files/s]\u001b[A\n",
            "Copying files: 71964 files [01:48, 705.27 files/s]\u001b[A\n",
            "Copying files: 72037 files [01:48, 537.00 files/s]\u001b[A\n",
            "Copying files: 72099 files [01:48, 470.75 files/s]\u001b[A\n",
            "Copying files: 72153 files [01:48, 404.51 files/s]\u001b[A\n",
            "Copying files: 72211 files [01:49, 444.75 files/s]\u001b[A\n",
            "Copying files: 72267 files [01:49, 471.52 files/s]\u001b[A\n",
            "Copying files: 72336 files [01:49, 518.16 files/s]\u001b[A\n",
            "Copying files: 72402 files [01:49, 552.28 files/s]\u001b[A\n",
            "Copying files: 72486 files [01:49, 590.44 files/s]\u001b[A\n",
            "Copying files: 72559 files [01:49, 626.31 files/s]\u001b[A\n",
            "Copying files: 72631 files [01:49, 651.37 files/s]\u001b[A\n",
            "Copying files: 72700 files [01:49, 662.34 files/s]\u001b[A\n",
            "Copying files: 72774 files [01:49, 683.67 files/s]\u001b[A\n",
            "Copying files: 72846 files [01:49, 693.43 files/s]\u001b[A\n",
            "Copying files: 72917 files [01:50, 693.96 files/s]\u001b[A\n",
            "Copying files: 72988 files [01:50, 695.48 files/s]\u001b[A\n",
            "Copying files: 73059 files [01:50, 536.06 files/s]\u001b[A\n",
            "Copying files: 73119 files [01:50, 485.90 files/s]\u001b[A\n",
            "Copying files: 73173 files [01:50, 482.03 files/s]\u001b[A\n",
            "Copying files: 73231 files [01:50, 505.26 files/s]\u001b[A\n",
            "Copying files: 73290 files [01:50, 526.56 files/s]\u001b[A\n",
            "Copying files: 73362 files [01:50, 537.94 files/s]\u001b[A\n",
            "Copying files: 73427 files [01:51, 566.09 files/s]\u001b[A\n",
            "Copying files: 73491 files [01:51, 585.77 files/s]\u001b[A\n",
            "Copying files: 73557 files [01:51, 605.85 files/s]\u001b[A\n",
            "Copying files: 73620 files [01:51, 610.22 files/s]\u001b[A\n",
            "Copying files: 73690 files [01:51, 630.12 files/s]\u001b[A\n",
            "Copying files: 73756 files [01:51, 638.46 files/s]\u001b[A\n",
            "Copying files: 73821 files [01:51, 623.85 files/s]\u001b[A\n",
            "Copying files: 73900 files [01:51, 665.40 files/s]\u001b[A\n",
            "Copying files: 73983 files [01:51, 705.59 files/s]\u001b[A\n",
            "Copying files: 74055 files [01:52, 497.54 files/s]\u001b[A\n",
            "Copying files: 74115 files [01:52, 487.97 files/s]\u001b[A\n",
            "Copying files: 74171 files [01:52, 483.35 files/s]\u001b[A\n",
            "Copying files: 74225 files [01:52, 494.59 files/s]\u001b[A\n",
            "Copying files: 74285 files [01:52, 522.05 files/s]\u001b[A\n",
            "Copying files: 74345 files [01:52, 541.72 files/s]\u001b[A\n",
            "Copying files: 74403 files [01:52, 549.43 files/s]\u001b[A\n",
            "Copying files: 74460 files [01:52, 516.49 files/s]\u001b[A\n",
            "Copying files: 74543 files [01:52, 580.62 files/s]\u001b[A\n",
            "Copying files: 74605 files [01:53, 577.61 files/s]\u001b[A\n",
            "Copying files: 74666 files [01:53, 316.48 files/s]\u001b[A\n",
            "Copying files: 74727 files [01:53, 369.75 files/s]\u001b[A\n",
            "Copying files: 74787 files [01:53, 417.02 files/s]\u001b[A\n",
            "Copying files: 74852 files [01:53, 467.22 files/s]\u001b[A\n",
            "Copying files: 74909 files [01:53, 489.33 files/s]\u001b[A\n",
            "Copying files: 74986 files [01:53, 549.24 files/s]\u001b[A\n",
            "Copying files: 75049 files [01:54, 449.60 files/s]\u001b[A\n",
            "Copying files: 75103 files [01:54, 445.00 files/s]\u001b[A\n",
            "Copying files: 75154 files [01:54, 460.74 files/s]\u001b[A\n",
            "Copying files: 75212 files [01:54, 489.28 files/s]\u001b[A\n",
            "Copying files: 75270 files [01:54, 509.78 files/s]\u001b[A\n",
            "Copying files: 75343 files [01:54, 559.71 files/s]\u001b[A\n",
            "Copying files: 75415 files [01:54, 599.59 files/s]\u001b[A\n",
            "Copying files: 75521 files [01:54, 686.45 files/s]\u001b[A\n",
            "Copying files: 75597 files [01:55, 659.84 files/s]\u001b[A\n",
            "Copying files: 75668 files [01:55, 634.70 files/s]\u001b[A\n",
            "Copying files: 75739 files [01:55, 655.34 files/s]\u001b[A\n",
            "Copying files: 75811 files [01:55, 672.87 files/s]\u001b[A\n",
            "Copying files: 75884 files [01:55, 689.04 files/s]\u001b[A\n",
            "Copying files: 75957 files [01:55, 700.41 files/s]\u001b[A\n",
            "Copying files: 76029 files [01:55, 678.44 files/s]\u001b[A\n",
            "Copying files: 76098 files [01:55, 561.92 files/s]\u001b[A\n",
            "Copying files: 76159 files [01:55, 527.79 files/s]\u001b[A\n",
            "Copying files: 76215 files [01:56, 512.55 files/s]\u001b[A\n",
            "Copying files: 76269 files [01:56, 425.21 files/s]\u001b[A\n",
            "Copying files: 76346 files [01:56, 486.12 files/s]\u001b[A\n",
            "Copying files: 76423 files [01:56, 546.20 files/s]\u001b[A\n",
            "Copying files: 76501 files [01:56, 597.04 files/s]\u001b[A\n",
            "Copying files: 76567 files [01:56, 584.47 files/s]\u001b[A\n",
            "Copying files: 76681 files [01:56, 612.24 files/s]\u001b[A\n",
            "Copying files: 76763 files [01:56, 662.54 files/s]\u001b[A\n",
            "Copying files: 76833 files [01:57, 660.10 files/s]\u001b[A\n",
            "Copying files: 76918 files [01:57, 707.46 files/s]\u001b[A\n",
            "Copying files: 76992 files [01:57, 709.02 files/s]\u001b[A\n",
            "Copying files: 77065 files [01:57, 554.60 files/s]\u001b[A\n",
            "Copying files: 77127 files [01:57, 515.15 files/s]\u001b[A\n",
            "Copying files: 77184 files [01:57, 511.22 files/s]\u001b[A\n",
            "Copying files: 77239 files [01:57, 506.99 files/s]\u001b[A\n",
            "Copying files: 77299 files [01:57, 531.27 files/s]\u001b[A\n",
            "Copying files: 77369 files [01:58, 572.33 files/s]\u001b[A\n",
            "Copying files: 77430 files [01:58, 576.77 files/s]\u001b[A\n",
            "Copying files: 77508 files [01:58, 601.44 files/s]\u001b[A\n",
            "Copying files: 77574 files [01:58, 617.83 files/s]\u001b[A\n",
            "Copying files: 77641 files [01:58, 632.60 files/s]\u001b[A\n",
            "Copying files: 77709 files [01:58, 640.99 files/s]\u001b[A\n",
            "Copying files: 77779 files [01:58, 654.66 files/s]\u001b[A\n",
            "Copying files: 77846 files [01:58, 655.42 files/s]\u001b[A\n",
            "Copying files: 77925 files [01:58, 690.35 files/s]\u001b[A\n",
            "Copying files: 77999 files [01:58, 704.13 files/s]\u001b[A\n",
            "Copying files: 78071 files [01:59, 560.98 files/s]\u001b[A\n",
            "Copying files: 78133 files [01:59, 526.88 files/s]\u001b[A\n",
            "Copying files: 78192 files [01:59, 543.02 files/s]\u001b[A\n",
            "Copying files: 78255 files [01:59, 564.84 files/s]\u001b[A\n",
            "Copying files: 78326 files [01:59, 600.88 files/s]\u001b[A\n",
            "Copying files: 78396 files [01:59, 626.71 files/s]\u001b[A\n",
            "Copying files: 78485 files [01:59, 685.46 files/s]\u001b[A\n",
            "Copying files: 78557 files [01:59, 675.41 files/s]\u001b[A\n",
            "Copying files: 78627 files [02:00, 669.52 files/s]\u001b[A\n",
            "Copying files: 78696 files [02:00, 606.95 files/s]\u001b[A\n",
            "Copying files: 78770 files [02:00, 625.59 files/s]\u001b[A\n",
            "Copying files: 78845 files [02:00, 658.12 files/s]\u001b[A\n",
            "Copying files: 78918 files [02:00, 677.99 files/s]\u001b[A\n",
            "Copying files: 78994 files [02:00, 694.95 files/s]\u001b[A\n",
            "Copying files: 79065 files [02:00, 539.40 files/s]\u001b[A\n",
            "Copying files: 79125 files [02:00, 497.70 files/s]\u001b[A\n",
            "Copying files: 79180 files [02:01, 498.66 files/s]\u001b[A\n",
            "Copying files: 79234 files [02:01, 495.41 files/s]\u001b[A\n",
            "Copying files: 79288 files [02:01, 505.82 files/s]\u001b[A\n",
            "Copying files: 79358 files [02:01, 550.61 files/s]\u001b[A\n",
            "Copying files: 79422 files [02:01, 574.49 files/s]\u001b[A\n",
            "Copying files: 79501 files [02:01, 579.61 files/s]\u001b[A\n",
            "Copying files: 79567 files [02:01, 601.49 files/s]\u001b[A\n",
            "Copying files: 79638 files [02:01, 630.33 files/s]\u001b[A\n",
            "Copying files: 79703 files [02:01, 620.45 files/s]\u001b[A\n",
            "Copying files: 79790 files [02:01, 678.84 files/s]\u001b[A\n",
            "Copying files: 79861 files [02:02, 685.66 files/s]\u001b[A\n",
            "Copying files: 79932 files [02:02, 682.57 files/s]\u001b[A\n",
            "Copying files: 80002 files [02:02, 647.11 files/s]\u001b[A\n",
            "Copying files: 80068 files [02:02, 528.01 files/s]\u001b[A\n",
            "Copying files: 80126 files [02:02, 491.98 files/s]\u001b[A\n",
            "Copying files: 80180 files [02:02, 503.86 files/s]\u001b[A\n",
            "Copying files: 80233 files [02:02, 501.29 files/s]\u001b[A\n",
            "Copying files: 80289 files [02:02, 516.57 files/s]\u001b[A\n",
            "Copying files: 80360 files [02:03, 560.78 files/s]\u001b[A\n",
            "Copying files: 80447 files [02:03, 625.79 files/s]\u001b[A\n",
            "Copying files: 80514 files [02:03, 566.12 files/s]\u001b[A\n",
            "Copying files: 80575 files [02:03, 550.44 files/s]\u001b[A\n",
            "Copying files: 80658 files [02:03, 611.05 files/s]\u001b[A\n",
            "Copying files: 80727 files [02:03, 632.67 files/s]\u001b[A\n",
            "Copying files: 80822 files [02:03, 701.85 files/s]\u001b[A\n",
            "Copying files: 80905 files [02:03, 613.48 files/s]\u001b[A\n",
            "Copying files: 81000 files [02:03, 686.34 files/s]\u001b[A\n",
            "Copying files: 81076 files [02:04, 538.10 files/s]\u001b[A\n",
            "Copying files: 81140 files [02:04, 508.48 files/s]\u001b[A\n",
            "Copying files: 81198 files [02:04, 524.07 files/s]\u001b[A\n",
            "Copying files: 81258 files [02:04, 541.74 files/s]\u001b[A\n",
            "Copying files: 81320 files [02:04, 562.01 files/s]\u001b[A\n",
            "Copying files: 81395 files [02:04, 607.35 files/s]\u001b[A\n",
            "Copying files: 81483 files [02:04, 667.90 files/s]\u001b[A\n",
            "Copying files: 81565 files [02:04, 650.71 files/s]\u001b[A\n",
            "Copying files: 81641 files [02:05, 679.72 files/s]\u001b[A\n",
            "Copying files: 81714 files [02:05, 693.81 files/s]\u001b[A\n",
            "Copying files: 81791 files [02:05, 713.98 files/s]\u001b[A\n",
            "Copying files: 81870 files [02:05, 735.14 files/s]\u001b[A\n",
            "Copying files: 81946 files [02:05, 742.05 files/s]\u001b[A\n",
            "Copying files: 82021 files [02:05, 735.66 files/s]\u001b[A\n",
            "Copying files: 82096 files [02:05, 577.26 files/s]\u001b[A\n",
            "Copying files: 82160 files [02:05, 543.36 files/s]\u001b[A\n",
            "Copying files: 82219 files [02:06, 534.90 files/s]\u001b[A\n",
            "Copying files: 82279 files [02:06, 552.19 files/s]\u001b[A\n",
            "Copying files: 82352 files [02:06, 592.61 files/s]\u001b[A\n",
            "Copying files: 82420 files [02:06, 614.05 files/s]\u001b[A\n",
            "Copying files: 82484 files [02:06, 587.65 files/s]\u001b[A\n",
            "Copying files: 82545 files [02:06, 587.13 files/s]\u001b[A\n",
            "Copying files: 82616 files [02:06, 583.38 files/s]\u001b[A\n",
            "Copying files: 82685 files [02:06, 611.48 files/s]\u001b[A\n",
            "Copying files: 82748 files [02:06, 616.36 files/s]\u001b[A\n",
            "Copying files: 82816 files [02:06, 634.14 files/s]\u001b[A\n",
            "Copying files: 82881 files [02:07, 630.37 files/s]\u001b[A\n",
            "Copying files: 82959 files [02:07, 668.43 files/s]\u001b[A\n",
            "Copying files: 83027 files [02:07, 557.87 files/s]\u001b[A\n",
            "Copying files: 83087 files [02:07, 469.85 files/s]\u001b[A\n",
            "Copying files: 83139 files [02:07, 449.96 files/s]\u001b[A\n",
            "Copying files: 83195 files [02:07, 476.14 files/s]\u001b[A\n",
            "Copying files: 83249 files [02:07, 493.17 files/s]\u001b[A\n",
            "Copying files: 83321 files [02:07, 542.85 files/s]\u001b[A\n",
            "Copying files: 83391 files [02:08, 581.79 files/s]\u001b[A\n",
            "Copying files: 83464 files [02:08, 588.54 files/s]\u001b[A\n",
            "Copying files: 83525 files [02:08, 558.94 files/s]\u001b[A\n",
            "Copying files: 83589 files [02:08, 580.77 files/s]\u001b[A\n",
            "Copying files: 83677 files [02:08, 646.38 files/s]\u001b[A\n",
            "Copying files: 83748 files [02:08, 663.91 files/s]\u001b[A\n",
            "Copying files: 83824 files [02:08, 686.12 files/s]\u001b[A\n",
            "Copying files: 83904 files [02:08, 716.23 files/s]\u001b[A\n",
            "Copying files: 83981 files [02:08, 731.54 files/s]\u001b[A\n",
            "Copying files: 84056 files [02:09, 522.40 files/s]\u001b[A\n",
            "Copying files: 84118 files [02:09, 490.33 files/s]\u001b[A\n",
            "Copying files: 84174 files [02:09, 501.13 files/s]\u001b[A\n",
            "Copying files: 84239 files [02:09, 536.07 files/s]\u001b[A\n",
            "Copying files: 84297 files [02:09, 541.64 files/s]\u001b[A\n",
            "Copying files: 84377 files [02:09, 597.93 files/s]\u001b[A\n",
            "Copying files: 84458 files [02:09, 648.55 files/s]\u001b[A\n",
            "Copying files: 84533 files [02:09, 675.67 files/s]\u001b[A\n",
            "Copying files: 84608 files [02:09, 695.86 files/s]\u001b[A\n",
            "Copying files: 84686 files [02:10, 718.95 files/s]\u001b[A\n",
            "Copying files: 84768 files [02:10, 746.00 files/s]\u001b[A\n",
            "Copying files: 84864 files [02:10, 799.44 files/s]\u001b[A\n",
            "Copying files: 84947 files [02:10, 651.68 files/s]\u001b[A\n",
            "Copying files: 85022 files [02:10, 674.80 files/s]\u001b[A\n",
            "Copying files: 85094 files [02:10, 550.11 files/s]\u001b[A\n",
            "Copying files: 85156 files [02:10, 526.27 files/s]\u001b[A\n",
            "Copying files: 85214 files [02:11, 517.94 files/s]\u001b[A\n",
            "Copying files: 85270 files [02:11, 512.63 files/s]\u001b[A\n",
            "Copying files: 85324 files [02:11, 484.47 files/s]\u001b[A\n",
            "Copying files: 85383 files [02:11, 510.85 files/s]\u001b[A\n",
            "Copying files: 85466 files [02:11, 550.33 files/s]\u001b[A\n",
            "Copying files: 85530 files [02:11, 572.04 files/s]\u001b[A\n",
            "Copying files: 85603 files [02:11, 611.25 files/s]\u001b[A\n",
            "Copying files: 85670 files [02:11, 627.34 files/s]\u001b[A\n",
            "Copying files: 85735 files [02:11, 633.05 files/s]\u001b[A\n",
            "Copying files: 85805 files [02:11, 650.41 files/s]\u001b[A\n",
            "Copying files: 85871 files [02:12, 650.55 files/s]\u001b[A\n",
            "Copying files: 85938 files [02:12, 656.18 files/s]\u001b[A\n",
            "Copying files: 86005 files [02:12, 606.98 files/s]\u001b[A\n",
            "Copying files: 86067 files [02:12, 509.28 files/s]\u001b[A\n",
            "Copying files: 86122 files [02:12, 480.50 files/s]\u001b[A\n",
            "Copying files: 86173 files [02:12, 475.75 files/s]\u001b[A\n",
            "Copying files: 86224 files [02:12, 483.33 files/s]\u001b[A\n",
            "Copying files: 86274 files [02:12, 477.49 files/s]\u001b[A\n",
            "Copying files: 86348 files [02:13, 533.92 files/s]\u001b[A\n",
            "Copying files: 86405 files [02:13, 539.16 files/s]\u001b[A\n",
            "Copying files: 86461 files [02:13, 433.93 files/s]\u001b[A\n",
            "Copying files: 86551 files [02:13, 513.46 files/s]\u001b[A\n",
            "Copying files: 86612 files [02:13, 508.94 files/s]\u001b[A\n",
            "Copying files: 86687 files [02:13, 519.38 files/s]\u001b[A\n",
            "Copying files: 86757 files [02:13, 562.66 files/s]\u001b[A\n",
            "Copying files: 86818 files [02:13, 568.83 files/s]\u001b[A\n",
            "Copying files: 86890 files [02:13, 606.97 files/s]\u001b[A\n",
            "Copying files: 86959 files [02:14, 629.60 files/s]\u001b[A\n",
            "Copying files: 87025 files [02:14, 529.26 files/s]\u001b[A\n",
            "Copying files: 87083 files [02:14, 476.95 files/s]\u001b[A\n",
            "Copying files: 87135 files [02:14, 461.66 files/s]\u001b[A\n",
            "Copying files: 87187 files [02:14, 477.35 files/s]\u001b[A\n",
            "Copying files: 87238 files [02:14, 483.82 files/s]\u001b[A\n",
            "Copying files: 87300 files [02:14, 516.32 files/s]\u001b[A\n",
            "Copying files: 87375 files [02:14, 567.56 files/s]\u001b[A\n",
            "Copying files: 87442 files [02:15, 594.54 files/s]\u001b[A\n",
            "Copying files: 87540 files [02:15, 619.13 files/s]\u001b[A\n",
            "Copying files: 87612 files [02:15, 645.78 files/s]\u001b[A\n",
            "Copying files: 87685 files [02:15, 668.80 files/s]\u001b[A\n",
            "Copying files: 87757 files [02:15, 682.96 files/s]\u001b[A\n",
            "Copying files: 87827 files [02:15, 685.02 files/s]\u001b[A\n",
            "Copying files: 87901 files [02:15, 700.14 files/s]\u001b[A\n",
            "Copying files: 87972 files [02:15, 700.36 files/s]\u001b[A\n",
            "Copying files: 88043 files [02:15, 631.52 files/s]\u001b[A\n",
            "Copying files: 88108 files [02:16, 546.69 files/s]\u001b[A\n",
            "Copying files: 88166 files [02:16, 510.77 files/s]\u001b[A\n",
            "Copying files: 88221 files [02:16, 520.73 files/s]\u001b[A\n",
            "Copying files: 88275 files [02:16, 525.71 files/s]\u001b[A\n",
            "Copying files: 88342 files [02:16, 560.16 files/s]\u001b[A\n",
            "Copying files: 88416 files [02:16, 600.21 files/s]\u001b[A\n",
            "Copying files: 88492 files [02:16, 638.03 files/s]\u001b[A\n",
            "Copying files: 88558 files [02:16, 636.63 files/s]\u001b[A\n",
            "Copying files: 88636 files [02:16, 672.50 files/s]\u001b[A\n",
            "Copying files: 88724 files [02:17, 624.71 files/s]\u001b[A\n",
            "Copying files: 88808 files [02:17, 676.35 files/s]\u001b[A\n",
            "Copying files: 88879 files [02:17, 683.61 files/s]\u001b[A\n",
            "Copying files: 88950 files [02:17, 690.37 files/s]\u001b[A\n",
            "Copying files: 89021 files [02:17, 584.52 files/s]\u001b[A\n",
            "Copying files: 89084 files [02:17, 515.75 files/s]\u001b[A\n",
            "Copying files: 89140 files [02:17, 489.98 files/s]\u001b[A\n",
            "Copying files: 89194 files [02:17, 503.10 files/s]\u001b[A\n",
            "Copying files: 89247 files [02:18, 508.69 files/s]\u001b[A\n",
            "Copying files: 89300 files [02:18, 503.78 files/s]\u001b[A\n",
            "Copying files: 89352 files [02:18, 272.16 files/s]\u001b[A\n",
            "Copying files: 89418 files [02:18, 329.58 files/s]\u001b[A\n",
            "Copying files: 89511 files [02:18, 408.21 files/s]\u001b[A\n",
            "Copying files: 89581 files [02:18, 465.60 files/s]\u001b[A\n",
            "Copying files: 89650 files [02:18, 464.51 files/s]\u001b[A\n",
            "Copying files: 89720 files [02:19, 516.63 files/s]\u001b[A\n",
            "Copying files: 89788 files [02:19, 552.58 files/s]\u001b[A\n",
            "Copying files: 89868 files [02:19, 608.99 files/s]\u001b[A\n",
            "Copying files: 89937 files [02:19, 620.16 files/s]\u001b[A\n",
            "Copying files: 90005 files [02:19, 596.41 files/s]\u001b[A\n",
            "Copying files: 90069 files [02:19, 485.55 files/s]\u001b[A\n",
            "Copying files: 90124 files [02:19, 463.47 files/s]\u001b[A\n",
            "Copying files: 90175 files [02:19, 469.52 files/s]\u001b[A\n",
            "Copying files: 90227 files [02:20, 481.59 files/s]\u001b[A\n",
            "Copying files: 90286 files [02:20, 508.87 files/s]\u001b[A\n",
            "Copying files: 90352 files [02:20, 545.03 files/s]\u001b[A\n",
            "Copying files: 90414 files [02:20, 563.92 files/s]\u001b[A\n",
            "Copying files: 90477 files [02:20, 528.91 files/s]\u001b[A\n",
            "Copying files: 90538 files [02:20, 549.86 files/s]\u001b[A\n",
            "Copying files: 90599 files [02:20, 561.15 files/s]\u001b[A\n",
            "Copying files: 90660 files [02:20, 573.52 files/s]\u001b[A\n",
            "Copying files: 90730 files [02:20, 604.21 files/s]\u001b[A\n",
            "Copying files: 90792 files [02:20, 607.00 files/s]\u001b[A\n",
            "Copying files: 90854 files [02:21, 597.95 files/s]\u001b[A\n",
            "Copying files: 90926 files [02:21, 629.74 files/s]\u001b[A\n",
            "Copying files: 91001 files [02:21, 633.95 files/s]\u001b[A\n",
            "Copying files: 91065 files [02:21, 510.36 files/s]\u001b[A\n",
            "Copying files: 91121 files [02:21, 474.84 files/s]\u001b[A\n",
            "Copying files: 91172 files [02:21, 472.47 files/s]\u001b[A\n",
            "Copying files: 91222 files [02:21, 463.85 files/s]\u001b[A\n",
            "Copying files: 91272 files [02:21, 470.10 files/s]\u001b[A\n",
            "Copying files: 91352 files [02:22, 511.22 files/s]\u001b[A\n",
            "Copying files: 91419 files [02:22, 550.11 files/s]\u001b[A\n",
            "Copying files: 91489 files [02:22, 587.57 files/s]\u001b[A\n",
            "Copying files: 91557 files [02:22, 612.45 files/s]\u001b[A\n",
            "Copying files: 91623 files [02:22, 623.00 files/s]\u001b[A\n",
            "Copying files: 91690 files [02:22, 635.71 files/s]\u001b[A\n",
            "Copying files: 91755 files [02:22, 637.99 files/s]\u001b[A\n",
            "Copying files: 91824 files [02:22, 652.47 files/s]\u001b[A\n",
            "Copying files: 91895 files [02:22, 667.24 files/s]\u001b[A\n",
            "Copying files: 91966 files [02:22, 679.48 files/s]\u001b[A\n",
            "Copying files: 92035 files [02:23, 633.14 files/s]\u001b[A\n",
            "Copying files: 92100 files [02:23, 538.35 files/s]\u001b[A\n",
            "Copying files: 92157 files [02:23, 518.80 files/s]\u001b[A\n",
            "Copying files: 92212 files [02:23, 527.46 files/s]\u001b[A\n",
            "Copying files: 92271 files [02:23, 543.51 files/s]\u001b[A\n",
            "Copying files: 92343 files [02:23, 586.35 files/s]\u001b[A\n",
            "Copying files: 92408 files [02:23, 602.94 files/s]\u001b[A\n",
            "Copying files: 92475 files [02:23, 620.46 files/s]\u001b[A\n",
            "Copying files: 92542 files [02:23, 631.15 files/s]\u001b[A\n",
            "Copying files: 92624 files [02:24, 677.76 files/s]\u001b[A\n",
            "Copying files: 92695 files [02:24, 686.71 files/s]\u001b[A\n",
            "Copying files: 92769 files [02:24, 701.75 files/s]\u001b[A\n",
            "Copying files: 92841 files [02:24, 671.58 files/s]\u001b[A\n",
            "Copying files: 92910 files [02:24, 630.81 files/s]\u001b[A\n",
            "Copying files: 92992 files [02:24, 608.01 files/s]\u001b[A\n",
            "Copying files: 93054 files [02:24, 515.69 files/s]\u001b[A\n",
            "Copying files: 93109 files [02:24, 475.68 files/s]\u001b[A\n",
            "Copying files: 93160 files [02:25, 454.07 files/s]\u001b[A\n",
            "Copying files: 93213 files [02:25, 473.85 files/s]\u001b[A\n",
            "Copying files: 93263 files [02:25, 475.68 files/s]\u001b[A\n",
            "Copying files: 93316 files [02:25, 487.78 files/s]\u001b[A\n",
            "Copying files: 93381 files [02:25, 522.62 files/s]\u001b[A\n",
            "Copying files: 93472 files [02:25, 598.18 files/s]\u001b[A\n",
            "Copying files: 93544 files [02:25, 629.06 files/s]\u001b[A\n",
            "Copying files: 93611 files [02:25, 620.91 files/s]\u001b[A\n",
            "Copying files: 93689 files [02:25, 590.48 files/s]\u001b[A\n",
            "Copying files: 93760 files [02:26, 621.48 files/s]\u001b[A\n",
            "Copying files: 93830 files [02:26, 642.66 files/s]\u001b[A\n",
            "Copying files: 93896 files [02:26, 644.54 files/s]\u001b[A\n",
            "Copying files: 93971 files [02:26, 672.71 files/s]\u001b[A\n",
            "Copying files: 94040 files [02:26, 597.28 files/s]\u001b[A\n",
            "Copying files: 94103 files [02:26, 529.75 files/s]\u001b[A\n",
            "Copying files: 94160 files [02:26, 493.00 files/s]\u001b[A\n",
            "Copying files: 94219 files [02:26, 518.06 files/s]\u001b[A\n",
            "Copying files: 94277 files [02:27, 534.20 files/s]\u001b[A\n",
            "Copying files: 94343 files [02:27, 564.83 files/s]\u001b[A\n",
            "Copying files: 94414 files [02:27, 600.36 files/s]\u001b[A\n",
            "Copying files: 94480 files [02:27, 615.99 files/s]\u001b[A\n",
            "Copying files: 94543 files [02:27, 592.16 files/s]\u001b[A\n",
            "Copying files: 94631 files [02:27, 594.56 files/s]\u001b[A\n",
            "Copying files: 94700 files [02:27, 619.88 files/s]\u001b[A\n",
            "Copying files: 94770 files [02:27, 641.78 files/s]\u001b[A\n",
            "Copying files: 94836 files [02:27, 647.13 files/s]\u001b[A\n",
            "Copying files: 94902 files [02:27, 650.94 files/s]\u001b[A\n",
            "Copying files: 94968 files [02:28, 651.71 files/s]\u001b[A\n",
            "Copying files: 95034 files [02:28, 277.06 files/s]\u001b[A\n",
            "Copying files: 95084 files [02:28, 300.69 files/s]\u001b[A\n",
            "Copying files: 95130 files [02:28, 329.03 files/s]\u001b[A\n",
            "Copying files: 95184 files [02:28, 372.04 files/s]\u001b[A\n",
            "Copying files: 95237 files [02:29, 407.17 files/s]\u001b[A\n",
            "Copying files: 95292 files [02:29, 441.24 files/s]\u001b[A\n",
            "Copying files: 95368 files [02:29, 502.21 files/s]\u001b[A\n",
            "Copying files: 95436 files [02:29, 519.91 files/s]\u001b[A\n",
            "Copying files: 95511 files [02:29, 572.40 files/s]\u001b[A\n",
            "Copying files: 95574 files [02:29, 582.85 files/s]\u001b[A\n",
            "Copying files: 95641 files [02:29, 605.45 files/s]\u001b[A\n",
            "Copying files: 95712 files [02:29, 632.75 files/s]\u001b[A\n",
            "Copying files: 95791 files [02:29, 672.51 files/s]\u001b[A\n",
            "Copying files: 95861 files [02:30, 668.05 files/s]\u001b[A\n",
            "Copying files: 95930 files [02:30, 672.21 files/s]\u001b[A\n",
            "Copying files: 96001 files [02:30, 652.90 files/s]\u001b[A\n",
            "Copying files: 96068 files [02:30, 541.63 files/s]\u001b[A\n",
            "Copying files: 96126 files [02:30, 504.73 files/s]\u001b[A\n",
            "Copying files: 96190 files [02:30, 536.49 files/s]\u001b[A\n",
            "Copying files: 96249 files [02:30, 548.77 files/s]\u001b[A\n",
            "Copying files: 96306 files [02:30, 551.33 files/s]\u001b[A\n",
            "Copying files: 96377 files [02:30, 588.44 files/s]\u001b[A\n",
            "Copying files: 96455 files [02:31, 632.16 files/s]\u001b[A\n",
            "Copying files: 96543 files [02:31, 688.72 files/s]\u001b[A\n",
            "Copying files: 96625 files [02:31, 635.89 files/s]\u001b[A\n",
            "Copying files: 96698 files [02:31, 655.97 files/s]\u001b[A\n",
            "Copying files: 96771 files [02:31, 669.17 files/s]\u001b[A\n",
            "Copying files: 96853 files [02:31, 707.15 files/s]\u001b[A\n",
            "Copying files: 96930 files [02:31, 724.71 files/s]\u001b[A\n",
            "Copying files: 97004 files [02:31, 674.69 files/s]\u001b[A\n",
            "Copying files: 97074 files [02:32, 518.08 files/s]\u001b[A\n",
            "Copying files: 97133 files [02:32, 491.60 files/s]\u001b[A\n",
            "Copying files: 97188 files [02:32, 497.07 files/s]\u001b[A\n",
            "Copying files: 97244 files [02:32, 511.65 files/s]\u001b[A\n",
            "Copying files: 97307 files [02:32, 541.23 files/s]\u001b[A\n",
            "Copying files: 97373 files [02:32, 555.08 files/s]\u001b[A\n",
            "Copying files: 97440 files [02:32, 583.97 files/s]\u001b[A\n",
            "Copying files: 97507 files [02:32, 607.02 files/s]\u001b[A\n",
            "Copying files: 97573 files [02:32, 621.79 files/s]\u001b[A\n",
            "Copying files: 97640 files [02:33, 635.49 files/s]\u001b[A\n",
            "Copying files: 97705 files [02:33, 635.07 files/s]\u001b[A\n",
            "Copying files: 97770 files [02:33, 618.31 files/s]\u001b[A\n",
            "Copying files: 97870 files [02:33, 697.98 files/s]\u001b[A\n",
            "Copying files: 97944 files [02:33, 691.80 files/s]\u001b[A\n",
            "Copying files: 98016 files [02:33, 596.89 files/s]\u001b[A\n",
            "Copying files: 98080 files [02:33, 503.97 files/s]\u001b[A\n",
            "Copying files: 98136 files [02:33, 469.20 files/s]\u001b[A\n",
            "Copying files: 98188 files [02:33, 479.67 files/s]\u001b[A\n",
            "Copying files: 98240 files [02:34, 482.65 files/s]\u001b[A\n",
            "Copying files: 98305 files [02:34, 522.47 files/s]\u001b[A\n",
            "Copying files: 98367 files [02:34, 547.12 files/s]\u001b[A\n",
            "Copying files: 98424 files [02:34, 552.92 files/s]\u001b[A\n",
            "Copying files: 98487 files [02:34, 572.85 files/s]\u001b[A\n",
            "Copying files: 98548 files [02:34, 580.32 files/s]\u001b[A\n",
            "Copying files: 98619 files [02:34, 613.25 files/s]\u001b[A\n",
            "Copying files: 98682 files [02:34, 577.79 files/s]\u001b[A\n",
            "Copying files: 98771 files [02:34, 645.52 files/s]\u001b[A\n",
            "Copying files: 98839 files [02:35, 538.41 files/s]\u001b[A\n",
            "Copying files: 98910 files [02:35, 580.27 files/s]\u001b[A\n",
            "Copying files: 98973 files [02:35, 589.50 files/s]\u001b[A\n",
            "Copying files: 99036 files [02:35, 568.53 files/s]\u001b[A\n",
            "Copying files: 99096 files [02:35, 500.82 files/s]\u001b[A\n",
            "Copying files: 99150 files [02:35, 481.27 files/s]\u001b[A\n",
            "Copying files: 99201 files [02:35, 488.90 files/s]\u001b[A\n",
            "Copying files: 99255 files [02:35, 500.72 files/s]\u001b[A\n",
            "Copying files: 99321 files [02:36, 536.45 files/s]\u001b[A\n",
            "Copying files: 99389 files [02:36, 571.62 files/s]\u001b[A\n",
            "Copying files: 99467 files [02:36, 621.01 files/s]\u001b[A\n",
            "Copying files: 99541 files [02:36, 651.06 files/s]\u001b[A\n",
            "Copying files: 99616 files [02:36, 677.58 files/s]\u001b[A\n",
            "Copying files: 99686 files [02:36, 681.80 files/s]\u001b[A\n",
            "Copying files: 99768 files [02:36, 715.91 files/s]\u001b[A\n",
            "Copying files: 99845 files [02:36, 730.87 files/s]\u001b[A\n",
            "Copying files: 99920 files [02:36, 711.33 files/s]\u001b[A\n",
            "Copying files: 100001 files [02:36, 727.70 files/s]\u001b[A\n",
            "Copying files: 100075 files [02:37, 583.99 files/s]\u001b[A\n",
            "Copying files: 100139 files [02:37, 540.67 files/s]\u001b[A\n",
            "Copying files: 100198 files [02:37, 539.56 files/s]\u001b[A\n",
            "Copying files: 100256 files [02:37, 549.49 files/s]\u001b[A\n",
            "Copying files: 100334 files [02:37, 602.91 files/s]\u001b[A\n",
            "Copying files: 100405 files [02:37, 629.77 files/s]\u001b[A\n",
            "Copying files: 100488 files [02:37, 631.50 files/s]\u001b[A\n",
            "Copying files: 100561 files [02:37, 657.75 files/s]\u001b[A\n",
            "Copying files: 100629 files [02:38, 593.38 files/s]\u001b[A\n",
            "Copying files: 100726 files [02:38, 671.27 files/s]\u001b[A\n",
            "Copying files: 100799 files [02:38, 577.08 files/s]\u001b[A\n",
            "Copying files: 100863 files [02:38, 591.97 files/s]\u001b[A\n",
            "Copying files: 101000 files [02:38, 636.80 files/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g97IGfJkjvyP"
      },
      "source": [
        "**Baseline Model Development**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXnM1OeMjvkC"
      },
      "source": [
        "Model 1 - Simple Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ3xUfqd7MTz"
      },
      "source": [
        "#init some parameters\r\n",
        "target_size = (64,64)\r\n",
        "input_shape = (64,64,3) # 3 do rgb\r\n",
        "batch_size=64\r\n",
        "categorias=101\r\n",
        "steps_per_epoch=160\r\n",
        "validation_steps=10\r\n",
        "epochs=100\r\n",
        "patience=30"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxb5VB_ZkT4W",
        "outputId": "1b989e12-0289-4047-c7b2-9007d3cd8bef"
      },
      "source": [
        "#get dataset\r\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\r\n",
        "valid_datagen = ImageDataGenerator(rescale = 1./255)\r\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\r\n",
        "\r\n",
        "\r\n",
        "train_generator = train_datagen.flow_from_directory(directory=\"/content/output/train\",\r\n",
        "                                                    subset=\"training\",\r\n",
        "                                                    batch_size=batch_size,\r\n",
        "                                                    shuffle=True,\r\n",
        "                                                    class_mode=\"categorical\",\r\n",
        "                                                    target_size=target_size,\r\n",
        "                                                    seed=42)\r\n",
        "\r\n",
        "valid_generator=valid_datagen.flow_from_directory(directory=\"/content/output/val\",\r\n",
        "                                                  batch_size=batch_size,\r\n",
        "                                                  shuffle=True,\r\n",
        "                                                  class_mode=\"categorical\",\r\n",
        "                                                  target_size=target_size,\r\n",
        "                                                  seed=42)\r\n",
        "\r\n",
        "test_generator=test_datagen.flow_from_directory(directory=\"/content/output/test\",\r\n",
        "                                                  batch_size=batch_size,\r\n",
        "                                                  shuffle=True,\r\n",
        "                                                  class_mode=\"categorical\",\r\n",
        "                                                  target_size=target_size,\r\n",
        "                                                  seed=42)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 70700 images belonging to 101 classes.\n",
            "Found 20200 images belonging to 101 classes.\n",
            "Found 10100 images belonging to 101 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMZ61toPkT1E",
        "outputId": "6eb33652-5780-4ed8-a124-f5d23d0d4f40"
      },
      "source": [
        "#Input Layer\r\n",
        "baseline_model_1 = Sequential()\r\n",
        "\r\n",
        "baseline_model_1.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model_1.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_1.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_1.add(Dense(2048, activation='relu'))\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_1.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_1.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_45 (Conv2D)           (None, 62, 62, 128)       3584      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 123008)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2048)              251922432 \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 101)               206949    \n",
            "=================================================================\n",
            "Total params: 252,132,965\n",
            "Trainable params: 252,132,965\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yht656TKlytu"
      },
      "source": [
        "#compile\r\n",
        "opt_adam = Adam()\r\n",
        "\r\n",
        "baseline_model_1.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_adam,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62uPuYyulyqG",
        "outputId": "fd45d16a-3ddf-4795-aa07-4f551cf67f0c"
      },
      "source": [
        "#training\r\n",
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "history_baseline_model_1 = baseline_model_1.fit_generator(train_generator,\r\n",
        "                              steps_per_epoch=steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator,\r\n",
        "                              validation_steps=validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_1.save('food_baseline_model_1.h5')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "160/160 [==============================] - 58s 360ms/step - loss: 9.5363 - acc: 0.0100 - val_loss: 4.5150 - val_acc: 0.0203\n",
            "Epoch 2/100\n",
            "160/160 [==============================] - 54s 340ms/step - loss: 4.4052 - acc: 0.0445 - val_loss: 4.2939 - val_acc: 0.0656\n",
            "Epoch 3/100\n",
            "160/160 [==============================] - 52s 323ms/step - loss: 4.1753 - acc: 0.0818 - val_loss: 4.1003 - val_acc: 0.0906\n",
            "Epoch 4/100\n",
            "160/160 [==============================] - 50s 313ms/step - loss: 3.9793 - acc: 0.1157 - val_loss: 4.1202 - val_acc: 0.0922\n",
            "Epoch 5/100\n",
            "160/160 [==============================] - 49s 303ms/step - loss: 3.7380 - acc: 0.1779 - val_loss: 3.9768 - val_acc: 0.1078\n",
            "Epoch 6/100\n",
            "160/160 [==============================] - 47s 294ms/step - loss: 3.5233 - acc: 0.2239 - val_loss: 3.9326 - val_acc: 0.1203\n",
            "Epoch 7/100\n",
            "160/160 [==============================] - 47s 292ms/step - loss: 3.2720 - acc: 0.2773 - val_loss: 4.2932 - val_acc: 0.0812\n",
            "Epoch 8/100\n",
            "160/160 [==============================] - 46s 290ms/step - loss: 3.0093 - acc: 0.3370 - val_loss: 4.0126 - val_acc: 0.1203\n",
            "Epoch 9/100\n",
            "160/160 [==============================] - 46s 288ms/step - loss: 2.8006 - acc: 0.3895 - val_loss: 4.0010 - val_acc: 0.1469\n",
            "Epoch 10/100\n",
            "160/160 [==============================] - 46s 285ms/step - loss: 2.5425 - acc: 0.4507 - val_loss: 4.2033 - val_acc: 0.1234\n",
            "Epoch 11/100\n",
            "160/160 [==============================] - 46s 285ms/step - loss: 2.3109 - acc: 0.5069 - val_loss: 4.1788 - val_acc: 0.1234\n",
            "Epoch 12/100\n",
            "160/160 [==============================] - 45s 283ms/step - loss: 2.0793 - acc: 0.5563 - val_loss: 4.3283 - val_acc: 0.1234\n",
            "Epoch 13/100\n",
            "160/160 [==============================] - 45s 281ms/step - loss: 1.8502 - acc: 0.6068 - val_loss: 4.2896 - val_acc: 0.1406\n",
            "Epoch 14/100\n",
            "160/160 [==============================] - 45s 279ms/step - loss: 1.6721 - acc: 0.6519 - val_loss: 4.2864 - val_acc: 0.1094\n",
            "Epoch 15/100\n",
            "160/160 [==============================] - 44s 276ms/step - loss: 1.5605 - acc: 0.6805 - val_loss: 4.4654 - val_acc: 0.1203\n",
            "Epoch 16/100\n",
            "160/160 [==============================] - 44s 275ms/step - loss: 1.3640 - acc: 0.7166 - val_loss: 4.6127 - val_acc: 0.0828\n",
            "Epoch 17/100\n",
            "160/160 [==============================] - 44s 275ms/step - loss: 1.2474 - acc: 0.7430 - val_loss: 4.3408 - val_acc: 0.1016\n",
            "Epoch 18/100\n",
            "160/160 [==============================] - 44s 273ms/step - loss: 1.1232 - acc: 0.7676 - val_loss: 4.5565 - val_acc: 0.1078\n",
            "Epoch 19/100\n",
            "160/160 [==============================] - 44s 274ms/step - loss: 0.9324 - acc: 0.8092 - val_loss: 4.7466 - val_acc: 0.1406\n",
            "Epoch 20/100\n",
            "160/160 [==============================] - 44s 273ms/step - loss: 0.8809 - acc: 0.8238 - val_loss: 4.8271 - val_acc: 0.1187\n",
            "Epoch 21/100\n",
            "160/160 [==============================] - 43s 271ms/step - loss: 0.7605 - acc: 0.8495 - val_loss: 4.9880 - val_acc: 0.1109\n",
            "Epoch 22/100\n",
            "160/160 [==============================] - 43s 271ms/step - loss: 0.7031 - acc: 0.8574 - val_loss: 4.7651 - val_acc: 0.1109\n",
            "Epoch 23/100\n",
            "160/160 [==============================] - 44s 272ms/step - loss: 0.6162 - acc: 0.8756 - val_loss: 4.9523 - val_acc: 0.1031\n",
            "Epoch 24/100\n",
            "160/160 [==============================] - 43s 270ms/step - loss: 0.5558 - acc: 0.8926 - val_loss: 5.4207 - val_acc: 0.1156\n",
            "Epoch 25/100\n",
            "160/160 [==============================] - 43s 270ms/step - loss: 0.4902 - acc: 0.9050 - val_loss: 5.4485 - val_acc: 0.1063\n",
            "Epoch 26/100\n",
            "160/160 [==============================] - 43s 272ms/step - loss: 0.4464 - acc: 0.9120 - val_loss: 5.1025 - val_acc: 0.1109\n",
            "Epoch 27/100\n",
            "160/160 [==============================] - 43s 270ms/step - loss: 0.3986 - acc: 0.9241 - val_loss: 5.7756 - val_acc: 0.0969\n",
            "Epoch 28/100\n",
            "160/160 [==============================] - 43s 270ms/step - loss: 0.3647 - acc: 0.9303 - val_loss: 5.3497 - val_acc: 0.1000\n",
            "Epoch 29/100\n",
            "160/160 [==============================] - 43s 269ms/step - loss: 0.2971 - acc: 0.9423 - val_loss: 5.6096 - val_acc: 0.1031\n",
            "Epoch 30/100\n",
            "160/160 [==============================] - 43s 267ms/step - loss: 0.2848 - acc: 0.9468 - val_loss: 5.6974 - val_acc: 0.1109\n",
            "Epoch 31/100\n",
            "160/160 [==============================] - 43s 268ms/step - loss: 0.2446 - acc: 0.9532 - val_loss: 5.7898 - val_acc: 0.1156\n",
            "Epoch 32/100\n",
            "160/160 [==============================] - 43s 267ms/step - loss: 0.2549 - acc: 0.9510 - val_loss: 6.0279 - val_acc: 0.1000\n",
            "Epoch 33/100\n",
            "160/160 [==============================] - 43s 269ms/step - loss: 0.1929 - acc: 0.9643 - val_loss: 5.8238 - val_acc: 0.1203\n",
            "Epoch 34/100\n",
            "160/160 [==============================] - 43s 267ms/step - loss: 0.1672 - acc: 0.9714 - val_loss: 6.2817 - val_acc: 0.1250\n",
            "Epoch 35/100\n",
            "160/160 [==============================] - 43s 267ms/step - loss: 0.1427 - acc: 0.9731 - val_loss: 6.3982 - val_acc: 0.1156\n",
            "Epoch 36/100\n",
            "160/160 [==============================] - 43s 267ms/step - loss: 0.1691 - acc: 0.9687 - val_loss: 6.3401 - val_acc: 0.0984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCBokBRToi1J"
      },
      "source": [
        "Model 2 - Add Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RZ-sbV4oSBd",
        "outputId": "a991fa06-ca0d-42d6-eca9-290f0c3db01c"
      },
      "source": [
        "#Input Layer\r\n",
        "baseline_model_2 = Sequential()\r\n",
        "\r\n",
        "baseline_model_2.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model_2.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_2.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_2.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_2.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_2.add(Dense(2048, activation='relu'))\r\n",
        "\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_2.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_2.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_46 (Conv2D)           (None, 62, 62, 128)       3584      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 31, 31, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 29, 29, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 50176)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2048)              102762496 \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 101)               206949    \n",
            "=================================================================\n",
            "Total params: 103,268,197\n",
            "Trainable params: 103,268,197\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxFkxM4WoR42"
      },
      "source": [
        "\r\n",
        "baseline_model_2.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_adam,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6_E0rxBoRxo",
        "outputId": "a2697d5e-7e47-4df3-ac64-5340f4b7089f"
      },
      "source": [
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model_2 = baseline_model_2.fit_generator(train_generator,\r\n",
        "                              steps_per_epoch=steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator,\r\n",
        "                              validation_steps=validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_2.save('food_baseline_model_2.h5')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "160/160 [==============================] - 58s 354ms/step - loss: 4.9451 - acc: 0.0094 - val_loss: 4.5476 - val_acc: 0.0312\n",
            "Epoch 2/100\n",
            "160/160 [==============================] - 53s 333ms/step - loss: 4.5208 - acc: 0.0277 - val_loss: 4.4208 - val_acc: 0.0344\n",
            "Epoch 3/100\n",
            "160/160 [==============================] - 50s 315ms/step - loss: 4.3726 - acc: 0.0462 - val_loss: 4.3272 - val_acc: 0.0516\n",
            "Epoch 4/100\n",
            "160/160 [==============================] - 48s 300ms/step - loss: 4.2860 - acc: 0.0582 - val_loss: 4.2483 - val_acc: 0.0688\n",
            "Epoch 5/100\n",
            "160/160 [==============================] - 47s 291ms/step - loss: 4.1904 - acc: 0.0663 - val_loss: 4.1122 - val_acc: 0.0812\n",
            "Epoch 6/100\n",
            "160/160 [==============================] - 45s 283ms/step - loss: 4.1228 - acc: 0.0824 - val_loss: 4.1023 - val_acc: 0.0688\n",
            "Epoch 7/100\n",
            "160/160 [==============================] - 44s 278ms/step - loss: 4.0274 - acc: 0.0969 - val_loss: 4.1030 - val_acc: 0.0938\n",
            "Epoch 8/100\n",
            "160/160 [==============================] - 44s 275ms/step - loss: 3.9587 - acc: 0.1137 - val_loss: 4.1422 - val_acc: 0.0922\n",
            "Epoch 9/100\n",
            "160/160 [==============================] - 44s 273ms/step - loss: 3.8774 - acc: 0.1228 - val_loss: 3.9110 - val_acc: 0.1172\n",
            "Epoch 10/100\n",
            "160/160 [==============================] - 43s 269ms/step - loss: 3.8087 - acc: 0.1385 - val_loss: 3.9183 - val_acc: 0.1156\n",
            "Epoch 11/100\n",
            "160/160 [==============================] - 43s 268ms/step - loss: 3.7299 - acc: 0.1466 - val_loss: 4.0526 - val_acc: 0.1016\n",
            "Epoch 12/100\n",
            "160/160 [==============================] - 43s 267ms/step - loss: 3.6103 - acc: 0.1736 - val_loss: 3.9180 - val_acc: 0.1141\n",
            "Epoch 13/100\n",
            "160/160 [==============================] - 42s 265ms/step - loss: 3.5360 - acc: 0.1883 - val_loss: 3.8984 - val_acc: 0.1344\n",
            "Epoch 14/100\n",
            "160/160 [==============================] - 42s 263ms/step - loss: 3.3363 - acc: 0.2289 - val_loss: 3.9349 - val_acc: 0.1297\n",
            "Epoch 15/100\n",
            "160/160 [==============================] - 42s 264ms/step - loss: 3.2015 - acc: 0.2625 - val_loss: 3.8047 - val_acc: 0.1500\n",
            "Epoch 16/100\n",
            "160/160 [==============================] - 43s 267ms/step - loss: 3.0347 - acc: 0.3072 - val_loss: 3.8181 - val_acc: 0.1641\n",
            "Epoch 17/100\n",
            "160/160 [==============================] - 42s 263ms/step - loss: 2.8604 - acc: 0.3382 - val_loss: 3.8261 - val_acc: 0.1656\n",
            "Epoch 18/100\n",
            "160/160 [==============================] - 42s 263ms/step - loss: 2.7050 - acc: 0.3760 - val_loss: 4.0537 - val_acc: 0.1203\n",
            "Epoch 19/100\n",
            "160/160 [==============================] - 42s 263ms/step - loss: 2.5232 - acc: 0.4307 - val_loss: 4.0465 - val_acc: 0.1422\n",
            "Epoch 20/100\n",
            "160/160 [==============================] - 42s 262ms/step - loss: 2.2866 - acc: 0.4851 - val_loss: 4.0617 - val_acc: 0.1594\n",
            "Epoch 21/100\n",
            "160/160 [==============================] - 42s 262ms/step - loss: 2.1273 - acc: 0.5138 - val_loss: 4.0769 - val_acc: 0.1453\n",
            "Epoch 22/100\n",
            "160/160 [==============================] - 42s 261ms/step - loss: 1.9981 - acc: 0.5538 - val_loss: 4.2445 - val_acc: 0.1516\n",
            "Epoch 23/100\n",
            "160/160 [==============================] - 42s 261ms/step - loss: 1.8205 - acc: 0.6007 - val_loss: 4.3703 - val_acc: 0.1422\n",
            "Epoch 24/100\n",
            "160/160 [==============================] - 42s 260ms/step - loss: 1.5982 - acc: 0.6454 - val_loss: 4.3383 - val_acc: 0.1609\n",
            "Epoch 25/100\n",
            "160/160 [==============================] - 41s 259ms/step - loss: 1.4663 - acc: 0.6785 - val_loss: 4.4575 - val_acc: 0.1437\n",
            "Epoch 26/100\n",
            "160/160 [==============================] - 41s 259ms/step - loss: 1.3284 - acc: 0.7129 - val_loss: 4.7026 - val_acc: 0.1437\n",
            "Epoch 27/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 1.2055 - acc: 0.7400 - val_loss: 4.7070 - val_acc: 0.1437\n",
            "Epoch 28/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 1.1161 - acc: 0.7652 - val_loss: 4.8845 - val_acc: 0.1562\n",
            "Epoch 29/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 1.0084 - acc: 0.7888 - val_loss: 4.9180 - val_acc: 0.1484\n",
            "Epoch 30/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 0.9068 - acc: 0.8076 - val_loss: 5.2167 - val_acc: 0.1266\n",
            "Epoch 31/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 0.8515 - acc: 0.8258 - val_loss: 5.3646 - val_acc: 0.1219\n",
            "Epoch 32/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 0.7121 - acc: 0.8467 - val_loss: 5.1701 - val_acc: 0.1344\n",
            "Epoch 33/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 0.6623 - acc: 0.8628 - val_loss: 5.2898 - val_acc: 0.1266\n",
            "Epoch 34/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 0.5822 - acc: 0.8773 - val_loss: 5.2918 - val_acc: 0.1516\n",
            "Epoch 35/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 0.5241 - acc: 0.8950 - val_loss: 5.5101 - val_acc: 0.1453\n",
            "Epoch 36/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 0.4727 - acc: 0.9049 - val_loss: 5.6430 - val_acc: 0.1266\n",
            "Epoch 37/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 0.4013 - acc: 0.9149 - val_loss: 5.7695 - val_acc: 0.1172\n",
            "Epoch 38/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 0.3620 - acc: 0.9263 - val_loss: 5.8322 - val_acc: 0.1312\n",
            "Epoch 39/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 0.3405 - acc: 0.9308 - val_loss: 6.0015 - val_acc: 0.1219\n",
            "Epoch 40/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 0.3319 - acc: 0.9316 - val_loss: 5.7574 - val_acc: 0.1453\n",
            "Epoch 41/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 0.2855 - acc: 0.9405 - val_loss: 6.1827 - val_acc: 0.1391\n",
            "Epoch 42/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 0.2440 - acc: 0.9502 - val_loss: 6.2141 - val_acc: 0.1609\n",
            "Epoch 43/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 0.2430 - acc: 0.9486 - val_loss: 6.5418 - val_acc: 0.1281\n",
            "Epoch 44/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 0.1979 - acc: 0.9600 - val_loss: 6.2659 - val_acc: 0.1453\n",
            "Epoch 45/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 0.1973 - acc: 0.9618 - val_loss: 6.6542 - val_acc: 0.1375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj_fQmtEHbEu"
      },
      "source": [
        "Better results in validation we will increase another layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miDNb6sTpbiX"
      },
      "source": [
        "Model 3 - Add Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbM6KH9YpXmw",
        "outputId": "97407263-dc40-4c90-9d91-ddb355bde058"
      },
      "source": [
        "#Input Layer\r\n",
        "baseline_model_3 = Sequential()\r\n",
        "\r\n",
        "baseline_model_3.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model_3.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_3.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_3.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_3.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_3.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_3.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_3.add(Dense(2048, activation='relu'))\r\n",
        "\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_3.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_3.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_48 (Conv2D)           (None, 62, 62, 128)       3584      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 31, 31, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 29, 29, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 12, 12, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2048)              37750784  \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 101)               206949    \n",
            "=================================================================\n",
            "Total params: 39,436,645\n",
            "Trainable params: 39,436,645\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEzKtjqqpXkI"
      },
      "source": [
        "\r\n",
        "\r\n",
        "baseline_model_3.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_adam,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGz6Oe8fpXgk",
        "outputId": "3c306c1c-da82-466e-ed3c-86107f5bbcfc"
      },
      "source": [
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model_3 = baseline_model_3.fit_generator(train_generator,\r\n",
        "                              steps_per_epoch=steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator,\r\n",
        "                              validation_steps=validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_3.save('food_baseline_model_3.h5')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "160/160 [==============================] - 43s 262ms/step - loss: 6.0338 - acc: 0.0090 - val_loss: 4.6113 - val_acc: 0.0094\n",
            "Epoch 2/100\n",
            "160/160 [==============================] - 42s 263ms/step - loss: 4.5876 - acc: 0.0140 - val_loss: 4.5471 - val_acc: 0.0203\n",
            "Epoch 3/100\n",
            "160/160 [==============================] - 42s 264ms/step - loss: 4.5196 - acc: 0.0200 - val_loss: 4.5045 - val_acc: 0.0219\n",
            "Epoch 4/100\n",
            "160/160 [==============================] - 42s 263ms/step - loss: 4.5076 - acc: 0.0251 - val_loss: 4.4588 - val_acc: 0.0312\n",
            "Epoch 5/100\n",
            "160/160 [==============================] - 42s 261ms/step - loss: 4.4264 - acc: 0.0352 - val_loss: 4.3760 - val_acc: 0.0453\n",
            "Epoch 6/100\n",
            "160/160 [==============================] - 42s 260ms/step - loss: 4.3734 - acc: 0.0434 - val_loss: 4.3738 - val_acc: 0.0359\n",
            "Epoch 7/100\n",
            "160/160 [==============================] - 41s 259ms/step - loss: 4.3391 - acc: 0.0403 - val_loss: 4.3270 - val_acc: 0.0469\n",
            "Epoch 8/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.3164 - acc: 0.0499 - val_loss: 4.3340 - val_acc: 0.0453\n",
            "Epoch 9/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.2760 - acc: 0.0542 - val_loss: 4.2933 - val_acc: 0.0578\n",
            "Epoch 10/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.2423 - acc: 0.0604 - val_loss: 4.3091 - val_acc: 0.0484\n",
            "Epoch 11/100\n",
            "160/160 [==============================] - 41s 259ms/step - loss: 4.2527 - acc: 0.0584 - val_loss: 4.2948 - val_acc: 0.0516\n",
            "Epoch 12/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.2293 - acc: 0.0585 - val_loss: 4.2729 - val_acc: 0.0469\n",
            "Epoch 13/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.2000 - acc: 0.0619 - val_loss: 4.2369 - val_acc: 0.0625\n",
            "Epoch 14/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.1482 - acc: 0.0740 - val_loss: 4.1642 - val_acc: 0.0859\n",
            "Epoch 15/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.1601 - acc: 0.0765 - val_loss: 4.2398 - val_acc: 0.0625\n",
            "Epoch 16/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.1209 - acc: 0.0791 - val_loss: 4.1687 - val_acc: 0.0781\n",
            "Epoch 17/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.1112 - acc: 0.0800 - val_loss: 4.2036 - val_acc: 0.0641\n",
            "Epoch 18/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.0910 - acc: 0.0785 - val_loss: 4.2095 - val_acc: 0.0703\n",
            "Epoch 19/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.0785 - acc: 0.0861 - val_loss: 4.1257 - val_acc: 0.0719\n",
            "Epoch 20/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.0469 - acc: 0.0892 - val_loss: 4.1256 - val_acc: 0.0797\n",
            "Epoch 21/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.0175 - acc: 0.0939 - val_loss: 4.0788 - val_acc: 0.0797\n",
            "Epoch 22/100\n",
            "160/160 [==============================] - 40s 253ms/step - loss: 3.9911 - acc: 0.0946 - val_loss: 4.0850 - val_acc: 0.0922\n",
            "Epoch 23/100\n",
            "160/160 [==============================] - 41s 253ms/step - loss: 3.9660 - acc: 0.1033 - val_loss: 4.0244 - val_acc: 0.1016\n",
            "Epoch 24/100\n",
            "160/160 [==============================] - 41s 253ms/step - loss: 3.9676 - acc: 0.1039 - val_loss: 4.0914 - val_acc: 0.0703\n",
            "Epoch 25/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.9224 - acc: 0.1092 - val_loss: 4.1377 - val_acc: 0.0891\n",
            "Epoch 26/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.9043 - acc: 0.1161 - val_loss: 3.9842 - val_acc: 0.0875\n",
            "Epoch 27/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.9071 - acc: 0.1098 - val_loss: 4.0160 - val_acc: 0.1031\n",
            "Epoch 28/100\n",
            "160/160 [==============================] - 41s 253ms/step - loss: 3.9121 - acc: 0.1182 - val_loss: 3.9739 - val_acc: 0.1000\n",
            "Epoch 29/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.8838 - acc: 0.1092 - val_loss: 3.9831 - val_acc: 0.0938\n",
            "Epoch 30/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.8279 - acc: 0.1278 - val_loss: 3.9538 - val_acc: 0.0750\n",
            "Epoch 31/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.8271 - acc: 0.1254 - val_loss: 3.9882 - val_acc: 0.1016\n",
            "Epoch 32/100\n",
            "160/160 [==============================] - 41s 253ms/step - loss: 3.8272 - acc: 0.1223 - val_loss: 3.9069 - val_acc: 0.1344\n",
            "Epoch 33/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.8296 - acc: 0.1263 - val_loss: 4.0392 - val_acc: 0.0891\n",
            "Epoch 34/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.8209 - acc: 0.1340 - val_loss: 4.0107 - val_acc: 0.1063\n",
            "Epoch 35/100\n",
            "160/160 [==============================] - 41s 253ms/step - loss: 3.7688 - acc: 0.1311 - val_loss: 3.9119 - val_acc: 0.1141\n",
            "Epoch 36/100\n",
            "160/160 [==============================] - 40s 251ms/step - loss: 3.7778 - acc: 0.1242 - val_loss: 4.0188 - val_acc: 0.1109\n",
            "Epoch 37/100\n",
            "160/160 [==============================] - 41s 253ms/step - loss: 3.7459 - acc: 0.1394 - val_loss: 3.9392 - val_acc: 0.1000\n",
            "Epoch 38/100\n",
            "160/160 [==============================] - 41s 253ms/step - loss: 3.7450 - acc: 0.1384 - val_loss: 3.8227 - val_acc: 0.1109\n",
            "Epoch 39/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.7540 - acc: 0.1310 - val_loss: 3.9242 - val_acc: 0.1078\n",
            "Epoch 40/100\n",
            "160/160 [==============================] - 41s 253ms/step - loss: 3.7007 - acc: 0.1454 - val_loss: 3.9140 - val_acc: 0.1234\n",
            "Epoch 41/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 3.7130 - acc: 0.1492 - val_loss: 3.9509 - val_acc: 0.1047\n",
            "Epoch 42/100\n",
            "160/160 [==============================] - 41s 253ms/step - loss: 3.6919 - acc: 0.1498 - val_loss: 3.9556 - val_acc: 0.0891\n",
            "Epoch 43/100\n",
            "160/160 [==============================] - 40s 253ms/step - loss: 3.6951 - acc: 0.1507 - val_loss: 4.0649 - val_acc: 0.0781\n",
            "Epoch 44/100\n",
            "160/160 [==============================] - 41s 253ms/step - loss: 3.6758 - acc: 0.1493 - val_loss: 3.9352 - val_acc: 0.1156\n",
            "Epoch 45/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.6272 - acc: 0.1595 - val_loss: 3.8863 - val_acc: 0.1234\n",
            "Epoch 46/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.6223 - acc: 0.1605 - val_loss: 4.0004 - val_acc: 0.1125\n",
            "Epoch 47/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.6120 - acc: 0.1642 - val_loss: 3.9424 - val_acc: 0.1031\n",
            "Epoch 48/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.6087 - acc: 0.1596 - val_loss: 4.0676 - val_acc: 0.1063\n",
            "Epoch 49/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 3.5994 - acc: 0.1661 - val_loss: 3.9113 - val_acc: 0.1094\n",
            "Epoch 50/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.5703 - acc: 0.1668 - val_loss: 3.9568 - val_acc: 0.1109\n",
            "Epoch 51/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.5341 - acc: 0.1783 - val_loss: 3.9424 - val_acc: 0.1172\n",
            "Epoch 52/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.5625 - acc: 0.1673 - val_loss: 3.9706 - val_acc: 0.1141\n",
            "Epoch 53/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.5601 - acc: 0.1700 - val_loss: 4.0348 - val_acc: 0.0984\n",
            "Epoch 54/100\n",
            "160/160 [==============================] - 41s 253ms/step - loss: 3.5441 - acc: 0.1686 - val_loss: 4.1044 - val_acc: 0.1109\n",
            "Epoch 55/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 3.5202 - acc: 0.1851 - val_loss: 3.8806 - val_acc: 0.1312\n",
            "Epoch 56/100\n",
            "160/160 [==============================] - 41s 253ms/step - loss: 3.4873 - acc: 0.1840 - val_loss: 4.0278 - val_acc: 0.1109\n",
            "Epoch 57/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 3.4657 - acc: 0.1862 - val_loss: 3.9699 - val_acc: 0.1078\n",
            "Epoch 58/100\n",
            "160/160 [==============================] - 40s 252ms/step - loss: 3.4612 - acc: 0.1886 - val_loss: 4.0228 - val_acc: 0.1266\n",
            "Epoch 59/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.4569 - acc: 0.1878 - val_loss: 3.9733 - val_acc: 0.1203\n",
            "Epoch 60/100\n",
            "160/160 [==============================] - 40s 253ms/step - loss: 3.4393 - acc: 0.1927 - val_loss: 3.9971 - val_acc: 0.1234\n",
            "Epoch 61/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.4045 - acc: 0.2109 - val_loss: 4.1127 - val_acc: 0.0969\n",
            "Epoch 62/100\n",
            "160/160 [==============================] - 41s 253ms/step - loss: 3.4419 - acc: 0.1972 - val_loss: 3.9985 - val_acc: 0.1203\n",
            "Epoch 63/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.4157 - acc: 0.2046 - val_loss: 4.0916 - val_acc: 0.0938\n",
            "Epoch 64/100\n",
            "160/160 [==============================] - 41s 254ms/step - loss: 3.4114 - acc: 0.1981 - val_loss: 3.9870 - val_acc: 0.1187\n",
            "Epoch 65/100\n",
            "160/160 [==============================] - 41s 253ms/step - loss: 3.3711 - acc: 0.2062 - val_loss: 4.0708 - val_acc: 0.1109\n",
            "Epoch 66/100\n",
            "160/160 [==============================] - 40s 252ms/step - loss: 3.3022 - acc: 0.2174 - val_loss: 3.9105 - val_acc: 0.1594\n",
            "Epoch 67/100\n",
            "160/160 [==============================] - 40s 252ms/step - loss: 3.3436 - acc: 0.2085 - val_loss: 4.0489 - val_acc: 0.1250\n",
            "Epoch 68/100\n",
            "160/160 [==============================] - 40s 251ms/step - loss: 3.2886 - acc: 0.2207 - val_loss: 4.1381 - val_acc: 0.1047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD1DuF5bp_41"
      },
      "source": [
        "Model 4 - Add more Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RPXk_64oRXK",
        "outputId": "5f084d4c-d613-45ce-91da-ccbbedf5f4e0"
      },
      "source": [
        "\r\n",
        "#Input Layer\r\n",
        "baseline_model_4 = Sequential()\r\n",
        "\r\n",
        "baseline_model_4.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model_4.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_4.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_4.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_4.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_4.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_4.add(Conv2D(filters=1024, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_4.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_4.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_4.add(Dense(2048, activation='relu'))\r\n",
        "\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_4.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_4.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_51 (Conv2D)           (None, 62, 62, 128)       3584      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 31, 31, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 29, 29, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 12, 12, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_54 (Conv2D)           (None, 4, 4, 1024)        4719616   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2048)              8390656   \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 101)               206949    \n",
            "=================================================================\n",
            "Total params: 14,796,133\n",
            "Trainable params: 14,796,133\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBs4uIG2qM3r"
      },
      "source": [
        "\r\n",
        "\r\n",
        "baseline_model_4.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_adam,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgdZaB2tqM04",
        "outputId": "f81c0cca-3059-47e6-b26d-a4a2b93b5662"
      },
      "source": [
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model_4 = baseline_model_4.fit_generator(train_generator,\r\n",
        "                              steps_per_epoch=steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator,\r\n",
        "                              validation_steps=validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_4.save('food_baseline_model_4.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "160/160 [==============================] - 42s 257ms/step - loss: 5.5805 - acc: 0.0073 - val_loss: 4.6136 - val_acc: 0.0141\n",
            "Epoch 2/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6157 - acc: 0.0081 - val_loss: 4.6154 - val_acc: 0.0094\n",
            "Epoch 3/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6160 - acc: 0.0108 - val_loss: 4.6163 - val_acc: 0.0109\n",
            "Epoch 4/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6155 - acc: 0.0089 - val_loss: 4.6160 - val_acc: 0.0063\n",
            "Epoch 5/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6160 - acc: 0.0085 - val_loss: 4.6138 - val_acc: 0.0094\n",
            "Epoch 6/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6159 - acc: 0.0083 - val_loss: 4.6151 - val_acc: 0.0125\n",
            "Epoch 7/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6149 - acc: 0.0103 - val_loss: 4.6156 - val_acc: 0.0063\n",
            "Epoch 8/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6156 - acc: 0.0086 - val_loss: 4.6154 - val_acc: 0.0109\n",
            "Epoch 9/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6156 - acc: 0.0118 - val_loss: 4.6146 - val_acc: 0.0094\n",
            "Epoch 10/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6161 - acc: 0.0093 - val_loss: 4.6149 - val_acc: 0.0172\n",
            "Epoch 11/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6151 - acc: 0.0091 - val_loss: 4.6156 - val_acc: 0.0141\n",
            "Epoch 12/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6157 - acc: 0.0096 - val_loss: 4.6160 - val_acc: 0.0078\n",
            "Epoch 13/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6153 - acc: 0.0112 - val_loss: 4.6160 - val_acc: 0.0078\n",
            "Epoch 14/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6156 - acc: 0.0115 - val_loss: 4.6162 - val_acc: 0.0109\n",
            "Epoch 15/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6154 - acc: 0.0109 - val_loss: 4.6154 - val_acc: 0.0047\n",
            "Epoch 16/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6158 - acc: 0.0100 - val_loss: 4.6167 - val_acc: 0.0047\n",
            "Epoch 17/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6156 - acc: 0.0074 - val_loss: 4.6145 - val_acc: 0.0094\n",
            "Epoch 18/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6159 - acc: 0.0079 - val_loss: 4.6155 - val_acc: 0.0125\n",
            "Epoch 19/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6152 - acc: 0.0114 - val_loss: 4.6161 - val_acc: 0.0078\n",
            "Epoch 20/100\n",
            " 14/160 [=>............................] - ETA: 34s - loss: 4.6155 - acc: 0.0080"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSMTDbDiq5y0"
      },
      "source": [
        "Model 5 - Dense Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjfxikLxqMrv",
        "outputId": "6995cc62-91e8-4eab-87ef-880e63724de1"
      },
      "source": [
        "\r\n",
        "#Input Layer\r\n",
        "baseline_model_5 = Sequential()\r\n",
        "\r\n",
        "baseline_model_5.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model_5.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_5.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_5.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_5.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_5.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_5.add(Conv2D(filters=1024, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_5.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_5.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_5.add(Dense(2048, activation='relu'))\r\n",
        "baseline_model_5.add(Dense(4096, activation='relu'))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_5.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_5.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 62, 62, 128)       3584      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 31, 31, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 29, 29, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 12, 12, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 4, 4, 1024)        4719616   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 2048)              8390656   \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 4096)              8392704   \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 101)               413797    \n",
            "=================================================================\n",
            "Total params: 23,395,685\n",
            "Trainable params: 23,395,685\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx72TkMlqMh0"
      },
      "source": [
        "\r\n",
        "\r\n",
        "baseline_model_5.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_adam,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRikR3d3rEGe",
        "outputId": "37dca2e5-bd85-4f60-8528-edbcfe7fe265"
      },
      "source": [
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model_5 = baseline_model_5.fit_generator(train_generator,\r\n",
        "                              steps_per_epoch=steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator,\r\n",
        "                              validation_steps=validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_5.save('food_baseline_model_5.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "160/160 [==============================] - 43s 264ms/step - loss: 4.6190 - acc: 0.0080 - val_loss: 4.6155 - val_acc: 0.0109\n",
            "Epoch 2/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6156 - acc: 0.0090 - val_loss: 4.6156 - val_acc: 0.0063\n",
            "Epoch 3/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6155 - acc: 0.0078 - val_loss: 4.6145 - val_acc: 0.0031\n",
            "Epoch 4/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6153 - acc: 0.0091 - val_loss: 4.6141 - val_acc: 0.0141\n",
            "Epoch 5/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6155 - acc: 0.0089 - val_loss: 4.6147 - val_acc: 0.0094\n",
            "Epoch 6/100\n",
            "160/160 [==============================] - 42s 259ms/step - loss: 4.6149 - acc: 0.0108 - val_loss: 4.6164 - val_acc: 0.0109\n",
            "Epoch 7/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6152 - acc: 0.0099 - val_loss: 4.6152 - val_acc: 0.0078\n",
            "Epoch 8/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6153 - acc: 0.0091 - val_loss: 4.6149 - val_acc: 0.0078\n",
            "Epoch 9/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6156 - acc: 0.0092 - val_loss: 4.6143 - val_acc: 0.0156\n",
            "Epoch 10/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6156 - acc: 0.0072 - val_loss: 4.6161 - val_acc: 0.0109\n",
            "Epoch 11/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6159 - acc: 0.0106 - val_loss: 4.6156 - val_acc: 0.0078\n",
            "Epoch 12/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6157 - acc: 0.0106 - val_loss: 4.6154 - val_acc: 0.0141\n",
            "Epoch 13/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6152 - acc: 0.0117 - val_loss: 4.6169 - val_acc: 0.0063\n",
            "Epoch 14/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6151 - acc: 0.0118 - val_loss: 4.6166 - val_acc: 0.0063\n",
            "Epoch 15/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6155 - acc: 0.0108 - val_loss: 4.6141 - val_acc: 0.0141\n",
            "Epoch 16/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6154 - acc: 0.0099 - val_loss: 4.6157 - val_acc: 0.0109\n",
            "Epoch 17/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6158 - acc: 0.0082 - val_loss: 4.6143 - val_acc: 0.0109\n",
            "Epoch 18/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6152 - acc: 0.0080 - val_loss: 4.6158 - val_acc: 0.0109\n",
            "Epoch 19/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6154 - acc: 0.0081 - val_loss: 4.6167 - val_acc: 0.0109\n",
            "Epoch 20/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6160 - acc: 0.0077 - val_loss: 4.6166 - val_acc: 0.0031\n",
            "Epoch 21/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6147 - acc: 0.0092 - val_loss: 4.6149 - val_acc: 0.0047\n",
            "Epoch 22/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6153 - acc: 0.0098 - val_loss: 4.6155 - val_acc: 0.0109\n",
            "Epoch 23/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6152 - acc: 0.0103 - val_loss: 4.6155 - val_acc: 0.0078\n",
            "Epoch 24/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6154 - acc: 0.0086 - val_loss: 4.6146 - val_acc: 0.0156\n",
            "Epoch 25/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6153 - acc: 0.0086 - val_loss: 4.6159 - val_acc: 0.0078\n",
            "Epoch 26/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6155 - acc: 0.0088 - val_loss: 4.6156 - val_acc: 0.0109\n",
            "Epoch 27/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6150 - acc: 0.0132 - val_loss: 4.6142 - val_acc: 0.0125\n",
            "Epoch 28/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6157 - acc: 0.0106 - val_loss: 4.6152 - val_acc: 0.0047\n",
            "Epoch 29/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6155 - acc: 0.0113 - val_loss: 4.6166 - val_acc: 0.0047\n",
            "Epoch 30/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6155 - acc: 0.0091 - val_loss: 4.6137 - val_acc: 0.0172\n",
            "Epoch 31/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6153 - acc: 0.0115 - val_loss: 4.6153 - val_acc: 0.0109\n",
            "Epoch 32/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6155 - acc: 0.0085 - val_loss: 4.6140 - val_acc: 0.0063\n",
            "Epoch 33/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6163 - acc: 0.0098 - val_loss: 4.6156 - val_acc: 0.0063\n",
            "Epoch 34/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6156 - acc: 0.0082 - val_loss: 4.6185 - val_acc: 0.0172\n",
            "Epoch 35/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6148 - acc: 0.0107 - val_loss: 4.6157 - val_acc: 0.0109\n",
            "Epoch 36/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6159 - acc: 0.0083 - val_loss: 4.6172 - val_acc: 0.0188\n",
            "Epoch 37/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6153 - acc: 0.0097 - val_loss: 4.6149 - val_acc: 0.0031\n",
            "Epoch 38/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6151 - acc: 0.0077 - val_loss: 4.6151 - val_acc: 0.0063\n",
            "Epoch 39/100\n",
            "160/160 [==============================] - 42s 260ms/step - loss: 4.6157 - acc: 0.0098 - val_loss: 4.6152 - val_acc: 0.0047\n",
            "Epoch 40/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6156 - acc: 0.0108 - val_loss: 4.6163 - val_acc: 0.0094\n",
            "Epoch 41/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6152 - acc: 0.0089 - val_loss: 4.6152 - val_acc: 0.0234\n",
            "Epoch 42/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6154 - acc: 0.0081 - val_loss: 4.6143 - val_acc: 0.0141\n",
            "Epoch 43/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6158 - acc: 0.0092 - val_loss: 4.6152 - val_acc: 0.0172\n",
            "Epoch 44/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6151 - acc: 0.0109 - val_loss: 4.6158 - val_acc: 0.0125\n",
            "Epoch 45/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6157 - acc: 0.0096 - val_loss: 4.6148 - val_acc: 0.0125\n",
            "Epoch 46/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6158 - acc: 0.0088 - val_loss: 4.6155 - val_acc: 0.0172\n",
            "Epoch 47/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6153 - acc: 0.0101 - val_loss: 4.6164 - val_acc: 0.0047\n",
            "Epoch 48/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6152 - acc: 0.0097 - val_loss: 4.6160 - val_acc: 0.0141\n",
            "Epoch 49/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6155 - acc: 0.0099 - val_loss: 4.6155 - val_acc: 0.0094\n",
            "Epoch 50/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6150 - acc: 0.0101 - val_loss: 4.6172 - val_acc: 0.0094\n",
            "Epoch 51/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6152 - acc: 0.0098 - val_loss: 4.6162 - val_acc: 0.0047\n",
            "Epoch 52/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6159 - acc: 0.0115 - val_loss: 4.6159 - val_acc: 0.0109\n",
            "Epoch 53/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6158 - acc: 0.0085 - val_loss: 4.6143 - val_acc: 0.0125\n",
            "Epoch 54/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6152 - acc: 0.0106 - val_loss: 4.6155 - val_acc: 0.0094\n",
            "Epoch 55/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6150 - acc: 0.0124 - val_loss: 4.6148 - val_acc: 0.0141\n",
            "Epoch 56/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6153 - acc: 0.0102 - val_loss: 4.6142 - val_acc: 0.0109\n",
            "Epoch 57/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6154 - acc: 0.0096 - val_loss: 4.6146 - val_acc: 0.0172\n",
            "Epoch 58/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6153 - acc: 0.0114 - val_loss: 4.6158 - val_acc: 0.0125\n",
            "Epoch 59/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6154 - acc: 0.0082 - val_loss: 4.6163 - val_acc: 0.0125\n",
            "Epoch 60/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6157 - acc: 0.0105 - val_loss: 4.6146 - val_acc: 0.0031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye764gy7rdiI"
      },
      "source": [
        "Model 6 - Dense Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe0hgSTorD7L"
      },
      "source": [
        "\r\n",
        "#Input Layer\r\n",
        "baseline_model_6 = Sequential()\r\n",
        "\r\n",
        "baseline_model_6.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model_6.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_6.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_6.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_6.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_6.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_6.add(Conv2D(filters=1024, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_6.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_6.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_6.add(Dense(2048, activation='relu'))\r\n",
        "baseline_model_6.add(Dense(4096, activation='relu'))\r\n",
        "baseline_model_6.add(Dense(4096, activation='relu'))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_6.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_6.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6BLVBTyrD3x"
      },
      "source": [
        "\r\n",
        "\r\n",
        "baseline_model_6.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_adam,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWDn__b0rDz9",
        "outputId": "9eccb64b-b525-4c9f-96c5-dd58f4dfb726"
      },
      "source": [
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model_6 = baseline_model_6.fit_generator(train_generator,\r\n",
        "                              steps_per_epoch=steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator,\r\n",
        "                              validation_steps=validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_6.save('food_baseline_model_6.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "160/160 [==============================] - 42s 259ms/step - loss: 4.6330 - acc: 0.0099 - val_loss: 4.6164 - val_acc: 0.0125\n",
            "Epoch 2/100\n",
            "160/160 [==============================] - 42s 259ms/step - loss: 4.6153 - acc: 0.0102 - val_loss: 4.6153 - val_acc: 0.0125\n",
            "Epoch 3/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6156 - acc: 0.0102 - val_loss: 4.6151 - val_acc: 0.0094\n",
            "Epoch 4/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6155 - acc: 0.0100 - val_loss: 4.6175 - val_acc: 0.0063\n",
            "Epoch 5/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6156 - acc: 0.0096 - val_loss: 4.6150 - val_acc: 0.0078\n",
            "Epoch 6/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6149 - acc: 0.0080 - val_loss: 4.6174 - val_acc: 0.0078\n",
            "Epoch 7/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6155 - acc: 0.0109 - val_loss: 4.6153 - val_acc: 0.0188\n",
            "Epoch 8/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6153 - acc: 0.0105 - val_loss: 4.6156 - val_acc: 0.0125\n",
            "Epoch 9/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6153 - acc: 0.0108 - val_loss: 4.6149 - val_acc: 0.0141\n",
            "Epoch 10/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6155 - acc: 0.0107 - val_loss: 4.6135 - val_acc: 0.0172\n",
            "Epoch 11/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6155 - acc: 0.0073 - val_loss: 4.6144 - val_acc: 0.0156\n",
            "Epoch 12/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6153 - acc: 0.0106 - val_loss: 4.6151 - val_acc: 0.0141\n",
            "Epoch 13/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6155 - acc: 0.0105 - val_loss: 4.6156 - val_acc: 0.0000e+00\n",
            "Epoch 14/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6156 - acc: 0.0111 - val_loss: 4.6163 - val_acc: 0.0047\n",
            "Epoch 15/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6154 - acc: 0.0098 - val_loss: 4.6153 - val_acc: 0.0109\n",
            "Epoch 16/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6157 - acc: 0.0081 - val_loss: 4.6150 - val_acc: 0.0063\n",
            "Epoch 17/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6158 - acc: 0.0090 - val_loss: 4.6152 - val_acc: 0.0141\n",
            "Epoch 18/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6159 - acc: 0.0110 - val_loss: 4.6157 - val_acc: 0.0125\n",
            "Epoch 19/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6157 - acc: 0.0101 - val_loss: 4.6153 - val_acc: 0.0047\n",
            "Epoch 20/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6155 - acc: 0.0094 - val_loss: 4.6154 - val_acc: 0.0047\n",
            "Epoch 21/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6154 - acc: 0.0095 - val_loss: 4.6146 - val_acc: 0.0078\n",
            "Epoch 22/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6155 - acc: 0.0087 - val_loss: 4.6148 - val_acc: 0.0172\n",
            "Epoch 23/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6152 - acc: 0.0087 - val_loss: 4.6157 - val_acc: 0.0047\n",
            "Epoch 24/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6151 - acc: 0.0121 - val_loss: 4.6155 - val_acc: 0.0094\n",
            "Epoch 25/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6154 - acc: 0.0143 - val_loss: 4.6147 - val_acc: 0.0031\n",
            "Epoch 26/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6153 - acc: 0.0107 - val_loss: 4.6157 - val_acc: 0.0094\n",
            "Epoch 27/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6157 - acc: 0.0092 - val_loss: 4.6161 - val_acc: 0.0125\n",
            "Epoch 28/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6161 - acc: 0.0100 - val_loss: 4.6149 - val_acc: 0.0094\n",
            "Epoch 29/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6154 - acc: 0.0096 - val_loss: 4.6158 - val_acc: 0.0078\n",
            "Epoch 30/100\n",
            "160/160 [==============================] - 41s 255ms/step - loss: 4.6155 - acc: 0.0090 - val_loss: 4.6140 - val_acc: 0.0125\n",
            "Epoch 31/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6154 - acc: 0.0099 - val_loss: 4.6160 - val_acc: 0.0078\n",
            "Epoch 32/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6156 - acc: 0.0095 - val_loss: 4.6145 - val_acc: 0.0047\n",
            "Epoch 33/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6154 - acc: 0.0101 - val_loss: 4.6152 - val_acc: 0.0078\n",
            "Epoch 34/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6153 - acc: 0.0100 - val_loss: 4.6138 - val_acc: 0.0094\n",
            "Epoch 35/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6157 - acc: 0.0099 - val_loss: 4.6153 - val_acc: 0.0109\n",
            "Epoch 36/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6159 - acc: 0.0103 - val_loss: 4.6136 - val_acc: 0.0125\n",
            "Epoch 37/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6153 - acc: 0.0116 - val_loss: 4.6154 - val_acc: 0.0094\n",
            "Epoch 38/100\n",
            "160/160 [==============================] - 41s 256ms/step - loss: 4.6151 - acc: 0.0089 - val_loss: 4.6153 - val_acc: 0.0141\n",
            "Epoch 39/100\n",
            "160/160 [==============================] - 41s 257ms/step - loss: 4.6152 - acc: 0.0101 - val_loss: 4.6135 - val_acc: 0.0141\n",
            "Epoch 40/100\n",
            "160/160 [==============================] - 41s 258ms/step - loss: 4.6159 - acc: 0.0074 - val_loss: 4.6157 - val_acc: 0.0141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LD_KlL-tKXL"
      },
      "source": [
        "Model 7 - Add Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jv5rh4IrDsK",
        "outputId": "5e063cb5-9147-4284-b5cd-c8c5bbe25041"
      },
      "source": [
        "\r\n",
        "#Input Layer\r\n",
        "baseline_model_7 = Sequential()\r\n",
        "\r\n",
        "baseline_model_7.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model_7.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_7.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_7.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_7.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_7.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_7.add(Conv2D(filters=1024, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_7.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_7.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_7.add(Dense(2048, activation='relu'))\r\n",
        "baseline_model_7.add(Dropout(0.2))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_7.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_7.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 62, 62, 128)       3584      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 31, 31, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 29, 29, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 12, 12, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 1024)        4719616   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2048)              8390656   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 101)               206949    \n",
            "=================================================================\n",
            "Total params: 14,796,133\n",
            "Trainable params: 14,796,133\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ3wbN3-sfOW"
      },
      "source": [
        "\r\n",
        "\r\n",
        "baseline_model_7.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_adam,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peCgNieuskcB",
        "outputId": "a4bd1b74-5b9d-479c-eee9-3999b06428e8"
      },
      "source": [
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model_7 = baseline_model_7.fit_generator(train_generator,\r\n",
        "                              steps_per_epoch=steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator,\r\n",
        "                              validation_steps=validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_7.save('food_baseline_model_7.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "160/160 [==============================] - 73s 409ms/step - loss: 4.6211 - acc: 0.0096 - val_loss: 4.6162 - val_acc: 0.0063\n",
            "Epoch 2/100\n",
            "160/160 [==============================] - 62s 384ms/step - loss: 4.6154 - acc: 0.0099 - val_loss: 4.6158 - val_acc: 0.0109\n",
            "Epoch 3/100\n",
            "160/160 [==============================] - 59s 369ms/step - loss: 4.6155 - acc: 0.0098 - val_loss: 4.6155 - val_acc: 0.0109\n",
            "Epoch 4/100\n",
            "160/160 [==============================] - 57s 356ms/step - loss: 4.6152 - acc: 0.0117 - val_loss: 4.6155 - val_acc: 0.0109\n",
            "Epoch 5/100\n",
            "160/160 [==============================] - 56s 350ms/step - loss: 4.6156 - acc: 0.0093 - val_loss: 4.6154 - val_acc: 0.0141\n",
            "Epoch 6/100\n",
            "160/160 [==============================] - 55s 342ms/step - loss: 4.6155 - acc: 0.0108 - val_loss: 4.6155 - val_acc: 0.0109\n",
            "Epoch 7/100\n",
            "160/160 [==============================] - 54s 335ms/step - loss: 4.6154 - acc: 0.0100 - val_loss: 4.6160 - val_acc: 0.0078\n",
            "Epoch 8/100\n",
            "160/160 [==============================] - 53s 330ms/step - loss: 4.6154 - acc: 0.0104 - val_loss: 4.6156 - val_acc: 0.0078\n",
            "Epoch 9/100\n",
            "160/160 [==============================] - 52s 323ms/step - loss: 4.6154 - acc: 0.0110 - val_loss: 4.6157 - val_acc: 0.0047\n",
            "Epoch 10/100\n",
            "160/160 [==============================] - 51s 319ms/step - loss: 4.6152 - acc: 0.0110 - val_loss: 4.6166 - val_acc: 0.0109\n",
            "Epoch 11/100\n",
            "160/160 [==============================] - 51s 318ms/step - loss: 4.6158 - acc: 0.0097 - val_loss: 4.6160 - val_acc: 0.0047\n",
            "Epoch 12/100\n",
            "160/160 [==============================] - 51s 321ms/step - loss: 4.6155 - acc: 0.0121 - val_loss: 4.6148 - val_acc: 0.0125\n",
            "Epoch 13/100\n",
            "160/160 [==============================] - 51s 316ms/step - loss: 4.6152 - acc: 0.0108 - val_loss: 4.6170 - val_acc: 0.0094\n",
            "Epoch 14/100\n",
            "160/160 [==============================] - 51s 320ms/step - loss: 4.6158 - acc: 0.0091 - val_loss: 4.6143 - val_acc: 0.0156\n",
            "Epoch 15/100\n",
            " 95/160 [================>.............] - ETA: 19s - loss: 4.6152 - acc: 0.0095"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NywPVdS7uNoe"
      },
      "source": [
        "Model 8 - Increase Dropout Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EaOFWhYskOA"
      },
      "source": [
        "\r\n",
        "#Input Layer\r\n",
        "baseline_model_8 = Sequential()\r\n",
        "\r\n",
        "baseline_model_8.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model_8.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_8.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_8.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_8.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_8.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_8.add(Conv2D(filters=1024, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_8.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_8.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_8.add(Dense(2048, activation='relu'))\r\n",
        "baseline_model_8.add(Dropout(0.3))\r\n",
        "\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_8.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_8.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "YxRH7IbPskF_",
        "outputId": "d549b94b-465d-48d6-97f7-e63d893fcf90"
      },
      "source": [
        "\r\n",
        "\r\n",
        "baseline_model_8.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_adam,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-ceb4acfc4eac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mopt_8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m baseline_model_8.compile(loss='categorical_crossentropy',\n\u001b[0m\u001b[1;32m      4\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt_8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               metrics=['acc'])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'baseline_model_8' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lsvck5Fsj91"
      },
      "source": [
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model_8 = baseline_model_8.fit_generator(train_generator,\r\n",
        "                              steps_per_epoch=steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator,\r\n",
        "                              validation_steps=validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_8.save('food_baseline_model_8.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWBh3SbpvAOR"
      },
      "source": [
        "Model 9 - Add Kernel Constraint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGbABl79u_lt"
      },
      "source": [
        "\r\n",
        "#Input Layer\r\n",
        "baseline_model_9 = Sequential()\r\n",
        "\r\n",
        "baseline_model_9.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model_9.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_9.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_9.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_9.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_9.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_9.add(Conv2D(filters=1024, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_9.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_9.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_9.add(Dense(2048, activation='relu',kernel_constraint=maxnorm(3)))\r\n",
        "baseline_model_9.add(Dropout(0.3))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_9.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_9.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3msRhJjFu_h-"
      },
      "source": [
        "\r\n",
        "\r\n",
        "baseline_model_9.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_adam,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJLM-0W0u_ei"
      },
      "source": [
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model_9 = baseline_model_9.fit_generator(train_generator,\r\n",
        "                              steps_per_epoch=steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator,\r\n",
        "                              validation_steps=validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_9.save('food_baseline_model_9.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca1og51Wx1Xa"
      },
      "source": [
        "Model 10 - Add learning rate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l2hWxDCu_XU"
      },
      "source": [
        "\r\n",
        "#Input Layer\r\n",
        "baseline_model_10 = Sequential()\r\n",
        "\r\n",
        "baseline_model_10.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model_10.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_10.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_10.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_10.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_10.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_10.add(Conv2D(filters=1024, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_10.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_10.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_10.add(Dense(2048, activation='relu',kernel_constraint=maxnorm(3)))\r\n",
        "baseline_model_10.add(Dropout(0.3))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_10.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_10.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlHiXuhGu_Rz"
      },
      "source": [
        "opt_adam_lr = Adam(lr=0.001)\r\n",
        "\r\n",
        "baseline_model_10.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_adam_lr,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwEjGFv6u_Ns"
      },
      "source": [
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model_10 = baseline_model_10.fit_generator(train_generator,\r\n",
        "                              steps_per_epoch=steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator,\r\n",
        "                              validation_steps=validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_10.save('food_baseline_model_10.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI0MWpYCzqrB"
      },
      "source": [
        "Model 11 - Change optimizer to RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "688kC6s-u_C1"
      },
      "source": [
        "\r\n",
        "#Input Layer\r\n",
        "baseline_model_11 = Sequential()\r\n",
        "\r\n",
        "baseline_model_11.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model_11.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_11.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_11.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_11.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_11.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_11.add(Conv2D(filters=1024, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_11.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_11.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_11.add(Dense(2048, activation='relu',kernel_constraint=maxnorm(3)))\r\n",
        "baseline_model_11.add(Dropout(0.3))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_11.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_11.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwQfV-AOsfKY"
      },
      "source": [
        "opt_msrprop_lr = RMSprop(lr=0.001)\r\n",
        "\r\n",
        "baseline_model_11.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_msrprop_lr,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxyXKkqOsfGP"
      },
      "source": [
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model_11 = baseline_model_11.fit_generator(train_generator,\r\n",
        "                              steps_per_epoch=steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator,\r\n",
        "                              validation_steps=validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_11.save('food_baseline_model_11.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE2yI5_l1vfO"
      },
      "source": [
        "Model 12 - Increase Batch Size + Steps per epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVrEUyNkz7uD"
      },
      "source": [
        "\r\n",
        "#Input Layer\r\n",
        "baseline_model_12 = Sequential()\r\n",
        "\r\n",
        "baseline_model_12.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model_12.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_12.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_12.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_12.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_12.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_12.add(Conv2D(filters=1024, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_12.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_12.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_12.add(Dense(2048, activation='relu',kernel_constraint=maxnorm(3)))\r\n",
        "baseline_model_12.add(Dropout(0.3))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_12.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_12.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLCgjpTGz7ri"
      },
      "source": [
        "\r\n",
        "\r\n",
        "baseline_model_12.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_msrprop_lr,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txzAD2SWz7ov"
      },
      "source": [
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model_12 = baseline_model_12.fit_generator(train_generator,\r\n",
        "                              steps_per_epoch=320, #steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator,\r\n",
        "                              validation_steps=32, #validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_12.save('food_baseline_model_12.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue_61CzS22PW"
      },
      "source": [
        "Model 13 - Change padding (Worse Results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq9DT1DS2iVr"
      },
      "source": [
        "\r\n",
        "#Input Layer\r\n",
        "baseline_model_13 = Sequential()\r\n",
        "\r\n",
        "baseline_model_13.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape))\r\n",
        "baseline_model_13.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_13.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\r\n",
        "baseline_model_13.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_13.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='same'))\r\n",
        "baseline_model_13.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_13.add(Conv2D(filters=1024, kernel_size=(3, 3), activation='relu', padding='same'))\r\n",
        "baseline_model_13.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_13.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_13.add(Dense(2048, activation='relu',kernel_constraint=maxnorm(3)))\r\n",
        "baseline_model_13.add(Dropout(0.3))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_13.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_13.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdLrhIoY2iSH"
      },
      "source": [
        "\r\n",
        "\r\n",
        "baseline_model_13.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_msrprop_lr,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU1qSgfM2iO7"
      },
      "source": [
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model_13 = baseline_model_13.fit_generator(train_generator,\r\n",
        "                              steps_per_epoch=320, #steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator,\r\n",
        "                              validation_steps=32, #validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_13.save('food_baseline_model_13.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLqTZ7m92-mK"
      },
      "source": [
        "Model 14 - Include Data Augmentation techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIk3wwAx2iLM"
      },
      "source": [
        "train_datagen_14 = ImageDataGenerator(samplewise_center = True,\r\n",
        "                                  samplewise_std_normalization = True,\r\n",
        "                                  rescale = 1./255.)\r\n",
        "\r\n",
        "valid_datagen_14 = ImageDataGenerator(rescale = 1./255)\r\n",
        "\r\n",
        "test_datagen_14 = ImageDataGenerator(rescale = 1./255)\r\n",
        "\r\n",
        "train_generator_14 = train_datagen_14.flow_from_directory(directory=\"/content/output/train\",\r\n",
        "                                                    subset=\"training\",\r\n",
        "                                                    batch_size=128,\r\n",
        "                                                    shuffle=True,\r\n",
        "                                                    class_mode=\"categorical\",\r\n",
        "                                                    target_size=(64,64),\r\n",
        "                                                    seed=42)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "valid_generator_14 = valid_datagen_14.flow_from_directory(directory=\"/content/output/val\",\r\n",
        "                                                  batch_size=128,\r\n",
        "                                                  shuffle=True,\r\n",
        "                                                  class_mode=\"categorical\",\r\n",
        "                                                  target_size=(64,64),\r\n",
        "                                                  seed=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpSQjwDL2iIS"
      },
      "source": [
        "\r\n",
        "#Input Layer\r\n",
        "baseline_model_14 = Sequential()\r\n",
        "\r\n",
        "baseline_model_14.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model_14.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_14.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_14.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_14.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_14.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_14.add(Conv2D(filters=1024, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_14.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_14.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_14.add(Dense(2048, activation='relu',kernel_constraint=maxnorm(3)))\r\n",
        "baseline_model_14.add(Dropout(0.3))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_14.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_14.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qfU0osT2iCh"
      },
      "source": [
        "\r\n",
        "\r\n",
        "baseline_model_14.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_msrprop_lr,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UycMYs8qz7iN"
      },
      "source": [
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model_14 = baseline_model_14.fit_generator(train_generator_14,\r\n",
        "                              steps_per_epoch=320, #steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator_14,\r\n",
        "                              validation_steps=32, #validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_14.save('food_baseline_model_14.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAAM1hcT5jyN"
      },
      "source": [
        "Model 15 - More Data Augmentation techniques\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HZWDJk15NWu"
      },
      "source": [
        "train_datagen_15 = ImageDataGenerator(samplewise_center = True,\r\n",
        "                                  samplewise_std_normalization = True,\r\n",
        "                                  rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True))\r\n",
        "\r\n",
        "valid_datagen_15 = ImageDataGenerator(rescale = 1./255)\r\n",
        "\r\n",
        "test_datagen_15 = ImageDataGenerator(rescale = 1./255)\r\n",
        "\r\n",
        "train_generator_15 = train_datagen_15.flow_from_directory(directory=\"/content/output/train\",\r\n",
        "                                                    subset=\"training\",\r\n",
        "                                                    batch_size=128,\r\n",
        "                                                    shuffle=True,\r\n",
        "                                                    class_mode=\"categorical\",\r\n",
        "                                                    target_size=(64,64),\r\n",
        "                                                    seed=42)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "valid_generator_15 = valid_datagen_15.flow_from_directory(directory=\"/content/output/val\",\r\n",
        "                                                  batch_size=128,\r\n",
        "                                                  shuffle=True,\r\n",
        "                                                  class_mode=\"categorical\",\r\n",
        "                                                  target_size=(64,64),\r\n",
        "                                                  seed=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOzZMn-p5NRj"
      },
      "source": [
        "\r\n",
        "#Input Layer\r\n",
        "baseline_model_15 = Sequential()\r\n",
        "\r\n",
        "baseline_model_15.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model_15.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_15.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_15.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_15.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_15.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model_15.add(Conv2D(filters=1024, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model_15.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model_15.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model_15.add(Dense(2048, activation='relu',kernel_constraint=maxnorm(3)))\r\n",
        "baseline_model_15.add(Dropout(0.3))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model_15.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model_15.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj_hkP-C5NOd"
      },
      "source": [
        "\r\n",
        "\r\n",
        "baseline_model_15.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt_msrprop_lr,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Ibdb7r5NLn"
      },
      "source": [
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model_15 = baseline_model_15.fit_generator(train_generator_15,\r\n",
        "                              steps_per_epoch=320, #steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator_15,\r\n",
        "                              validation_steps=32, #validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model_15.save('food_baseline_model_15.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5_jK77m5NIy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xhiIpwL5NGF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49v2Sqdt5NDT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tci4DDul5NAK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7LuuH5_5M9P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie_O_6I_5M6g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FErdiXlt5M31"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnS460PB5M0-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i28md_Mm5MyK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3E0RrbV5MuW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH1hzBwq5Mqy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "Yj9wjWbUoIbo",
        "outputId": "8c75d577-dc01-4ec9-9971-f07f24710cbc"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\r\n",
        "                                  samplewise_center = True,\r\n",
        "                                  samplewise_std_normalization = True,\r\n",
        "                                  rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\r\n",
        "\r\n",
        "valid_datagen = ImageDataGenerator(rescale = 1./255)\r\n",
        "\r\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\r\n",
        "\r\n",
        "train_generator = train_datagen.flow_from_directory(directory=\"/output/train/\",\r\n",
        "                                                    subset=\"training\",\r\n",
        "                                                    batch_size=64,\r\n",
        "                                                    shuffle=True,\r\n",
        "                                                    class_mode=\"categorical\",\r\n",
        "                                                    target_size=(64,64),\r\n",
        "                                                    seed=42)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "valid_generator=test_datagen.flow_from_directory(directory=\"/output/val/\",\r\n",
        "                                                  subset=\"validation\",\r\n",
        "                                                  batch_size=64,\r\n",
        "                                                  shuffle=True,\r\n",
        "                                                  class_mode=\"categorical\",\r\n",
        "                                                  target_size=(64,64),\r\n",
        "                                                  seed=42)\r\n",
        "\r\n",
        "\r\n",
        "teste=test_datagen.flow_from_directory(directory=\"/output/test\",\r\n",
        "                                                  subset=\"test\",\r\n",
        "                                                  batch_size=64,\r\n",
        "                                                  shuffle=True,\r\n",
        "                                                  class_mode=\"categorical\",\r\n",
        "                                                  target_size=(64,64),\r\n",
        "                                                  seed=42)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6b1c82cda679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                     \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                                     seed=42)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtest_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         )\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/output/images/train/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gh2LsAZ0dSG",
        "outputId": "27b54f80-0d8c-487c-ea70-81e96f997408"
      },
      "source": [
        "\r\n",
        "\r\n",
        "X_train, y_train = train_generator.next()\r\n",
        "#Input Layer\r\n",
        "baseline_model = Sequential()\r\n",
        "\r\n",
        "baseline_model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=input_shape))\r\n",
        "baseline_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "baseline_model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "\r\n",
        "baseline_model.add(Conv2D(filters=512, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "\r\n",
        "baseline_model.add(Conv2D(filters=1024, kernel_size=(3, 3), activation='relu', padding='valid'))\r\n",
        "baseline_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n",
        "\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model.add(Dense(4096, activation='relu',kernel_constraint=maxnorm(3)))\r\n",
        "baseline_model.add(Dropout(0.2))\r\n",
        "baseline_model.add(Dense(4096, activation='relu',kernel_constraint=maxnorm(3)))\r\n",
        "baseline_model.add(Dropout(0.2))\r\n",
        "baseline_model.add(Dense(2048, activation='relu',kernel_constraint=maxnorm(3)))\r\n",
        "baseline_model.add(Dropout(0.2))\r\n",
        "\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_110 (Conv2D)          (None, 62, 62, 128)       3584      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_51 (MaxPooling (None, 31, 31, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_111 (Conv2D)          (None, 29, 29, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_52 (MaxPooling (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_112 (Conv2D)          (None, 12, 12, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_53 (MaxPooling (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_113 (Conv2D)          (None, 4, 4, 1024)        4719616   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_54 (MaxPooling (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 2048)              8390656   \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 101)               206949    \n",
            "=================================================================\n",
            "Total params: 48,358,757\n",
            "Trainable params: 48,358,757\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24SOh5Al3p1p"
      },
      "source": [
        "opt = Adam(lr=0.001)\r\n",
        "\r\n",
        "baseline_model.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=opt,\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulUsdn3k0dPu",
        "outputId": "b1ab8b45-9fc0-4e12-b953-edfe8a3af3db"
      },
      "source": [
        "X_val, y_val = valid_generator.next()\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "\r\n",
        "history_baseline_model = baseline_model_1.fit_generator(train_generator,\r\n",
        "                              steps_per_epoch=steps_per_epoch,\r\n",
        "                              epochs=epochs,\r\n",
        "                              validation_data=valid_generator,\r\n",
        "                              validation_steps=validation_steps,\r\n",
        "                              callbacks=[early_stop],\r\n",
        "                              shuffle= True\r\n",
        "                             )\r\n",
        "\r\n",
        "baseline_model.save('food_baseline_model_1.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "160/160 [==============================] - 54s 336ms/step - loss: 4.2930 - acc: 0.0545 - val_loss: 4.3365 - val_acc: 0.0547\n",
            "Epoch 2/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 4.2158 - acc: 0.0665 - val_loss: 4.3475 - val_acc: 0.0453\n",
            "Epoch 3/100\n",
            "160/160 [==============================] - 54s 336ms/step - loss: 4.1883 - acc: 0.0685 - val_loss: 4.1274 - val_acc: 0.0656\n",
            "Epoch 4/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 4.1421 - acc: 0.0763 - val_loss: 4.0769 - val_acc: 0.0734\n",
            "Epoch 5/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 4.1097 - acc: 0.0757 - val_loss: 4.1044 - val_acc: 0.0812\n",
            "Epoch 6/100\n",
            "160/160 [==============================] - 54s 336ms/step - loss: 4.0798 - acc: 0.0794 - val_loss: 4.1218 - val_acc: 0.0734\n",
            "Epoch 7/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 4.0663 - acc: 0.0854 - val_loss: 4.0138 - val_acc: 0.1000\n",
            "Epoch 8/100\n",
            "160/160 [==============================] - 54s 335ms/step - loss: 4.0106 - acc: 0.0966 - val_loss: 3.9263 - val_acc: 0.0891\n",
            "Epoch 9/100\n",
            "160/160 [==============================] - 54s 336ms/step - loss: 3.9844 - acc: 0.1031 - val_loss: 3.9202 - val_acc: 0.0891\n",
            "Epoch 10/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.9832 - acc: 0.0984 - val_loss: 4.0941 - val_acc: 0.1000\n",
            "Epoch 11/100\n",
            "160/160 [==============================] - 54s 335ms/step - loss: 3.9288 - acc: 0.1095 - val_loss: 3.9572 - val_acc: 0.1000\n",
            "Epoch 12/100\n",
            "160/160 [==============================] - 54s 335ms/step - loss: 3.8888 - acc: 0.1178 - val_loss: 3.8715 - val_acc: 0.1016\n",
            "Epoch 13/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.8843 - acc: 0.1187 - val_loss: 3.8528 - val_acc: 0.1250\n",
            "Epoch 14/100\n",
            "160/160 [==============================] - 54s 335ms/step - loss: 3.8519 - acc: 0.1239 - val_loss: 3.8808 - val_acc: 0.1266\n",
            "Epoch 15/100\n",
            "160/160 [==============================] - 54s 335ms/step - loss: 3.8553 - acc: 0.1282 - val_loss: 4.0839 - val_acc: 0.0953\n",
            "Epoch 16/100\n",
            "160/160 [==============================] - 54s 336ms/step - loss: 3.8585 - acc: 0.1199 - val_loss: 3.9523 - val_acc: 0.1078\n",
            "Epoch 17/100\n",
            "160/160 [==============================] - 54s 335ms/step - loss: 3.8105 - acc: 0.1344 - val_loss: 3.8526 - val_acc: 0.1266\n",
            "Epoch 18/100\n",
            "160/160 [==============================] - 53s 333ms/step - loss: 3.8053 - acc: 0.1354 - val_loss: 3.7849 - val_acc: 0.1281\n",
            "Epoch 19/100\n",
            "160/160 [==============================] - 54s 335ms/step - loss: 3.8055 - acc: 0.1369 - val_loss: 3.8160 - val_acc: 0.1281\n",
            "Epoch 20/100\n",
            "160/160 [==============================] - 54s 336ms/step - loss: 3.7894 - acc: 0.1396 - val_loss: 3.6900 - val_acc: 0.1578\n",
            "Epoch 21/100\n",
            "160/160 [==============================] - 54s 336ms/step - loss: 3.7647 - acc: 0.1380 - val_loss: 3.6681 - val_acc: 0.1531\n",
            "Epoch 22/100\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 3.7385 - acc: 0.1454 - val_loss: 3.9261 - val_acc: 0.1312\n",
            "Epoch 23/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.7372 - acc: 0.1427 - val_loss: 3.7062 - val_acc: 0.1625\n",
            "Epoch 24/100\n",
            "160/160 [==============================] - 54s 336ms/step - loss: 3.7475 - acc: 0.1476 - val_loss: 3.7471 - val_acc: 0.1453\n",
            "Epoch 25/100\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 3.7225 - acc: 0.1499 - val_loss: 3.6470 - val_acc: 0.1391\n",
            "Epoch 26/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.7364 - acc: 0.1487 - val_loss: 3.6801 - val_acc: 0.1578\n",
            "Epoch 27/100\n",
            "160/160 [==============================] - 54s 340ms/step - loss: 3.7397 - acc: 0.1479 - val_loss: 3.6127 - val_acc: 0.1391\n",
            "Epoch 28/100\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 3.7415 - acc: 0.1485 - val_loss: 3.6862 - val_acc: 0.1641\n",
            "Epoch 29/100\n",
            "160/160 [==============================] - 54s 340ms/step - loss: 3.7275 - acc: 0.1490 - val_loss: 3.5305 - val_acc: 0.1813\n",
            "Epoch 30/100\n",
            "160/160 [==============================] - 55s 341ms/step - loss: 3.7150 - acc: 0.1545 - val_loss: 3.7019 - val_acc: 0.1562\n",
            "Epoch 31/100\n",
            "160/160 [==============================] - 55s 342ms/step - loss: 3.7403 - acc: 0.1534 - val_loss: 3.6834 - val_acc: 0.1500\n",
            "Epoch 32/100\n",
            "160/160 [==============================] - 55s 341ms/step - loss: 3.7102 - acc: 0.1492 - val_loss: 3.6855 - val_acc: 0.1719\n",
            "Epoch 33/100\n",
            "160/160 [==============================] - 54s 340ms/step - loss: 3.7081 - acc: 0.1602 - val_loss: 3.6563 - val_acc: 0.1500\n",
            "Epoch 34/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.7134 - acc: 0.1541 - val_loss: 3.6173 - val_acc: 0.1578\n",
            "Epoch 35/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.6994 - acc: 0.1601 - val_loss: 3.5260 - val_acc: 0.1766\n",
            "Epoch 36/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.7694 - acc: 0.1483 - val_loss: 3.6450 - val_acc: 0.1766\n",
            "Epoch 37/100\n",
            "160/160 [==============================] - 54s 336ms/step - loss: 3.6951 - acc: 0.1599 - val_loss: 3.7911 - val_acc: 0.1516\n",
            "Epoch 38/100\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 3.7352 - acc: 0.1548 - val_loss: 3.5834 - val_acc: 0.1844\n",
            "Epoch 39/100\n",
            "160/160 [==============================] - 55s 341ms/step - loss: 3.7420 - acc: 0.1489 - val_loss: 3.6774 - val_acc: 0.1734\n",
            "Epoch 40/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.7121 - acc: 0.1542 - val_loss: 3.8230 - val_acc: 0.1562\n",
            "Epoch 41/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.7355 - acc: 0.1551 - val_loss: 3.5215 - val_acc: 0.2000\n",
            "Epoch 42/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.7138 - acc: 0.1552 - val_loss: 3.7669 - val_acc: 0.1453\n",
            "Epoch 43/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.7268 - acc: 0.1546 - val_loss: 3.8007 - val_acc: 0.1453\n",
            "Epoch 44/100\n",
            "160/160 [==============================] - 54s 340ms/step - loss: 3.7079 - acc: 0.1582 - val_loss: 3.7697 - val_acc: 0.1531\n",
            "Epoch 45/100\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 3.7805 - acc: 0.1458 - val_loss: 3.6400 - val_acc: 0.1734\n",
            "Epoch 46/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.7019 - acc: 0.1584 - val_loss: 3.5901 - val_acc: 0.1688\n",
            "Epoch 47/100\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 3.7050 - acc: 0.1586 - val_loss: 3.6296 - val_acc: 0.1875\n",
            "Epoch 48/100\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 3.7206 - acc: 0.1636 - val_loss: 3.6511 - val_acc: 0.1703\n",
            "Epoch 49/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.7301 - acc: 0.1519 - val_loss: 3.6634 - val_acc: 0.1547\n",
            "Epoch 50/100\n",
            "160/160 [==============================] - 54s 340ms/step - loss: 3.7418 - acc: 0.1559 - val_loss: 3.5576 - val_acc: 0.1688\n",
            "Epoch 51/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.7279 - acc: 0.1598 - val_loss: 3.6099 - val_acc: 0.1813\n",
            "Epoch 52/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.7423 - acc: 0.1487 - val_loss: 3.7685 - val_acc: 0.1359\n",
            "Epoch 53/100\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 3.7293 - acc: 0.1509 - val_loss: 3.5707 - val_acc: 0.1578\n",
            "Epoch 54/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.7431 - acc: 0.1555 - val_loss: 3.6244 - val_acc: 0.1781\n",
            "Epoch 55/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.7510 - acc: 0.1467 - val_loss: 3.6414 - val_acc: 0.1797\n",
            "Epoch 56/100\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 3.8016 - acc: 0.1446 - val_loss: 3.5118 - val_acc: 0.1953\n",
            "Epoch 57/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.7569 - acc: 0.1528 - val_loss: 3.6398 - val_acc: 0.1594\n",
            "Epoch 58/100\n",
            "160/160 [==============================] - 54s 336ms/step - loss: 3.7680 - acc: 0.1535 - val_loss: 3.7780 - val_acc: 0.1625\n",
            "Epoch 59/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.7913 - acc: 0.1433 - val_loss: 3.7635 - val_acc: 0.1594\n",
            "Epoch 60/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.8628 - acc: 0.1344 - val_loss: 3.8536 - val_acc: 0.1422\n",
            "Epoch 61/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.7635 - acc: 0.1507 - val_loss: 3.5256 - val_acc: 0.1750\n",
            "Epoch 62/100\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 3.8198 - acc: 0.1453 - val_loss: 4.0548 - val_acc: 0.1234\n",
            "Epoch 63/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.8040 - acc: 0.1491 - val_loss: 3.5750 - val_acc: 0.1672\n",
            "Epoch 64/100\n",
            "160/160 [==============================] - 54s 336ms/step - loss: 3.7937 - acc: 0.1505 - val_loss: 3.8417 - val_acc: 0.1203\n",
            "Epoch 65/100\n",
            "160/160 [==============================] - 55s 341ms/step - loss: 3.8364 - acc: 0.1385 - val_loss: 3.7733 - val_acc: 0.1562\n",
            "Epoch 66/100\n",
            "160/160 [==============================] - 55s 344ms/step - loss: 3.8185 - acc: 0.1432 - val_loss: 3.8386 - val_acc: 0.1516\n",
            "Epoch 67/100\n",
            "160/160 [==============================] - 55s 345ms/step - loss: 3.8317 - acc: 0.1385 - val_loss: 3.7958 - val_acc: 0.1516\n",
            "Epoch 68/100\n",
            "160/160 [==============================] - 56s 349ms/step - loss: 3.8166 - acc: 0.1424 - val_loss: 3.7081 - val_acc: 0.1562\n",
            "Epoch 69/100\n",
            "160/160 [==============================] - 55s 342ms/step - loss: 3.8158 - acc: 0.1431 - val_loss: 3.8014 - val_acc: 0.1594\n",
            "Epoch 70/100\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 3.8192 - acc: 0.1446 - val_loss: 3.8521 - val_acc: 0.1406\n",
            "Epoch 71/100\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 3.8430 - acc: 0.1350 - val_loss: 3.7000 - val_acc: 0.1547\n",
            "Epoch 72/100\n",
            "160/160 [==============================] - 55s 341ms/step - loss: 3.8431 - acc: 0.1388 - val_loss: 3.6782 - val_acc: 0.1734\n",
            "Epoch 73/100\n",
            "160/160 [==============================] - 54s 341ms/step - loss: 3.8360 - acc: 0.1401 - val_loss: 3.6813 - val_acc: 0.1766\n",
            "Epoch 74/100\n",
            "160/160 [==============================] - 54s 336ms/step - loss: 3.8216 - acc: 0.1383 - val_loss: 4.6390 - val_acc: 0.1344\n",
            "Epoch 75/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.8187 - acc: 0.1433 - val_loss: 3.6964 - val_acc: 0.1156\n",
            "Epoch 76/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.8077 - acc: 0.1435 - val_loss: 3.6240 - val_acc: 0.1766\n",
            "Epoch 77/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.8371 - acc: 0.1459 - val_loss: 3.7304 - val_acc: 0.1484\n",
            "Epoch 78/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.8415 - acc: 0.1459 - val_loss: 3.9189 - val_acc: 0.1437\n",
            "Epoch 79/100\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 3.8734 - acc: 0.1382 - val_loss: 3.8586 - val_acc: 0.1172\n",
            "Epoch 80/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.8851 - acc: 0.1325 - val_loss: 3.8580 - val_acc: 0.1266\n",
            "Epoch 81/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.8540 - acc: 0.1351 - val_loss: 3.6267 - val_acc: 0.1500\n",
            "Epoch 82/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.8512 - acc: 0.1395 - val_loss: 3.6816 - val_acc: 0.1641\n",
            "Epoch 83/100\n",
            "160/160 [==============================] - 54s 339ms/step - loss: 3.8525 - acc: 0.1404 - val_loss: 3.8742 - val_acc: 0.1063\n",
            "Epoch 84/100\n",
            "160/160 [==============================] - 54s 338ms/step - loss: 3.8692 - acc: 0.1359 - val_loss: 3.6280 - val_acc: 0.1625\n",
            "Epoch 85/100\n",
            "160/160 [==============================] - 55s 341ms/step - loss: 3.8847 - acc: 0.1312 - val_loss: 3.8435 - val_acc: 0.1359\n",
            "Epoch 86/100\n",
            "160/160 [==============================] - 54s 337ms/step - loss: 3.8707 - acc: 0.1396 - val_loss: 3.6929 - val_acc: 0.1547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSBLC1Uf0dM5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4D1lrAo0dKZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka3V6WPU0dHo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcRaNTnT0dDo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LoPwBDB0dAg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2oW4aeI0c4W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrgOz-j-0cs_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCj9jC5aERPZ",
        "outputId": "e59e0fb1-05a0-4b9f-d983-f2465f235e29"
      },
      "source": [
        "incnet = InceptionV3(weights='imagenet', include_top=False, input_tensor=layers.Input(shape=(299, 299, 3)))\r\n",
        "x = incnet.output\r\n",
        "x = layers.AveragePooling2D(pool_size=(8, 8))(x)\r\n",
        "x = layers.Dropout(.2)(x)\r\n",
        "x = layers.Flatten()(x)\r\n",
        "output = layers.Dense(categorias, activation='softmax', kernel_regularizer=regularizers.l2(.0005))(x)\r\n",
        "\r\n",
        "model = models.Model(inputs=incnet.input, outputs=output)\r\n",
        "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqS1V0ObHhcz",
        "outputId": "94cffeef-4532-4807-9ce6-d9421b47f0b2"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMnwx2_VHjDy",
        "outputId": "847469dc-ccfa-4379-fb8b-506b2ff63c1d"
      },
      "source": [
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=4)\r\n",
        "checkpoint_callback = ModelCheckpoint('InceptionNet.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\r\n",
        "history = model.fit_generator(train_generator,\r\n",
        "                            validation_data=valid_generator,\r\n",
        "                            epochs=10,\r\n",
        "                            workers=0,\r\n",
        "                            use_multiprocessing=False, \r\n",
        "                            callbacks=[early_stopping_callback, checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1263/1263 [==============================] - 1739s 1s/step - loss: 2.8844 - accuracy: 0.3382 - val_loss: 2.6619 - val_accuracy: 0.3891\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.66189, saving model to InceptionNet.h5\n",
            "Epoch 2/10\n",
            "1263/1263 [==============================] - 1675s 1s/step - loss: 1.4509 - accuracy: 0.6439 - val_loss: 2.6808 - val_accuracy: 0.4057\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 2.66189\n",
            "Epoch 3/10\n",
            "1263/1263 [==============================] - 1638s 1s/step - loss: 1.1333 - accuracy: 0.7240 - val_loss: 1.8525 - val_accuracy: 0.5699\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.66189 to 1.85251, saving model to InceptionNet.h5\n",
            "Epoch 4/10\n",
            "1263/1263 [==============================] - 1615s 1s/step - loss: 0.9297 - accuracy: 0.7748 - val_loss: 2.3600 - val_accuracy: 0.4794\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.85251\n",
            "Epoch 5/10\n",
            "1263/1263 [==============================] - 1600s 1s/step - loss: 0.7756 - accuracy: 0.8156 - val_loss: 1.8512 - val_accuracy: 0.5752\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.85251 to 1.85121, saving model to InceptionNet.h5\n",
            "Epoch 6/10\n",
            "1263/1263 [==============================] - 1576s 1s/step - loss: 0.6716 - accuracy: 0.8430 - val_loss: 2.0550 - val_accuracy: 0.5553\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.85121\n",
            "Epoch 7/10\n",
            "1263/1263 [==============================] - 1586s 1s/step - loss: 0.5669 - accuracy: 0.8716 - val_loss: 1.5541 - val_accuracy: 0.6615\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.85121 to 1.55413, saving model to InceptionNet.h5\n",
            "Epoch 8/10\n",
            "1263/1263 [==============================] - 1593s 1s/step - loss: 0.4740 - accuracy: 0.8973 - val_loss: 1.8449 - val_accuracy: 0.5967\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.55413\n",
            "Epoch 9/10\n",
            "1263/1263 [==============================] - 1575s 1s/step - loss: 0.4182 - accuracy: 0.9116 - val_loss: 2.1094 - val_accuracy: 0.5749\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.55413\n",
            "Epoch 10/10\n",
            "1263/1263 [==============================] - 1593s 1s/step - loss: 0.3734 - accuracy: 0.9240 - val_loss: 1.6664 - val_accuracy: 0.6562\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.55413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhP-Yb3T8V9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db8d5ebf-96b2-4d3e-fc05-4f8ba540c070"
      },
      "source": [
        "train_datagen = ImageDataGenerator(#data_format='channels_first',\r\n",
        "                                  validation_split=0.2,\r\n",
        "                                  samplewise_center = True,\r\n",
        "                                  samplewise_std_normalization = True,rescale = 1.0/255.)\r\n",
        "\r\n",
        "train_datageno = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\r\n",
        "\r\n",
        "train_generator = train_datagen.flow_from_directory(directory=\"/content/images\",\r\n",
        "                                                    subset=\"training\",\r\n",
        "                                                    batch_size=20,\r\n",
        "                                                    shuffle=True,\r\n",
        "                                                    class_mode=\"categorical\",\r\n",
        "                                                    target_size=(224,224),\r\n",
        "                                                    seed=42)\r\n",
        "\r\n",
        "valid_generator=train_datagen.flow_from_directory(directory=\"/content/images\",\r\n",
        "                                                  subset=\"validation\",\r\n",
        "                                                  batch_size=20,\r\n",
        "                                                  shuffle=True,\r\n",
        "                                                  class_mode=\"categorical\",\r\n",
        "                                                  target_size=(224,224),\r\n",
        "                                                  seed=42)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 80800 images belonging to 101 classes.\n",
            "Found 20200 images belonging to 101 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV8BgNKg_pX8"
      },
      "source": [
        "train_datageno.fit(train_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaPfhC2H9KUy"
      },
      "source": [
        "train_generator\r\n",
        "\r\n",
        "valid_generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_02c-wY9oXi"
      },
      "source": [
        "#IMAGE AUGMENTATION\r\n",
        "\r\n",
        "# Add our data-augmentation parameters to ImageDataGenerator\r\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\r\n",
        "\r\n",
        "# Note that the validation data should not be augmented!\r\n",
        "test_datagen = ImageDataGenerator( rescale = 1.0/255. )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoYnP06t90UU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aab97e9-97ea-4f72-a9fd-2015313e2d87"
      },
      "source": [
        "#TRAINING\r\n",
        "\r\n",
        "# Flow training images in batches of 20 using train_datagen generator\r\n",
        "train_generator = train_datagen.flow_from_directory(directory=\"/content/output/train\", batch_size = 20, class_mode = 'categorical', target_size = (224, 224))\r\n",
        "\r\n",
        "# Flow validation images in batches of 20 using test_datagen generator\r\n",
        "validation_generator = test_datagen.flow_from_directory(directory=\"/content/output/val\",  batch_size = 20, class_mode = 'categorical', target_size = (224, 224))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 80800 images belonging to 101 classes.\n",
            "Found 20200 images belonging to 101 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn7T0OTtYH9X"
      },
      "source": [
        "#NOVO PROJETO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_YMJw-OYHHH"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import matplotlib.image as img\r\n",
        "%matplotlib inline\r\n",
        "import numpy as np\r\n",
        "from collections import defaultdict\r\n",
        "import collections\r\n",
        "from shutil import copy\r\n",
        "from shutil import copytree, rmtree\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import random\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "from tensorflow.keras import regularizers\r\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\r\n",
        "from tensorflow.keras.models import Sequential, Model\r\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\r\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\r\n",
        "from tensorflow.keras.optimizers import SGD\r\n",
        "from tensorflow.keras.regularizers import l2\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import models\r\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DBXuBB1YpKe"
      },
      "source": [
        "# Helper method to create train_mini and test_mini data samples\r\n",
        "def dataset_mini(food_list, src, dest):\r\n",
        "  if os.path.exists(dest):\r\n",
        "    rmtree(dest) # removing dataset_mini(if it already exists) folders so that we will have only the classes that we want\r\n",
        "  os.makedirs(dest)\r\n",
        "  for food_item in food_list :\r\n",
        "    print(\"Copying images into\",food_item)\r\n",
        "    copytree(os.path.join(src,food_item), os.path.join(dest,food_item))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rub3rI5JYqAd"
      },
      "source": [
        "food_list = ['apple_pie','pizza','omelette']\r\n",
        "src_train = '/content/output/train'\r\n",
        "dest_train = '/content/output/train_mini'\r\n",
        "src_test = '/content/output/val'\r\n",
        "dest_test = '/content/output/val_mini'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j2v4yf1YuTo",
        "outputId": "de2d8c33-8e1e-4b0e-f44c-1db3df558e13"
      },
      "source": [
        "print(\"Creating train data folder with new classes\")\r\n",
        "dataset_mini(food_list, src_train, dest_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating train data folder with new classes\n",
            "Copying images into apple_pie\n",
            "Copying images into pizza\n",
            "Copying images into omelette\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEUIlRZrZluW",
        "outputId": "ed08b9b0-1204-4bc6-cee5-f116ef097481"
      },
      "source": [
        "print(\"Total number of samples in train folder\")\r\n",
        "\r\n",
        "!find /content/output/train_mini -type d -or -type f -printf '.' | wc -c\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of samples in train folder\n",
            "2400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiCLhXisaAl0",
        "outputId": "c7152a01-1ec1-4a6e-af9e-d3e6fdad8893"
      },
      "source": [
        "\r\n",
        "\r\n",
        "print(\"Creating test data folder with new classes\")\r\n",
        "dataset_mini(food_list, src_test, dest_test)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating test data folder with new classes\n",
            "Copying images into apple_pie\n",
            "Copying images into pizza\n",
            "Copying images into omelette\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6r9eXEUacLj",
        "outputId": "54d50a5a-bf53-40e7-e056-f122d6784019"
      },
      "source": [
        "print(\"Total number of samples in test folder\")\r\n",
        "!find /content/output/val_mini -type d -or -type f -printf '.' | wc -c\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of samples in test folder\n",
            "600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMmnPuRTat7d",
        "outputId": "a296224e-1044-48c4-8d22-b872576de206"
      },
      "source": [
        "K.clear_session()\r\n",
        "n_classes = 3\r\n",
        "img_width, img_height = 299, 299\r\n",
        "train_data_dir = '/content/output/train_mini'\r\n",
        "validation_data_dir = '/content/output/val_mini'\r\n",
        "nb_train_samples = 2400 #75750\r\n",
        "nb_validation_samples = 600 #25250\r\n",
        "batch_size = 16\r\n",
        "\r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "    rescale=1. / 255,\r\n",
        "    shear_range=0.2,\r\n",
        "    zoom_range=0.2,\r\n",
        "    horizontal_flip=True)\r\n",
        "\r\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\r\n",
        "\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "    train_data_dir,\r\n",
        "    target_size=(img_height, img_width),\r\n",
        "    batch_size=batch_size,\r\n",
        "    class_mode='categorical')\r\n",
        "\r\n",
        "validation_generator = test_datagen.flow_from_directory(\r\n",
        "    validation_data_dir,\r\n",
        "    target_size=(img_height, img_width),\r\n",
        "    batch_size=batch_size,\r\n",
        "    class_mode='categorical')\r\n",
        "\r\n",
        "\r\n",
        "inception = InceptionV3(weights='imagenet', include_top=False)\r\n",
        "x = inception.output\r\n",
        "x = GlobalAveragePooling2D()(x)\r\n",
        "x = Dense(128,activation='relu')(x)\r\n",
        "x = Dropout(0.2)(x)\r\n",
        "\r\n",
        "predictions = Dense(3,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\r\n",
        "\r\n",
        "model = Model(inputs=inception.input, outputs=predictions)\r\n",
        "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "checkpointer = ModelCheckpoint(filepath='best_model_3class.hdf5', verbose=1, save_best_only=True)\r\n",
        "csv_logger = CSVLogger('history_3class.log')\r\n",
        "\r\n",
        "history = model.fit_generator(train_generator,\r\n",
        "                    steps_per_epoch = nb_train_samples // batch_size,\r\n",
        "                    validation_data=validation_generator,\r\n",
        "                    validation_steps=nb_validation_samples // batch_size,\r\n",
        "                    epochs=30,\r\n",
        "                    verbose=1,\r\n",
        "                    callbacks=[csv_logger, checkpointer])\r\n",
        "\r\n",
        "model.save('model_trained_3class.hdf5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2400 images belonging to 3 classes.\n",
            "Found 600 images belonging to 3 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "150/150 [==============================] - 74s 442ms/step - loss: 1.1282 - accuracy: 0.4067 - val_loss: 0.8111 - val_accuracy: 0.6959\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.81108, saving model to best_model_3class.hdf5\n",
            "Epoch 2/30\n",
            "150/150 [==============================] - 65s 432ms/step - loss: 0.8120 - accuracy: 0.6959 - val_loss: 0.6171 - val_accuracy: 0.7973\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.81108 to 0.61714, saving model to best_model_3class.hdf5\n",
            "Epoch 3/30\n",
            "150/150 [==============================] - 65s 433ms/step - loss: 0.6302 - accuracy: 0.7828 - val_loss: 0.4980 - val_accuracy: 0.8429\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.61714 to 0.49803, saving model to best_model_3class.hdf5\n",
            "Epoch 4/30\n",
            "150/150 [==============================] - 65s 434ms/step - loss: 0.5057 - accuracy: 0.8249 - val_loss: 0.4307 - val_accuracy: 0.8530\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.49803 to 0.43072, saving model to best_model_3class.hdf5\n",
            "Epoch 5/30\n",
            "150/150 [==============================] - 65s 432ms/step - loss: 0.4414 - accuracy: 0.8528 - val_loss: 0.3887 - val_accuracy: 0.8682\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.43072 to 0.38873, saving model to best_model_3class.hdf5\n",
            "Epoch 6/30\n",
            "150/150 [==============================] - 64s 428ms/step - loss: 0.3861 - accuracy: 0.8760 - val_loss: 0.3487 - val_accuracy: 0.8851\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.38873 to 0.34866, saving model to best_model_3class.hdf5\n",
            "Epoch 7/30\n",
            "150/150 [==============================] - 64s 425ms/step - loss: 0.3607 - accuracy: 0.8894 - val_loss: 0.3374 - val_accuracy: 0.8919\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.34866 to 0.33739, saving model to best_model_3class.hdf5\n",
            "Epoch 8/30\n",
            "150/150 [==============================] - 64s 424ms/step - loss: 0.3264 - accuracy: 0.8945 - val_loss: 0.3246 - val_accuracy: 0.8868\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.33739 to 0.32456, saving model to best_model_3class.hdf5\n",
            "Epoch 9/30\n",
            "150/150 [==============================] - 64s 425ms/step - loss: 0.3041 - accuracy: 0.8975 - val_loss: 0.3151 - val_accuracy: 0.8936\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.32456 to 0.31511, saving model to best_model_3class.hdf5\n",
            "Epoch 10/30\n",
            "150/150 [==============================] - 64s 423ms/step - loss: 0.2846 - accuracy: 0.9082 - val_loss: 0.2976 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.31511 to 0.29762, saving model to best_model_3class.hdf5\n",
            "Epoch 11/30\n",
            "150/150 [==============================] - 64s 423ms/step - loss: 0.2636 - accuracy: 0.9179 - val_loss: 0.2966 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.29762 to 0.29656, saving model to best_model_3class.hdf5\n",
            "Epoch 12/30\n",
            "150/150 [==============================] - 64s 423ms/step - loss: 0.2295 - accuracy: 0.9268 - val_loss: 0.2885 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.29656 to 0.28855, saving model to best_model_3class.hdf5\n",
            "Epoch 13/30\n",
            "150/150 [==============================] - 63s 422ms/step - loss: 0.2206 - accuracy: 0.9366 - val_loss: 0.2874 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.28855 to 0.28742, saving model to best_model_3class.hdf5\n",
            "Epoch 14/30\n",
            "150/150 [==============================] - 64s 426ms/step - loss: 0.2234 - accuracy: 0.9255 - val_loss: 0.2800 - val_accuracy: 0.9037\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.28742 to 0.28002, saving model to best_model_3class.hdf5\n",
            "Epoch 15/30\n",
            "150/150 [==============================] - 64s 425ms/step - loss: 0.2030 - accuracy: 0.9368 - val_loss: 0.2745 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.28002 to 0.27449, saving model to best_model_3class.hdf5\n",
            "Epoch 16/30\n",
            "150/150 [==============================] - 64s 424ms/step - loss: 0.1816 - accuracy: 0.9455 - val_loss: 0.2807 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.27449\n",
            "Epoch 17/30\n",
            "150/150 [==============================] - 64s 423ms/step - loss: 0.1822 - accuracy: 0.9529 - val_loss: 0.2749 - val_accuracy: 0.9172\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.27449\n",
            "Epoch 18/30\n",
            "150/150 [==============================] - 64s 425ms/step - loss: 0.1610 - accuracy: 0.9544 - val_loss: 0.2744 - val_accuracy: 0.9155\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.27449 to 0.27441, saving model to best_model_3class.hdf5\n",
            "Epoch 19/30\n",
            "150/150 [==============================] - 64s 427ms/step - loss: 0.1672 - accuracy: 0.9482 - val_loss: 0.2733 - val_accuracy: 0.9189\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.27441 to 0.27330, saving model to best_model_3class.hdf5\n",
            "Epoch 20/30\n",
            "150/150 [==============================] - 64s 424ms/step - loss: 0.1654 - accuracy: 0.9517 - val_loss: 0.2765 - val_accuracy: 0.9155\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.27330\n",
            "Epoch 21/30\n",
            "150/150 [==============================] - 64s 423ms/step - loss: 0.1414 - accuracy: 0.9660 - val_loss: 0.2750 - val_accuracy: 0.9172\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.27330\n",
            "Epoch 22/30\n",
            "150/150 [==============================] - 64s 424ms/step - loss: 0.1507 - accuracy: 0.9619 - val_loss: 0.2729 - val_accuracy: 0.9206\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.27330 to 0.27288, saving model to best_model_3class.hdf5\n",
            "Epoch 23/30\n",
            "150/150 [==============================] - 64s 424ms/step - loss: 0.1270 - accuracy: 0.9691 - val_loss: 0.2817 - val_accuracy: 0.9172\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.27288\n",
            "Epoch 24/30\n",
            "150/150 [==============================] - 64s 424ms/step - loss: 0.1339 - accuracy: 0.9702 - val_loss: 0.2642 - val_accuracy: 0.9223\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.27288 to 0.26419, saving model to best_model_3class.hdf5\n",
            "Epoch 25/30\n",
            "150/150 [==============================] - 64s 425ms/step - loss: 0.1213 - accuracy: 0.9708 - val_loss: 0.2761 - val_accuracy: 0.9172\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.26419\n",
            "Epoch 26/30\n",
            "150/150 [==============================] - 64s 423ms/step - loss: 0.1034 - accuracy: 0.9803 - val_loss: 0.2730 - val_accuracy: 0.9206\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.26419\n",
            "Epoch 27/30\n",
            "150/150 [==============================] - 64s 424ms/step - loss: 0.1191 - accuracy: 0.9684 - val_loss: 0.2831 - val_accuracy: 0.9172\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.26419\n",
            "Epoch 28/30\n",
            "150/150 [==============================] - 64s 424ms/step - loss: 0.1030 - accuracy: 0.9768 - val_loss: 0.2751 - val_accuracy: 0.9189\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.26419\n",
            "Epoch 29/30\n",
            "150/150 [==============================] - 64s 423ms/step - loss: 0.0950 - accuracy: 0.9789 - val_loss: 0.2841 - val_accuracy: 0.9189\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.26419\n",
            "Epoch 30/30\n",
            "150/150 [==============================] - 64s 424ms/step - loss: 0.0953 - accuracy: 0.9846 - val_loss: 0.2787 - val_accuracy: 0.9155\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.26419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJN9qKjxjFmN",
        "outputId": "ea95dc6e-cd58-4520-a878-f4a0dbaf5504"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\r\n",
        "\r\n",
        "baseline_model = Sequential()\r\n",
        "\r\n",
        "baseline_model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1,1), activation='relu', padding='valid', input_shape=(299, 299, 3)))\r\n",
        "baseline_model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "baseline_model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1,1), activation='relu', padding='valid'))\r\n",
        "baseline_model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "# MLP - input layer:\r\n",
        "baseline_model.add(Flatten())\r\n",
        "# MLP - hidden layer:\r\n",
        "baseline_model.add(Dense(128, activation='relu'))\r\n",
        "# MLP - output layer:\r\n",
        "baseline_model.add(Dense(categorias, activation='softmax'))\r\n",
        "\r\n",
        "# Check built model:\r\n",
        "baseline_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 297, 297, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 148, 148, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 146, 146, 32)      9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 170528)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               21827712  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 101)               13029     \n",
            "=================================================================\n",
            "Total params: 21,850,885\n",
            "Trainable params: 21,850,885\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V3EIXyxra3z"
      },
      "source": [
        "baseline_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
        "\r\n",
        "# Common attributes:\r\n",
        "steps_per_epoch = 10\r\n",
        "epochs = 1000\r\n",
        "validation_steps = 10\r\n",
        "patience=30\r\n",
        "shuffle=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKg1f8K8tdMo",
        "outputId": "2b4e1036-0036-444a-85e2-2a05c0fee494"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1. / 255)\r\n",
        "\r\n",
        "train_generator=train_datagen.flow_from_directory(\r\n",
        "                   \r\n",
        "                    directory=\"/content/output/train\",\r\n",
        "                    batch_size=100,\r\n",
        "                    seed=55551,\r\n",
        "                    shuffle=True,\r\n",
        "                    \r\n",
        "                    class_mode=\"categorical\",\r\n",
        "                    target_size=(299,299))\r\n",
        "\r\n",
        "valid_generator=train_datagen.flow_from_directory(\r\n",
        "                    \r\n",
        "                    directory=\"/content/output/val\",\r\n",
        "                    batch_size=100,\r\n",
        "                    seed=55551,\r\n",
        "                    shuffle=True,\r\n",
        "                  \r\n",
        "                    class_mode=\"categorical\",\r\n",
        "                    target_size=(299,299))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 80800 images belonging to 101 classes.\n",
            "Found 20200 images belonging to 101 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeQlyjuJrlav",
        "outputId": "4a9a919d-999e-4024-e6b7-54c42e4d79fd"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\r\n",
        "\r\n",
        "early_stop = EarlyStopping(patience=patience, monitor='val_loss', mode='min', restore_best_weights=True)\r\n",
        "\r\n",
        "history_baseline_model = baseline_model.fit_generator(train_generator,\r\n",
        "                                                      steps_per_epoch=steps_per_epoch,\r\n",
        "                                                      validation_data=valid_generator,\r\n",
        "                                                      validation_steps=validation_steps,\r\n",
        "                                                      epochs=epochs,\r\n",
        "                                                      callbacks=[early_stop],\r\n",
        "                                                      shuffle=shuffle\r\n",
        "                                                     ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "10/10 [==============================] - 23s 2s/step - loss: 8.0918 - acc: 0.0106 - val_loss: 4.6579 - val_acc: 0.0170\n",
            "Epoch 2/1000\n",
            "10/10 [==============================] - 14s 1s/step - loss: 4.6411 - acc: 0.0019 - val_loss: 4.6135 - val_acc: 0.0140\n",
            "Epoch 3/1000\n",
            "10/10 [==============================] - 14s 1s/step - loss: 4.6148 - acc: 0.0167 - val_loss: 4.6103 - val_acc: 0.0120\n",
            "Epoch 4/1000\n",
            "10/10 [==============================] - 14s 1s/step - loss: 4.6083 - acc: 0.0298 - val_loss: 4.6050 - val_acc: 0.0150\n",
            "Epoch 5/1000\n",
            "10/10 [==============================] - 13s 1s/step - loss: 4.6106 - acc: 0.0118 - val_loss: 4.6015 - val_acc: 0.0180\n",
            "Epoch 6/1000\n",
            "10/10 [==============================] - 13s 1s/step - loss: 4.5967 - acc: 0.0138 - val_loss: 4.5945 - val_acc: 0.0160\n",
            "Epoch 7/1000\n",
            "10/10 [==============================] - 13s 1s/step - loss: 4.5939 - acc: 0.0145 - val_loss: 4.5628 - val_acc: 0.0170\n",
            "Epoch 8/1000\n",
            "10/10 [==============================] - 13s 1s/step - loss: 4.5685 - acc: 0.0207 - val_loss: 4.5628 - val_acc: 0.0270\n",
            "Epoch 9/1000\n",
            "10/10 [==============================] - 13s 1s/step - loss: 4.5595 - acc: 0.0197 - val_loss: 4.5724 - val_acc: 0.0230\n",
            "Epoch 10/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.5665 - acc: 0.0258 - val_loss: 4.5373 - val_acc: 0.0310\n",
            "Epoch 11/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.5378 - acc: 0.0175 - val_loss: 4.5583 - val_acc: 0.0230\n",
            "Epoch 12/1000\n",
            "10/10 [==============================] - 13s 1s/step - loss: 4.5586 - acc: 0.0204 - val_loss: 4.5526 - val_acc: 0.0150\n",
            "Epoch 13/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.5109 - acc: 0.0204 - val_loss: 4.5462 - val_acc: 0.0230\n",
            "Epoch 14/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.4902 - acc: 0.0373 - val_loss: 4.5052 - val_acc: 0.0290\n",
            "Epoch 15/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.4945 - acc: 0.0338 - val_loss: 4.4776 - val_acc: 0.0390\n",
            "Epoch 16/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.4732 - acc: 0.0355 - val_loss: 4.4834 - val_acc: 0.0280\n",
            "Epoch 17/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.4691 - acc: 0.0319 - val_loss: 4.4394 - val_acc: 0.0310\n",
            "Epoch 18/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.4838 - acc: 0.0308 - val_loss: 4.4858 - val_acc: 0.0300\n",
            "Epoch 19/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.4642 - acc: 0.0232 - val_loss: 4.4611 - val_acc: 0.0390\n",
            "Epoch 20/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.3957 - acc: 0.0450 - val_loss: 4.4670 - val_acc: 0.0340\n",
            "Epoch 21/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.4423 - acc: 0.0380 - val_loss: 4.4387 - val_acc: 0.0520\n",
            "Epoch 22/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.3429 - acc: 0.0437 - val_loss: 4.4819 - val_acc: 0.0410\n",
            "Epoch 23/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.3926 - acc: 0.0506 - val_loss: 4.3600 - val_acc: 0.0510\n",
            "Epoch 24/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.3512 - acc: 0.0460 - val_loss: 4.4147 - val_acc: 0.0500\n",
            "Epoch 25/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.3307 - acc: 0.0690 - val_loss: 4.4001 - val_acc: 0.0510\n",
            "Epoch 26/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.3299 - acc: 0.0534 - val_loss: 4.3481 - val_acc: 0.0490\n",
            "Epoch 27/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.3146 - acc: 0.0785 - val_loss: 4.3515 - val_acc: 0.0540\n",
            "Epoch 28/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.2607 - acc: 0.0755 - val_loss: 4.3597 - val_acc: 0.0410\n",
            "Epoch 29/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.2684 - acc: 0.0684 - val_loss: 4.3014 - val_acc: 0.0620\n",
            "Epoch 30/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.2167 - acc: 0.0765 - val_loss: 4.3005 - val_acc: 0.0570\n",
            "Epoch 31/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.2419 - acc: 0.0898 - val_loss: 4.3766 - val_acc: 0.0410\n",
            "Epoch 32/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.2071 - acc: 0.0860 - val_loss: 4.3481 - val_acc: 0.0460\n",
            "Epoch 33/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.1594 - acc: 0.1032 - val_loss: 4.3074 - val_acc: 0.0480\n",
            "Epoch 34/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.1004 - acc: 0.1149 - val_loss: 4.3498 - val_acc: 0.0640\n",
            "Epoch 35/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.0807 - acc: 0.1017 - val_loss: 4.2770 - val_acc: 0.0740\n",
            "Epoch 36/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.1686 - acc: 0.1147 - val_loss: 4.3212 - val_acc: 0.0640\n",
            "Epoch 37/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.0796 - acc: 0.0961 - val_loss: 4.3119 - val_acc: 0.0650\n",
            "Epoch 38/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.0280 - acc: 0.1209 - val_loss: 4.2254 - val_acc: 0.0750\n",
            "Epoch 39/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.9736 - acc: 0.1284 - val_loss: 4.2641 - val_acc: 0.0770\n",
            "Epoch 40/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.0713 - acc: 0.1023 - val_loss: 4.1933 - val_acc: 0.0670\n",
            "Epoch 41/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.0594 - acc: 0.0975 - val_loss: 4.2463 - val_acc: 0.0720\n",
            "Epoch 42/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.9562 - acc: 0.1307 - val_loss: 4.2136 - val_acc: 0.0650\n",
            "Epoch 43/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.0166 - acc: 0.1284 - val_loss: 4.2344 - val_acc: 0.0580\n",
            "Epoch 44/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.9125 - acc: 0.1378 - val_loss: 4.2519 - val_acc: 0.0580\n",
            "Epoch 45/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.9201 - acc: 0.1635 - val_loss: 4.2016 - val_acc: 0.0710\n",
            "Epoch 46/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.9720 - acc: 0.1309 - val_loss: 4.2178 - val_acc: 0.0800\n",
            "Epoch 47/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.9124 - acc: 0.1410 - val_loss: 4.1788 - val_acc: 0.0810\n",
            "Epoch 48/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.8279 - acc: 0.1626 - val_loss: 4.1866 - val_acc: 0.0780\n",
            "Epoch 49/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.8513 - acc: 0.1694 - val_loss: 4.2515 - val_acc: 0.0710\n",
            "Epoch 50/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.8394 - acc: 0.1499 - val_loss: 4.2056 - val_acc: 0.0790\n",
            "Epoch 51/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.7454 - acc: 0.1789 - val_loss: 4.1885 - val_acc: 0.0800\n",
            "Epoch 52/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.7875 - acc: 0.1646 - val_loss: 4.1999 - val_acc: 0.0960\n",
            "Epoch 53/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.7216 - acc: 0.1813 - val_loss: 4.3013 - val_acc: 0.0740\n",
            "Epoch 54/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.7164 - acc: 0.2056 - val_loss: 4.2257 - val_acc: 0.0770\n",
            "Epoch 55/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.8254 - acc: 0.1640 - val_loss: 4.2379 - val_acc: 0.0780\n",
            "Epoch 56/1000\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.8048 - acc: 0.1608 - val_loss: 4.1730 - val_acc: 0.0850\n",
            "Epoch 57/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.6773 - acc: 0.2068 - val_loss: 4.2284 - val_acc: 0.0800\n",
            "Epoch 58/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.7645 - acc: 0.2013 - val_loss: 4.2072 - val_acc: 0.0760\n",
            "Epoch 59/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.5288 - acc: 0.2287 - val_loss: 4.3022 - val_acc: 0.0720\n",
            "Epoch 60/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.7024 - acc: 0.1937 - val_loss: 4.2573 - val_acc: 0.0750\n",
            "Epoch 61/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.5749 - acc: 0.2251 - val_loss: 4.1548 - val_acc: 0.0950\n",
            "Epoch 62/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.6350 - acc: 0.2145 - val_loss: 4.2436 - val_acc: 0.0880\n",
            "Epoch 63/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.5083 - acc: 0.2335 - val_loss: 4.1864 - val_acc: 0.1030\n",
            "Epoch 64/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.5864 - acc: 0.2383 - val_loss: 4.3112 - val_acc: 0.0790\n",
            "Epoch 65/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.5237 - acc: 0.2406 - val_loss: 4.2222 - val_acc: 0.1020\n",
            "Epoch 66/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.5552 - acc: 0.2300 - val_loss: 4.2255 - val_acc: 0.0790\n",
            "Epoch 67/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.4801 - acc: 0.2837 - val_loss: 4.3075 - val_acc: 0.0780\n",
            "Epoch 68/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.5954 - acc: 0.2419 - val_loss: 4.2470 - val_acc: 0.0890\n",
            "Epoch 69/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.4463 - acc: 0.2670 - val_loss: 4.1071 - val_acc: 0.1120\n",
            "Epoch 70/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.3872 - acc: 0.2965 - val_loss: 4.3043 - val_acc: 0.0870\n",
            "Epoch 71/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.3264 - acc: 0.2670 - val_loss: 4.2269 - val_acc: 0.0840\n",
            "Epoch 72/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.2686 - acc: 0.2920 - val_loss: 4.1715 - val_acc: 0.0880\n",
            "Epoch 73/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.3118 - acc: 0.2733 - val_loss: 4.2591 - val_acc: 0.0820\n",
            "Epoch 74/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.2501 - acc: 0.2934 - val_loss: 4.2520 - val_acc: 0.0860\n",
            "Epoch 75/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.2836 - acc: 0.2803 - val_loss: 4.3053 - val_acc: 0.0870\n",
            "Epoch 76/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.1090 - acc: 0.3155 - val_loss: 4.2951 - val_acc: 0.0930\n",
            "Epoch 77/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.2406 - acc: 0.2944 - val_loss: 4.3430 - val_acc: 0.0890\n",
            "Epoch 78/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.2875 - acc: 0.3050 - val_loss: 4.2426 - val_acc: 0.0890\n",
            "Epoch 79/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.1790 - acc: 0.3312 - val_loss: 4.2755 - val_acc: 0.0900\n",
            "Epoch 80/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.2192 - acc: 0.3179 - val_loss: 4.1505 - val_acc: 0.0960\n",
            "Epoch 81/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.0384 - acc: 0.3691 - val_loss: 4.1679 - val_acc: 0.0870\n",
            "Epoch 82/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.1791 - acc: 0.3270 - val_loss: 4.2358 - val_acc: 0.0980\n",
            "Epoch 83/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.1107 - acc: 0.3451 - val_loss: 4.3043 - val_acc: 0.0820\n",
            "Epoch 84/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.0125 - acc: 0.3328 - val_loss: 4.2743 - val_acc: 0.0990\n",
            "Epoch 85/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.0817 - acc: 0.3647 - val_loss: 4.1521 - val_acc: 0.1110\n",
            "Epoch 86/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.9843 - acc: 0.3681 - val_loss: 4.2973 - val_acc: 0.0910\n",
            "Epoch 87/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.0167 - acc: 0.3670 - val_loss: 4.2551 - val_acc: 0.1070\n",
            "Epoch 88/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.9887 - acc: 0.3465 - val_loss: 4.2164 - val_acc: 0.1040\n",
            "Epoch 89/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.1338 - acc: 0.3260 - val_loss: 4.2206 - val_acc: 0.0910\n",
            "Epoch 90/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.9280 - acc: 0.3805 - val_loss: 4.3019 - val_acc: 0.0940\n",
            "Epoch 91/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.9450 - acc: 0.3616 - val_loss: 4.2841 - val_acc: 0.0910\n",
            "Epoch 92/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.8035 - acc: 0.4154 - val_loss: 4.2557 - val_acc: 0.0940\n",
            "Epoch 93/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.8710 - acc: 0.3908 - val_loss: 4.3550 - val_acc: 0.0880\n",
            "Epoch 94/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.8114 - acc: 0.3940 - val_loss: 4.2070 - val_acc: 0.1020\n",
            "Epoch 95/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.8815 - acc: 0.3919 - val_loss: 4.2943 - val_acc: 0.0990\n",
            "Epoch 96/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.7793 - acc: 0.3849 - val_loss: 4.3895 - val_acc: 0.1020\n",
            "Epoch 97/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.7339 - acc: 0.4321 - val_loss: 4.2087 - val_acc: 0.0970\n",
            "Epoch 98/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.8253 - acc: 0.4136 - val_loss: 4.3663 - val_acc: 0.0950\n",
            "Epoch 99/1000\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.7324 - acc: 0.4422 - val_loss: 4.3112 - val_acc: 0.1060\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}